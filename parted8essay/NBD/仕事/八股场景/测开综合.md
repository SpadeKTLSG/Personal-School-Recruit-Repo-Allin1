‍

‍

## 岗位选择与介绍

‍

### 什么是软件测试？一句话概括

利用一定的方法对软件的质量或者使用性进行判断和评估的过程

‍

‍

### 为什么投测开(送命题)

我很细心，注重细节，喜欢找bug.

我要求代码质量, 但是在开发中这可能成为累赘. 我感觉自己适合测开.

‍

从专注于"测开需要什么"和"我有什么"来进行论述:

* 测开需要开发能力来更好的进行各类测试与问题定位, 需要测试开发人员的细心参与, 同时最好需要对软件架构的理解能力
* 我有良好的从架构到后台前台的开发素质, 日常开发沉稳细心, 注重细节, 经常参与问题定位抓bug工作, 有意向往测开方向发展

‍

‍

为什么选择测试开发 (偏聊天)

> 首先，我认为的测开是测试和开发工作都在做的。一方面，据我了解，在近几年，国内对软件测试越来越重视，并且从用户角度来说，对于同类产品，可能会更加注重产品的质量和服务，所以我觉得测试的发展前景是非常好的。其次，测试在一个项目开发的过程中是非常重要的一环。测试人员的责任非常大，责任越大成就感就越大。我很喜欢这样的工作。另一方面，测开还有一部分开发工作，无论是自动化脚本还是测试工具或框架，都提高了测试的效率，为质量效率保证工作提供了有力的保障。并且测开的所需技术广度也是很高，所以我认为测开会激发我对这个岗位的热爱和持续学习的态度。（并且来说，我目前具备了一些测开所必备的理论知识和技能并且还在不断地学习中，我认为我可以较快地胜任这个岗位。）

‍

‍

### 怎么理解测试开发 (偏聊天)

> 首先从岗位名字来看，要求测开工程师即要懂测试，又要懂开发。其次这个岗位融合了开发角色和质量意识，要求我们兼具开发人员的技能和测试人员的思维。总的来说，测试开发工程师的定位就是保障产品的质量和提高测试的效率

‍

> 还是要对接JD, 要看看是开发测试还是测试开发. 开发测试主要的含义是你的工作是为了搭建更好测试的平台, 测试开发甚至部分还主要停留在性能测试方面

‍

### 怎么理解测试 (这件事) (偏聊天)

> 测试是无处不在的，撇开软件，从生活来看比如买回来一个东西，会去检查质量问题，考试交卷前会检查等其实这都是在测试，目的就是为了发现错误，避免影响应用体验。回到程序中，测试是产品上线的最后一道把关，如果测试工作做得到位，就能避免很多的问题，像复工后钉钉系统短崩、12306高峰期买票老进不去，这其实就是性能做的不够好，测试人员在性能测试时也没测出来它的饱和值，所以说，测试在软件中是非常必要的，可以找出软件中存在的问题

‍

> 我还是会正经点, 区分为日常的测试含义, 和软件工程中的测试, 联系到软件开发的生命周期, 结合自己开发的经历说明测试是一件很重要的事情.

‍

‍

### 为什么选择测试(方向) (偏聊天)

一方面我做事情和处理问题喜欢刨根问底，喜欢把问题全都找出来再一一解决，喜欢并享受这个过程，另一方面因为国内计算机已经过了野蛮发展的阶段，所以以后在存量市场是以用户体验和质量取胜的，而测试是这其中最关键的一环，所以肯定会越来越受到重视，选择测试是看中了发展的前景，并且个人很喜欢站在客户的角度去看待产品，很适合这个行业，所以我选择测试。

‍

‍

### 你对SQA的职责和工作活动(如软件度量)的理解? (问问你QA组的理解) (偏聊天)

SQA就是独立于软件开发的项目组，通过对软件开发过程的监控，来保证软件的开发流程按照指定的CMM规程(如果有相应的CMM规程),对于不符合项及时提出建议和改进方案，必要时可以向高层经理汇报以求问题的解决。通过这样的途径来预防缺陷的引入，从而减少后期软件的维护成本。

SQA主要的工作活动包括制定SQA工作计划，参与阶段产物的评审，进行过程质量、功能配置及物理配置的审计等;对项目开发过程中产生的数据进行度量等等。

‍

‍

### 软件测试人员就是QA吗？

软件测试人员和QA（质量保证）虽然有相似的职责，但它们的角色和工作范围有所不同。软件测试人员主要负责执行测试，找出软件中的缺陷，而QA则更加关注整体的软件质量管理和流程优化。

‍

1. **软件测试人员**：主要负责制定和执行测试用例，进行功能、性能、兼容性等方面的测试，目的是发现并报告软件中的缺陷。
2. **QA（质量保证）** ：负责制定质量标准、建立测试流程和规范，确保整个软件开发过程中的质量控制。QA更偏向于管理和流程优化，确保产品质量从开发到交付的每个环节都得到保障。

‍

‍

‍

### 软件测试工程师的工作内容

1.寻找软件中的bug，并且越早发现越好

2.确认bug的可重复性以及bug产生的步骤

3.确认bug是否被解决

4.测试方法，测试计划，测试平台，测试代码，测试用例，测试文档，测试报告的确定、编写和执行。

‍

‍

### 你觉得测试开发岗和开发岗是有什么区别？/ 你认为做好测试要具备什么技能

‍

‍

#### 测试开发岗

1. **主要职责**：

    * 设计和编写测试用例。
    * 开发和维护自动化测试脚本。
    * 执行测试，记录和跟踪缺陷。
    * 分析测试结果，编写测试报告。
    * 确保软件质量和稳定性。
2. **技能要求**：

    * 熟悉测试理论和方法。
    * 掌握自动化测试工具和框架（如 Selenium、JUnit、TestNG 等）。
    * 具备编程能力，通常需要掌握一到两门编程语言（如 Java、Python）。
    * 了解持续集成和持续交付（CI/CD）流程。

‍

‍

#### 开发岗

1. **主要职责**：

    * 设计和实现软件功能。
    * 编写和维护代码。
    * 进行代码审查和优化。
    * 修复软件缺陷。
    * 参与需求分析和系统设计。
2. **技能要求**：

    * 熟悉软件开发生命周期（SDLC）。
    * 掌握编程语言和开发框架（如 Java、JavaScript、Spring、React 等）。
    * 具备问题分析和解决能力。
    * 了解版本控制工具（如 Git）。

‍

‍

#### 总结

* **测试开发岗**更 侧重于软件质量保证，通过测试用例和自动化测试来发现和解决问题。
* **开发岗**更侧重于功能实现和代码编写，通过设计和实现软件功能来满足需求。

两者在工作中有一定的交集，但侧重点不同。

‍

‍

‍

### 测试对企业的价值: 通过对测试的学习，以及项目经验，说一说

> 这里要联系自己在项目中的关于测试的交互引申到对项目对企业的好处, 主要是说明测试能干甚么, 能有什么意义

* **保证质量**：测试帮助确保产品在发布前达到预期质量，降低线上风险。
* **降低成本**：早期发现问题可以减少后期修复和客户投诉的成本，提高维护效率。
* **提升用户满意度**：高质量的软件更能满足用户需求，从而提升用户体验和企业品牌形象。
* **促进持续改进**：通过测试数据和缺陷分析，企业可以不断优化流程、改进设计和提升产品竞争力。
* **风险管理**：测试在需求评审、设计评审等阶段提供预防性的质量保障，降低项目失败风险。

‍

### 测试在各个阶段的角色和作用（以需求评审为例）

‍

* **需求评审阶段**：

  * **参与需求讨论**：测试人员参与需求评审，提出测试可行性、边界情况、异常流程等问题，确保需求的明确性和完整性。
  * **制定初步测试计划**：基于需求文档，测试人员可以初步设计测试思路和制定测试计划，确定验收标准。
* **设计阶段**：

  * **评估设计方案**：检查设计文档是否具备良好的可测试性，提出潜在的风险和改进建议。
* **开发阶段**：

  * **编写测试用例**：基于需求和设计编写详细的测试用例，覆盖正常流程和异常场景。
* **上线前与上线后**：

  * **执行回归测试和验收测试**，确保新功能和改动没有破坏旧功能，及时发现并反馈缺陷。
* **总体**：测试人员在各阶段都是质量的守护者，起到预防问题、发现问题、并帮助团队不断改进产品质量的作用。

‍

‍

### 软件测试工程师人员如何分工

软件测试工程师的分工通常根据团队的规模、项目的复杂性以及每个成员的技能来进行。常见的分工方式包括：

1. **测试经理/测试负责人**：负责整体测试策略和计划，协调团队成员的工作，确保测试资源的合理分配。
2. **测试架构师/高级测试工程师**：设计测试框架、选择测试工具，并指导团队使用自动化测试工具，处理技术难题。
3. **手动测试工程师**：执行功能测试、兼容性测试等手动测试任务，确保软件按预期运行。
4. **自动化测试工程师**：编写、执行自动化测试脚本，自动化回归测试等，提升测试效率。
5. **性能测试工程师**：专门进行性能测试，确保系统在高负载下的稳定性。
6. **安全测试工程师**：负责进行安全性测试，确保软件没有漏洞和安全隐患。
7. **测试支持人员**：提供环境搭建、测试数据准备等支持工作，确保测试顺利进行。

‍

‍

## 软件工程/软件测试理论

‍

‍

‍

### 软件开发的几个阶段

1. 项目启动阶段：了解客户需求、配置相关资源
2. 项目设计阶段：明确客户需求，确立软件开发、测试的方法
3. 项目执行阶段：开发与测试阶段
4. 项目竣工阶段：软件的上市、后期维护与技术支持

‍

‍

‍

‍

### 做好软件测试的一些关键点

‍

1-测试人员必须经过测试基础知识和理论的相关培训

2-测试人员必须**熟悉系统功能和业务 ? 不一定, 简单的测试不需要**

3-测试要有计划，而且测试方案要和整个项目计划协调好

4-必须实现编写测试用例，测试执行阶段必须根据测试用例进行

5-易用性，功能，分支，边界，性能等功能行和非功能性需求都要进行测试

6-对于复杂的流程一定要进行流程分支，组合条件分析，再进行等价类划分准备相关测试数据

7-测试设计的一个重要内容是要准备好具体的测试数据，清楚这个测试数据是测试那个场景或分支的。

8-个人任务平均每三个测试用例至少应该发现一个BUG，否则只能说明测试用例质量不好

9-除了每天构建的重复测试可以考虑测试自动化外，其他暂时都不要考虑去自动话

‍

‍

‍

‍

### 软件测试方法分类

1）白盒、黑盒、灰盒

2）单元测试、集成测试、确认测试、系统测试、验收测试、 回归测试、Alpha 测试、Beta 测试

3）静态测试和动态测试

‍

#### 你所熟悉的软件测试类型有哪些?

**功能测试、性能测试、界面测试**

‍

功能测试在测试工作中占有比例最大，功能测试也叫**黑盒测试**。

‍

性能测试是通过自动化的测试工具模拟多种正常、峰值以及异常负载条件来对系统的各项性能指标进行测试。负载测试和压力测试都属于性能测试，两者可以结合进行。

‍

界面测试，界面是软件与用户交互的最直接的层，界面的好坏决定用户对软件的第一印象。

‍

区别在于，功能测试关注产品的所有功能，要考虑到每个细节功能，每个可能存在的功能问题。性能测试主要关注产品整体的多用户并发下的稳定性和健壮性。界面测试则关注与用户体验相关内容，用户使用该产品的时候是否已用，是否易懂，是否规范(用户无意输入无效的数据，当然考虑到体验性，不能太粗鲁的弹出警告)。做某个性能测试的时候，首先它可能是个功能点，首先要保证她的功能是没有问题的，然后再考虑性能的问题。

‍

‍

### 白盒测试有哪些

白盒测试（White-box Testing）是一种软件测试方法，测试人员需要了解被测试软件的内部结构和实现细节。白盒测试的主要类型包括：

‍

1. **单元测试**：测试单个模块或函数的功能，确保其按预期工作。
2. **集成测试**：测试多个模块或组件之间的交互，确保它们协同工作。
3. **静态代码分析**：通过工具或手动检查代码，发现潜在的错误、代码规范问题和安全漏洞。
4. **代码覆盖率分析**：通过分析测试用例执行时覆盖的代码路径，评估测试的充分性。
5. **路径测试**：测试所有可能的执行路径，确保每条路径都被测试到。
6. **分支测试**：测试代码中的每个分支（如 if-else 语句），确保所有分支都被执行。
7. **循环测试**：测试代码中的循环结构，确保循环在各种边界条件下都能正确执行。

‍

‍

### 测试左移

测试左移（Shift Left Testing）是一种测试策略，旨在将测试活动尽早地引入到软件开发生命周期的早期阶段。通过在需求分析、设计和开发阶段进行测试，可以及早发现和修复缺陷，从而提高软件质量并降低修复成本。

‍

‍

#### 测试左移的关键实践

1. **需求分析阶段的测试**：在需求分析阶段，测试人员参与需求评审，确保需求的可测试性，并编写初步的测试用例。
2. **设计阶段的测试**：在设计阶段，测试人员参与设计评审，识别潜在的设计缺陷，并编写详细的测试用例。
3. **开发阶段的测试**：在开发阶段，测试人员与开发人员紧密合作，进行单元测试、集成测试和代码审查，确保代码质量。
4. **持续集成和持续交付（CI/CD）** ：将测试集成到持续集成和持续交付流程中，确保每次代码变更都经过自动化测试验证。

通过在开发阶段进行单元测试和集成测试，可以及早发现和修复缺陷，从而提高软件质量。

‍

‍

### 逆等测试

逆等测试（Negative Testing）是一种测试方法，旨在验证系统在异常或错误输入情况下的行为。与正向测试（Positive Testing）不同，逆等测试关注的是系统在不正常或无效输入下的反应，以确保系统能够正确处理错误情况并保持稳定性。

‍

逆等测试的常见场景

1. **无效输入**：输入无效的数据，如空值、超长字符串、非法字符等。
2. **边界条件**：测试输入数据的边界值，如最小值、最大值、超出范围的值等。
3. **异常情况**：模拟系统异常情况，如网络断开、服务器宕机等。
4. **权限问题**：测试用户在没有权限的情况下尝试执行操作。

‍

‍

### 双向测试是什么

双向测试（Bidirectional Testing）是一种测试方法，旨在验证系统在正向和反向操作中的一致性和正确性。它通常用于验证数据转换、数据同步和接口通信等场景，确保数据在不同方向上的处理结果一致。

‍

‍

### 假如给你一个新产品，你将从哪些方面来保障它的质量

‍

可以从代码开发、测试保障、线上质量三个方面来保障。

在代码开发阶段，有单元测试、代码Review、静态代码扫描等；

测试保障阶段，有功能测试、性能测试、高可用测试、稳定性测试、兼容性测试等；

在线上质量方面，有灰度发布、紧急回滚、故障演练、线上监控和巡检等。

‍

‍

### 高可用测试怎么做？

* **模拟故障**：设计测试场景模拟单点故障、网络中断、服务器崩溃等情况，检验系统的自动恢复和容错能力。
* **压力和负载测试**：使用工具（如 JMeter、LoadRunner）进行压力测试，确定系统在高并发下的响应和稳定性。
* **分布式环境测试**：测试集群或多机部署的场景，验证负载均衡、故障转移和数据一致性。
* **混沌工程**：采用 Chaos Monkey 等工具随机制造故障，验证系统整体高可用性设计是否健壮。

‍

‍

### 效果测试怎么做？主观打分怎么保证准确性？

‍

效果测试方法：

* **用户体验测试**：邀请目标用户进行真实场景测试，收集定性反馈。
* **A/B 测试**：对比不同设计方案，使用统计数据验证哪种方案效果更好。
* **问卷调查和评分**：设计标准化的问卷和评分标准，让多位用户对某个体验进行打分。

‍

保证准确性的措施：

* **标准化评估**：制定明确的评价指标和打分标准，减少主观差异。
* **多评审员制**：采用多位评审员的打分平均值，降低单人主观偏差。
* **数据驱动**：结合客观数据（如点击率、留存率等）进行综合评估。
* **盲测**：设计盲测方案，避免用户受到预期影响，从而提高评分客观性。

‍

‍

### **Alpha测试**和**Beta测试**

**Alpha测试**和**Beta测试**是软件开发生命周期中的两个重要阶段，它们的主要区别如下：

1. **Alpha测试**：

    * **时间**：通常在软件开发的后期，内部开发人员和测试团队进行。
    * **目标**：主要是发现软件中的缺陷，验证软件的基本功能是否稳定，通常是在开发团队内部进行。
    * **参与者**：由开发人员、测试人员和部分公司内部人员（如技术支持团队）参与。
    * **环境**：在开发环境或专门的测试环境中进行，软件尚未对外部用户开放。
    * **性质**：属于内部测试，目的是在产品发布前发现并修复潜在问题。
2. **Beta测试**：

    * **时间**：通常在Alpha测试之后，接近产品发布时进行。
    * **目标**：通过让真实用户使用软件，收集反馈和发现潜在的使用问题，验证软件是否适合广泛使用。
    * **参与者**：由外部用户参与，可以是公司的现有客户，也可以是自愿参与的用户群体。
    * **环境**：通常是在用户的实际环境中进行，软件对外部用户开放。
    * **性质**：属于外部测试，目的是通过真实用户的反馈完善产品。

‍

‍

### 文档测试主要包含什么内容

文档测试主要是对软件项目相关文档进行检查和验证，确保文档的准确性、完整性和一致性。它通常包含以下几个方面：

1. **需求文档测试**：检查需求是否清晰、完整、无歧义，并与实际开发一致。
2. **设计文档测试**：验证设计文档是否能够准确表达系统架构、模块设计等内容，是否符合需求。
3. **用户文档测试**：检查用户手册、操作指南等是否易懂、准确，能够帮助用户有效使用软件。
4. **测试文档测试**：确保测试计划、测试用例和测试报告等文档准确、清晰，并与实际测试过程一致。

‍

‍

### 用户共同测试（UAT测试）的注意点有哪些

用户验收测试（UAT）是软件开发过程中由最终用户执行的测试，主要目的是验证软件是否符合用户需求和实际使用场景。进行UAT时需要注意以下几点：

1. **明确测试目标**：确保UAT的目标是验证软件是否满足业务需求，而不是执行所有的功能测试。
2. **用户参与**：UAT测试应由真实用户进行，他们更了解自己的业务流程和需求，能有效发现潜在问题。
3. **准备测试环境**：提供一个接近生产环境的测试环境，确保测试结果具有参考价值。
4. **制定测试用例**：测试用例应基于业务需求和用户场景，确保覆盖关键功能和用户需求。
5. **记录问题并反馈**：在UAT过程中，及时记录和反馈问题，确保问题得到及时解决。

‍

‍

### 测试评估？测试评估的范围是什么？

‍

**测试评估**：  
测试评估是指对软件测试活动的效果进行检查和评估，以确保测试过程和测试结果的质量。它是一个持续的过程，用来验证测试是否按计划进行、是否能够充分覆盖所有需求、是否发现了足够的缺陷，并且最终测试是否达到了预定的目标。

‍

**测试评估的范围**：  
测试评估的范围包括以下几个方面：

1. **测试过程的评估**：  
    – 评估整个测试过程的效率和效果，是否按照测试计划顺利进行，是否存在资源浪费，测试活动是否达到了预定目标。  
    – 例如，检查测试执行的进度、测试用例的执行情况、缺陷的跟踪和修复情况等。
2. **测试覆盖率的评估**：

    * 评估测试是否充分覆盖了需求，检查是否漏测了关键功能。可以通过功能覆盖、代码覆盖、路径覆盖等方式进行评估。
    * 例如，查看哪些需求未被测试用例覆盖，哪些代码路径未经过测试等。
3. **缺陷管理的评估**：

    * 评估缺陷的发现、记录和处理情况，是否有效地发现了系统中的缺陷，并且缺陷的修复是否及时和有效。
    * 例如，评估缺陷的严重性分布、修复周期、缺陷的闭环处理等。
4. **测试结果的评估**：

    * 评估测试结果的质量，是否满足产品质量标准，是否发现足够多的严重缺陷，最终的测试是否能为产品发布提供决策支持。
    * 例如，评估测试执行后是否报告了足够的测试用例结果、缺陷报告的完整性、测试报告的准确性等。
5. **资源使用的评估**：

    * 评估在测试过程中使用的资源（如时间、人员、环境等）是否得到了有效利用，是否存在资源浪费。
    * 例如，评估测试过程中是否存在不必要的重复工作，测试人员是否按计划完成工作，测试环境是否配置合理等。

‍

‍

### bug的几个要素

1. **标题（Title）** ：简洁地描述Bug的主要问题。
2. **描述（Description）** ：详细描述Bug的具体情况，包括出现问题的功能模块、操作步骤、预期结果和实际结果。
3. **重现步骤（Steps to Reproduce）** ：列出可以复现Bug的具体操作步骤。
4. **预期结果（Expected Result）** ：描述在正常情况下，操作应该达到的结果。
5. **实际结果（Actual Result）** ：描述Bug发生时，实际的输出或行为。
6. **环境信息（Environment）** ：包括操作系统、浏览器版本、硬件配置、网络环境等，以便在相同环境下重现问题。
7. **截图/日志（Screenshot/Log）** ：提供相关的截图或错误日志，有助于定位问题。
8. **优先级/严重性（Priority/Severity）** ：Bug的紧急程度和对系统功能的影响。
9. **状态（Status）** ：Bug的当前状态，如“新建”、“已修复”、“待验证”等。

‍

‍

## 测试日常流程

‍

### 测试(类型分)分为哪几个阶段?

一般来说分为5个阶段：单元测试、集成测试、确认测试、系统测试、验收测试

‍

‍

### 测试驱动开发？

测试驱动开发（Test-Driven Development，简称TDD）是一种软件开发方法，其核心思想是**先编写测试用例**，然后编写实现代码，以通过测试用例。TDD 的主要步骤如下

‍

1. **编写测试用例**：根据需求编写一个失败的测试用例，测试用例应该尽可能简单，明确测试目标。
2. **运行测试**：运行测试用例，确保测试用例失败，以验证测试用例的有效性。
3. **编写实现代码**：编写最少量的代码，使测试用例通过。
4. **运行测试**：再次运行测试用例，确保测试通过。
5. **重构代码**：在确保测试通过的前提下，重构代码，提高代码质量和可维护性。
6. **重复**：重复以上步骤，逐步完善功能。

‍

‍

### 测试流程的生命周期

‍

简单点的

* 需求调查：全面了解系统概况、应用领域、软件开发周期、软件开发环境、开发组织、时间安排、功能需求、性能需求、质量需求及测试要求等。根据系统概况进行项目所需的人员、时间和工作量估计以及项目报价。制定初步的项目计划。
* 测试准备：组织测试团队、培训、建立测试和管理环境等。
* 测试设计：按照测试要求进行每个测试项的测试设计，包括测试用例的设计和测试脚本的开发等。
* 测试实施：按照测试计划实施测试。
* 测试评估：根据测试的结果，出具测试评估报告。

‍

1. **需求分析**：

    * 了解和分析需求，确定测试目标和范围。
    * 编写测试计划，定义测试策略和方法。
2. **测试计划**：

    * 制定详细的测试计划，包括测试资源、时间表、测试环境等。
    * 确定测试用例的优先级和测试数据。
3. **测试设计**：

    * 编写测试用例，详细描述测试步骤、预期结果等。
    * 准备测试数据和测试环境。
4. **测试执行**：

    * 按照测试计划和测试用例执行测试。
    * 记录测试结果和发现的缺陷。
5. **缺陷管理**：

    * 记录和跟踪缺陷，确保缺陷得到及时修复。
    * 重新测试修复后的缺陷，验证修复效果。
6. **测试报告**：

    * 汇总测试结果，编写测试报告。
    * 分析测试覆盖率和测试效果，提出改进建议。
7. **测试总结**：

    * 评估测试过程和测试结果，总结经验教训。
    * 更新测试文档和测试用例，为后续测试提供参考。

‍

‍

### 如何确定测试的重点和优先级？

确定测试的重点和优先级可以通过以下几个步骤进行：

1. **需求分析**：

    * 了解和分析需求，确定哪些功能是关键功能，哪些是次要功能。
    * 确定业务需求和用户需求的优先级。
2. **风险评估**：

    * 评估各个功能模块的风险，确定哪些模块可能会对系统的稳定性和安全性产生重大影响。
    * 根据风险评估结果，确定测试的重点。
3. **历史数据**：

    * 分析历史缺陷数据，找出哪些模块或功能在过去出现过较多问题。
    * 对这些高风险区域进行重点测试。
4. **用户反馈**：

    * 收集用户反馈，了解用户在使用过程中遇到的主要问题。
    * 根据用户反馈，确定测试的重点和优先级。
5. **测试覆盖率**：

    * 确保测试覆盖率高，优先测试那些尚未覆盖或覆盖率较低的功能模块。
    * 使用测试覆盖率工具来辅助确定测试的重点。
6. **资源和时间**：

    * 根据项目的资源和时间限制，合理分配测试资源，确定测试的优先级。
    * 优先测试那些对项目成功至关重要的功能。

‍

‍

‍

‍

### 当开发人员说不是BUG时，你如何应付?

开发人员说不是BUG，有2种情况

‍

一是需求没有确定，所以我可以这么做，这个时候可以找来产品经理进行确认，需不需要改动。3方商量确定好后再看要不要改

二是这种情况不可能发生，所以不需要修改，这个时候，我可以先尽可能的说出是BUG的依据是什么?如果被用户发现或出了问题，会有什么不良结果? 程序员可能会给你很多理由，你可以对他的解释进行反驳。如果还是不行，那我可以给这个问题提出来，跟开发经理和测试经理进行确认，如果要修改就改，如果不要修改就不改。其实有些真的不是BUG，我也只是建议的方式写进测试文档中，如果开发人员不修改也没有大问题。如果不是BUG的话，一定要坚持自己的立场，让问题得到最后的确认。

‍

‍

### 测试人员可以做什么推动开发的质量提升呢

测试人员可以通过以下几种方式推动开发的质量提升：

1. **早期介入**：在需求分析和设计阶段就参与进来，帮助识别潜在的问题和风险。
2. **编写测试用例**：根据需求文档编写详细的测试用例，确保覆盖所有功能和边界情况。
3. **自动化测试**：使用自动化测试工具（如 Selenium、JUnit 等）编写和执行自动化测试脚本，提高测试效率和覆盖率。
4. **持续集成**：与开发团队合作，将测试集成到持续集成（CI）流程中，确保每次代码变更都经过测试验证。
5. **代码审查**：参与代码审查，帮助发现代码中的潜在问题和改进点。
6. **性能测试**：进行性能测试，评估系统在高负载下的表现，找出性能瓶颈并提出优化建议。
7. **用户反馈**：收集和分析用户反馈，帮助开发团队了解用户需求和改进产品质量。
8. **培训和沟通**：与开发团队保持良好的沟通，分享测试经验和最佳实践，提升团队整体的质量意识。

通过这些方式，测试人员可以有效地推动开发质量的提升，确保产品的稳定性和可靠性。

‍

‍

‍

### bug单怎么写

‍

1. 软件版本
2. 开发的接口人员
3. 优先级
4. 严重程度
5. 属于的模块
6. 描述，需要尽量给出bug的重现步骤
7. 附件中能给出相关的日志和截图

尽可能的去帮助开发进行bug 的定位

‍

‍

### 缺陷管理的生命周期

缺陷（Bug）的生命周期描述了从缺陷被发现到最终关闭的整个过程

‍

**主要阶段**

* **新建（New）** ：测试人员发现缺陷并创建缺陷报告。
* **确认/分级（Triage/Classification）** ：团队对缺陷进行审核，确定其严重性、优先级及是否有效。
* **分配（Assignment）** ：将缺陷分配给相关开发人员处理。
* **修复（Fix）** ：开发人员定位问题并提交修复代码。
* **验证（Retest）** ：测试人员对修复后的版本进行验证，确认缺陷是否已解决。
* **关闭（Closure）** ：缺陷经验证无误后，标记为关闭；如果问题依旧存在，可重新打开。

* > 有时还包括“挂起”（Pending）、“拒绝”（Rejected）等状态，以处理无法复现或非缺陷的情况
  >

‍

‍

### 缺陷提交以后做什么

* **审核和评审**：提交后，缺陷需要经过审核和评审，确定问题的真实性和优先级。
* **分配修复**：经过评审后，将缺陷分配给对应的开发人员进行修复，同时可能会更新状态为“已接受”或“处理中”。
* **跟踪与反馈**：修复完成后，开发人员提交修改，测试团队进行回归测试验证修复效果。
* **记录和分析**：整个过程中记录缺陷数据，便于后续统计、质量改进和风险评估。

‍

‍

### 如果有一个bug不能复现怎么去跟开发沟通

* **收集详细信息**：

  * **环境与日志**：提供详细的运行环境、操作系统、浏览器/设备信息、日志和错误截图。
  * **重现步骤**：详细记录操作步骤、输入数据和异常现象，最好录制视频帮助说明。
* **分析与讨论**：

  * **协同分析**：与开发一起讨论可能的触发条件、是否为间歇性问题、是否存在并发或环境依赖。
  * **提出假设**：列出可能的原因，并建议对相关代码或配置进行重点检查。
* **跟踪与验证**：

  * **反馈更新**：如果开发进行了修改，协助验证修复效果。
  * **监控指标**：建议在生产中增加监控，观察是否还有类似异常发生。

‍

‍

### 上线后的出现漏测点怎么处理？

当上线后发现漏测点，首先需要进行问题复现，确保问题的真实性。

接下来，迅速评估该漏测点的严重性和影响范围。

如果是高优先级的问题，应该紧急修复并进行回归测试，确保修复不会影响其他功能。

然后，分析漏测的原因，是否是测试用例覆盖不足、需求不清晰，还是测试执行中出现疏漏。

最后，改进测试流程，增强测试覆盖度，避免类似问题再发生

‍

‍

### 项目提测的工作清单

1. **需求文档的确认**：确保测试人员对需求文档有充分的了解，理解需求的功能、非功能性要求和预期结果。
2. **测试计划编写**：根据项目需求，编写测试计划，包括测试策略、测试范围、测试资源、时间安排等。
3. **测试用例设计**：编写详细的测试用例，覆盖所有功能点，考虑正常情况和边界情况。
4. **测试环境准备**：配置测试环境，确保硬件、软件、网络等条件满足测试需求。
5. **测试数据准备**：准备必要的测试数据，确保测试用例的执行不受数据限制。
6. **缺陷管理工具配置**：确保缺陷管理工具（如JIRA、Bugzilla等）配置完毕，测试人员能够有效记录、跟踪和管理缺陷。
7. **回归测试和冒烟测试**：执行回归测试，确保新功能的加入不会影响原有功能；同时进行冒烟测试，确保系统的基本功能正常。
8. **测试进度跟踪**：在测试过程中进行进度跟踪，确保测试按时进行。
9. **缺陷修复验证**：测试人员在开发人员修复缺陷后，进行验证测试，确认缺陷已被解决。
10. **测试总结报告**：完成测试后，编写测试报告，总结测试过程、缺陷情况以及测试覆盖率等信息。

‍

‍

### 如何定位问题/判断BUG是前端还是后端

‍

定位问题和判断BUG是前端还是后端，首先可以通过以下几步来分析：

1. **检查浏览器控制台和网络请求**：如果是前端问题，可以在浏览器的开发者工具中查看控制台日志，检查是否有JavaScript错误或者请求失败。检查网络请求（Network tab）是否返回正确的数据。
2. **验证后端响应**：如果前端正常发起请求但未能显示数据，可能是后端问题。可以通过Postman等工具直接测试后端接口，验证响应是否正确。
3. **排除依赖关系**：如果问题出现在界面渲染或用户交互时，检查前端代码是否正确处理数据。如果后端响应异常或缺失数据，则是后端问题。
4. **日志与错误码**：通过后端的日志或API返回的错误码来判断问题是出现在后端处理（如数据库查询、数据处理）还是前端的展示。

‍

## 用例设计

‍

‍

### 测试用例几大要素

标识符，测试内容，输入条件，预期结果，测试环境信息，与其他测试用例的依赖关系，测试用例需要被开发、审阅、使用、维护和保存。

‍

‍

### 设计测试用例的主要方法

1）等价类划分

2）边界值分析法

3）因果图法

4）场景法

‍

#### 1-等价类划分

等价类是指某个输入域的子集合

在该子集合中,各个输入数据对于揭露程序中的错误都是等效的

并合理地假定:测试某等价类的代表值就等于对这一类其它值的测试. 因此,可以把全部输入数据合理划分为若干等价类,在每一个等价类中取一个数据作为测试的输入条件,就可以用少量代表性的测试数据.取得较好的测试结果

等价类划分可有两种不同的情况:有效等价类和无效等价类.

‍

‍

#### 2-边界值分析法

边界值分析方法是对等价类划分方法的补充

大量的错误是发生在输入或输出范围的边界上, 而不是发生在输入输出范围的内部.因此针对各种边界情况设计测试用例,可以查出更多的错误

使用边界值分析方法设计测试用例,首先应确定边界情况.

通常输入和输出等价类的边界,就是应着重测试的边界情况. 应当选取正好等于刚刚大于或刚刚小于边界的值作为测试数据, 而不是选取等价类中的典型值或任意值作为测试数据

‍

‍

#### 3-错误推测法

基于经验和直觉推测程序中所有可能存在的各种错误, 从而有针对性的设计测试用例的方法.

基本思想: 列举出程序中所有可能有的错误和容易发生错误的特殊情况,根据他们选择测试用例

‍

‍

#### 4-因果图方法

前面介绍的等价类划分方法和边界值分析方法, 都是着重考虑输入条件, 但未考虑输入条件之间的联系, 相互组合等

‍

考虑输入条件之间的相互组合,可能会产生一些新的情况

但要检查输入条件的组合不是一件容易的事情, 即使把所有输入条件划分成等价类,他们之间的组合情况也相当多

因此必须考虑采用一种适合于描述对于多种条件的组合, 相应产生多个动作的形式来考虑设计测试用例-这就需要利用因果图(逻辑模型)

因果图方法最终生成的就是判定表, 它适合于检查程序输入条件的各种组合情况.

‍

‍

#### 5-正交表分析法

有时候，可能因为大量的参数的组合而引起测试用例数量上的激增，同时，这些测试用例并没有明显的优先级上的差距，而测试人员又无法完成这么多数量的测试，就可以通过正交表来进行缩减一些用例，从而达到尽量少的用例覆盖尽量大的范围的可能性。

‍

‍

#### 6-场景分析方法

指根据用户场景来模拟用户的操作步骤，这个比较类似因果图，但是可能执行的深度和可行性更好。

‍

‍

‍

### 测试用例包含哪些要素, 如何优化?

‍

测试用例通常包含以下要素：

1. **测试用例编号**：唯一标识测试用例的编号。
2. **测试用例名称**：简要描述测试用例的名称。
3. **前置条件**：执行测试用例前需要满足的条件。
4. **测试步骤**：详细描述执行测试的步骤。
5. **预期结果**：执行测试步骤后预期的结果。
6. **实际结果**：执行测试步骤后实际的结果（执行后填写）。
7. **优先级**：测试用例的优先级（高、中、低）。
8. **测试数据**：执行测试所需的数据。
9. **备注**：其他需要说明的事项。

‍

‍

#### 示例

```plaintext
测试用例编号: TC001
测试用例名称: 登录功能测试
前置条件: 用户已注册并激活账号
测试步骤:
  1. 打开登录页面
  2. 输入有效的用户名和密码
  3. 点击登录按钮
预期结果: 用户成功登录并跳转到主页
实际结果: （执行后填写）
优先级: 高
测试数据: 用户名: testuser, 密码: testpass
备注: 无
```

‍

‍

#### 优化测试用例的方法

1. **明确和简洁**：确保测试用例描述清晰、简洁，避免歧义。
2. **覆盖全面**：确保测试用例覆盖所有功能和边界情况。
3. **可重复性**：测试用例应具有可重复性，确保每次执行结果一致。
4. **独立性**：测试用例应独立，避免相互依赖。
5. **自动化**：尽量将测试用例自动化，提高执行效率。
6. **定期审查**：定期审查和更新测试用例，确保其有效性和完整性。

‍

‍

‍

‍

### Xmind以外的用例管理工具

**常见工具**：

* **TestRail**：一款功能全面的测试管理工具，支持测试用例管理、执行、报告及协作。
* **Zephyr**：JIRA 插件或独立工具，集成性好、易于与缺陷管理、持续集成配合。
* **qTest**：提供全面的测试管理解决方案，支持需求追踪和缺陷管理。
* **TestLink**：开源的测试管理平台，适合中小团队使用。
* **PractiTest**：提供集中化的测试管理及实时分析功能。

‍

**其他选择**：JIRA（结合 Xray 或 Zephyr 插件）、HP ALM/Quality Center 等

‍

‍

### 如何去保证写出来的代码每个分支能被测试到（也就是如何优化大量if else)

* **测试覆盖率**：使用单元测试和集成测试来确保每个条件分支都被执行，可以借助覆盖率工具（如 JaCoCo）监控覆盖情况。
* **设计优化**：

  * **重构代码**：减少复杂的 if/else 嵌套，采用策略模式、状态模式、或多态来优化代码结构，使分支逻辑更清晰、易于测试。
  * **TDD 开发**：采用测试驱动开发，先写测试用例确保每个分支都能被触发，然后编写代码满足所有测试用例。
* **自动化测试**：构建完善的自动化测试套件，确保每次代码修改后所有条件分支均通过回归测试。

‍

‍

‍

### 正交表测试用例设计方法

正交表测试用例设计方法是一种通过数学模型（正交表）来优化测试用例设计的技术，主要特点如下：

‍

1. **减少测试用例数量**：正交表能够在保证覆盖所有可能组合的前提下，显著减少测试用例的数量，避免了传统的穷举法生成大量冗余的测试用例。
2. **提高测试效率**：通过正交表，测试人员可以高效地组合不同的输入条件，以更少的测试用例覆盖更广泛的输入情况，从而提高测试的效率。
3. **保证组合覆盖**：正交表确保了每个因素的不同取值组合都能被测试到，能够有效覆盖潜在的交互问题，特别是在多因素、多选项的场景下。
4. **简单易懂**：正交表是一种简明直观的测试设计方法，测试人员不需要掌握复杂的数学知识即可使用。通过交叉组合的方式设计测试用例，使得不同因素的组合能够全面覆盖。
5. **适用于多因素测试**：正交表特别适用于因素较多且每个因素有多个取值的测试场景，如产品配置、用户设置等。

‍

‍

### 常见的测试用例的边界？

在测试用例设计中，边界通常指的是输入数据的边界条件或测试范围的边界。常见的测试用例的边界包括：

1. **输入数据边界**：指输入值的上下限值，通常是有效的最小值和最大值。

    * 例如：对于一个年龄字段，边界值可能是0（最小值）和120（最大值），需要测试这两个边界及其周围的值（如-1、1、119、121）来验证系统的处理能力。
2. **数组/集合边界**：对于数组或集合，边界包括数组的第一个元素、最后一个元素以及空数组。

    * 例如：对于一个数组，测试用例需要验证第一个元素和最后一个元素的正确性，以及数组为空时的处理。
3. **字符/字符串边界**：对字符串的长度及其内容进行边界测试，检查系统对极限长度字符串的处理。

    * 例如：对于一个用户名字段，测试用户名长度的最大值、最小值以及超出范围的情况。
4. **时间边界**：测试与时间相关的功能时，应该考虑时区变化、闰年、月份的最大天数等边界条件。

    * 例如：测试一个日期选择器的边界值，检查是否能正确处理2月29日（闰年）或12月31日。
5. **负值/无效值边界**：测试系统如何处理负值或超出预期范围的无效输入。

    * 例如：一个系统要求用户输入的年龄必须大于等于0，测试用例需要验证是否能正确处理负数输入。

‍

‍

### 什么是状态迁移法

状态迁移法（State Transition Testing）是一种基于系统状态的测试方法，旨在通过验证系统在不同状态之间的迁移是否正确来发现缺陷。在这种方法中，系统的行为被建模为一系列状态，测试人员通过定义状态间的转换规则来设计测试用例，确保系统在状态转换时符合预期。

状态迁移法常用于有明确状态定义的系统，比如操作系统、工作流引擎、ATM机等，通过验证系统在不同输入下的状态转变是否符合预期，确保没有逻辑错误。

‍

‍

### 什么是判定表法

判定表法（Decision Table Testing）是一种系统化的测试用例设计方法，主要用于处理具有复杂条件和多个条件组合的系统。通过判定表将所有可能的条件组合列出，并映射到相应的输出结果，从而生成全面的测试用例。

判定表法将系统的输入条件、条件组合与输出结果用表格的形式表示，每一行代表一种条件组合，每一列代表一个输入条件，通过这种方式，我们可以清晰地看到所有输入条件的组合情况及其对应的输出，从而确保测试的完整性和覆盖性。

‍

‍

### 什么是因果图法

因果图法（Cause-Effect Graphing）是一种基于逻辑推理的测试用例设计方法，它通过将输入条件（因）与预期输出结果（果）之间的因果关系进行图形化表示，帮助测试人员识别可能的测试场景。因果图通过逻辑关系（如“与”运算、“或”运算）将输入条件和输出结果的关系构建成一个图形模型，从而生成可执行的测试用例。

因果图法的核心思想是，通过分析系统的规则和条件之间的关系，构建因果图，找出所有可能的组合条件和结果，帮助我们生成全面而高效的测试用例。

‍

### 简述如何保障测试用例可以无限运行？如何解决测试数据的问题 ？

‍

要保障测试用例可以无限运行，主要考虑以下几个方面：

1. **测试用例的独立性**：每个测试用例都应该是独立的，确保一个用例的执行不会依赖于其他用例的执行结果。这可以通过清理测试数据、恢复环境来保证每次执行时环境一致。
2. **自动化环境的稳定性**：使用持续集成（CI）工具（如Jenkins）来定期执行测试，并确保自动化测试环境的稳定性。测试环境的管理与配置要确保每次测试时都使用相同的配置。
3. **异常处理和日志记录**：添加详细的异常处理和日志记录机制，以便当用例失败时，能够及时捕捉并分析失败原因，保证不会影响其他测试用例的执行。
4. **定期清理测试数据**：设计合适的数据清理机制，确保每次测试前环境中的数据保持一致，防止数据污染。

‍

解决测试数据的问题，可以通过以下方式：

1. **数据生成工具**：使用自动化脚本或数据生成工具动态生成测试数据，确保数据的多样性和完整性。
2. **模拟数据**：对于某些场景，可以使用模拟数据（如mock、stub）来代替真实数据进行测试。
3. **数据库回滚**：通过在每次测试前备份数据库，测试完成后通过回滚来恢复数据，确保数据的一致性。

‍

‍

‍

## 单元测试

‍

‍

### 单测原则

好的单元测试必须遵守**AIR原则**

‍

说明:单元测试在线上运行时，感觉像空气(AIR)一样感觉不到，但在测试质量的保障上，却是非常关键的。好的单元测试宏观上来说，具有自动化、独立性、可重复执行的特点。

* A: Automatic (自动化)
* l: Independent(独立性)
* R: Repeatable (可重复)

‍

#### 【强制】单元测试应该是全自动执行的，并且非交互式的。测试用例通常是被定期重复执行的

完全自动化才有意义。输出结果需要人工检查的测试不是一个好的单元测试。

不准使用System.out来进行**人肉验证**，单元测试必须使用assert来验证。

‍

#### 【强制】保持单元测试的独立性。为了保证单元测试稳定可靠且便于维护，单元测试用例之间决不能互相调用，也不能依赖执行的先后次序。

‍

#### 【强制】单元测试是可以重复执行的，不能受到外界环境的影响。

说明:单元测试通常会被放到持续集成中，每次有代码push时单元测试都会被执行。如果单测对外部环境（网络、服务、中间件等）有依赖，容易导致持续集成机制的不可用。

正例:为了不受外界环境影响，要求设计代码时就把SUT(System under test)的依赖改成注入，在测试时用Spring这样的DI框架注入一个本地(内存)实现或者Mock 实现。

‍

#### 【强制】对于单元测试，要保证测试粒度足够小，有助于精确定位问题。单测粒度至多是类级别，一般是方法级别。

‍

‍

‍

## 接口测试

‍

### 什么是 API 测试 (综合)

API 测试是对应用程序接口（API）的验证过程，目的是确保接口按照预期的功能、性能和安全要求正常工作。API测试主要涉及检查API的功能、数据传输、错误处理和安全性。常见的测试包括功能测试、负载测试和安全测试等。

‍

‍

### API 测试中使用的协议有哪些？

在API测试中，常见的协议主要有以下几种：

1. **HTTP/HTTPS**：最常用的协议，用于传输网页数据和API请求，HTTP是明文传输，HTTPS是加密传输。
2. **SOAP (Simple Object Access Protocol)** ：一种基于XML的协议，用于交换结构化信息，通常用于Web服务中。
3. **REST (Representational State Transfer)** ：一种基于HTTP的架构风格，通常用于创建和操作Web服务，使用HTTP方法如GET、POST、PUT、DELETE等。
4. **gRPC**：一种高效的开源RPC（远程过程调用）框架，使用HTTP/2作为传输协议，支持多种语言并具有高性能的特点。
5. **WebSocket**：一种全双工通信协议，通常用于实时通信应用，如即时聊天、股票报价等。
6. **GraphQL**：一种用于API的查询语言，可以根据客户端的需求获取指定的数据。

‍

‍

### 常见的API测试工具

常见的API测试工具包括以下几种：

1. **Postman**：一个广泛使用的API测试工具，支持发送各种类型的HTTP请求，验证响应，自动化测试等功能。其界面直观，适合开发人员和测试人员使用。
2. **SoapUI**：一个专门用于测试SOAP和REST API的工具，支持Web服务的功能测试、安全测试、负载测试等，功能强大，适合复杂的API测试。
3. **JMeter**：主要用于性能测试的开源工具，但也可以用于API的功能和负载测试。支持多协议测试，如HTTP、SOAP、REST等。
4. **Rest Assured**：一个Java库，专门用于REST API的自动化测试，适合开发者进行API测试自动化集成。
5. **Insomnia**：一个功能强大的REST客户端，支持HTTP请求的创建和响应的验证，适用于调试和测试RESTful API。
6. **Katalon Studio**：一个集成化的自动化测试工具，支持API、Web、移动等多种测试场景，具有图形化界面和强大的脚本能力，适合全面的API自动化测试。
7. **Swagger**：Swagger工具集提供了强大的API文档生成和接口测试功能，开发人员可以在Swagger UI中直接与API进行交互。

‍

‍

### 接口测试经常会遇见的问题

接口测试中常见的问题包括：

1. **接口文档不完善或不一致**：接口文档可能存在缺失或描述不清晰的情况，导致测试人员无法准确理解接口的功能或使用方法。
2. **请求参数错误**：发送的请求参数不符合接口要求，可能是格式错误、缺少必填字段或参数类型不匹配。
3. **接口返回数据不一致**：接口返回的数据可能与预期不一致，例如缺少字段、数据格式错误、返回值错误等。
4. **身份验证与权限问题**：接口请求需要授权（如token、API Key等），若授权信息不正确或未提供，可能会导致接口无法访问。
5. **性能问题**：在高并发请求时，接口响应时间较长，或在负载较大时出现错误，未经过压力测试的接口可能无法在生产环境中稳定运行。
6. **环境问题**：接口在不同环境（开发、测试、生产等）中表现不一致，可能是由于配置不同或环境资源限制导致的。
7. **错误处理不足**：接口没有良好的错误处理机制，无法返回详细的错误信息，或者错误信息不够明确，导致无法快速定位问题。

‍

‍

‍

## 性能测试

‍

‍

### 性能测试工作的完整过程

‍

1. **需求分析与目标设定**：  
    我首先与项目经理、开发团队和运维团队沟通，明确性能测试的目标和需求。例如，测试目标可能包括：系统在最大并发用户数下的响应时间、吞吐量和稳定性要求。
2. **选择合适的测试工具**：  
    根据项目的需求，我选择了合适的性能测试工具。在之前的项目中，我主要使用了JMeter进行负载和压力测试。对于一些复杂的场景，也使用了LoadRunner进行性能评估。
3. **设计测试用例与脚本开发**：  
    根据系统的功能设计测试用例，确保覆盖常见的用户操作路径。我设计了多个负载场景，如登录、浏览商品、下单、支付等操作，并编写了相应的JMeter脚本模拟多个用户的并发访问。
4. **搭建测试环境**：  
    我与运维团队协作，搭建了与生产环境相似的测试环境。包括配置多个Web服务器、数据库服务器以及相关的负载均衡和网络带宽配置，确保测试环境能够模拟实际的生产环境负载。
5. **执行测试与监控**：  
    在执行测试时，我逐步增加负载并模拟多个并发用户访问系统。期间，我密切监控了系统的CPU、内存使用情况、磁盘I/O、网络带宽等关键指标，确保数据的准确性。
6. **分析测试结果**：  
    测试完成后，我对采集的数据进行了详细分析，重点分析了响应时间、吞吐量、资源消耗等指标。通过比对不同负载下的表现，我识别出了系统在某些特定操作（如数据库查询）中的性能瓶颈。
7. **优化建议与改进**：  
    根据测试结果，我向开发团队和运维团队提出了针对性的优化建议。例如，针对数据库性能瓶颈，建议优化SQL查询并增加数据库索引；对于内存使用过高的问题，建议进行内存管理优化。

‍

> 1. **需求分析**：了解系统的性能需求，明确测试目标和预期结果。
> 2. **测试计划制定**：根据需求分析，制定详细的测试计划，包括测试的类型、测试工具、测试环境等。
> 3. **脚本设计与开发**：根据测试需求，编写性能测试脚本，模拟用户行为。
> 4. **环境搭建与配置**：准备好测试所需的硬件、软件环境，确保与生产环境相似。
> 5. **执行测试**：按照预定的计划和脚本，执行性能测试。
> 6. **结果分析**：收集测试数据，分析系统的性能表现，找出瓶颈或异常。
> 7. **报告编写与优化建议**：根据测试结果编写测试报告，并提出系统优化建议。

‍

### 性能测试思路

‍

1. **确定测试目标**：

    * 明确性能测试的目标，例如响应时间、吞吐量、资源利用率等。
2. **选择测试工具**：

    * 选择合适的性能测试工具，如 Apache JMeter、LoadRunner、Gatling 等。
3. **设计测试场景**：

    * 根据实际使用情况设计测试场景，包括并发用户数、请求类型、数据量等。
4. **准备测试环境**：

    * 搭建与生产环境相似的测试环境，确保测试结果的可靠性。
5. **编写测试脚本**：

    * 使用测试工具编写测试脚本，模拟用户行为和负载。
6. **执行测试**：

    * 逐步增加负载，执行测试，监控系统性能指标。
7. **分析测试结果**：

    * 收集和分析测试结果，找出性能瓶颈和问题。
8. **优化和调整**：

    * 根据测试结果进行系统优化和调整，提升性能。
9. **重复测试**：

    * 重新执行测试，验证优化效果，确保性能达到预期目标。

‍

‍

### 性能测试的性能指标有哪些

‍

1. **响应时间**：系统对请求作出响应所需的时间。
2. **吞吐量**：单位时间内系统处理的请求数量。
3. **并发用户数**：同时访问系统的用户数量。
4. **资源利用率**：系统资源（如 CPU、内存、磁盘 I/O 等）的使用情况。
5. **错误率**：在测试过程中发生的错误请求的比例。
6. **延迟**：请求在系统中传输和处理的时间。
7. **事务处理时间**：特定事务从开始到结束所需的时间。

‍

这些指标可以帮助评估系统在不同负载条件下的性能表现和稳定性。

‍

‍

### 性能测试包含的方法有哪些

性能测试主要包含以下几种方法：

1. **负载测试**：通过模拟一定数量的用户请求，测试系统在一定负载下的表现，确保系统在预期负载下能正常运行。
2. **压力测试**：测试系统在超出正常负载的情况下的表现，目的是找出系统的最大承载能力，通常会导致系统崩溃。
3. **稳定性测试**：长时间运行系统，检测在长时间负载下系统的稳定性和性能是否会下降。
4. **容量测试**：评估系统在不同负载下的性能表现，帮助判断系统是否能够承载预期的用户数量或业务量。
5. **基准测试**：对系统的性能进行基准评估，与行业标准或历史数据对比，衡量系统性能的优劣。

‍

‍

### 性能测试里面，固定tps模式和虚拟用户模式的区别

‍

1. **固定TPS模式**

    * **定义**：TPS（Transactions Per Second）模式是指在测试过程中保持每秒事务数（请求数）的恒定。
    * **特点**：这种模式下，系统会根据设定的TPS值来生成请求，确保每秒钟的请求数保持不变。
    * **适用场景**：适用于需要测试系统在特定TPS下的性能表现，例如评估系统在高并发请求下的稳定性和响应时间。
2. **虚拟用户模式**

    * **定义**：虚拟用户模式是指在测试过程中模拟一定数量的并发用户，每个用户按照设定的行为模式进行操作。
    * **特点**：这种模式下，系统会根据设定的虚拟用户数来生成请求，用户的行为可以是随机的或预定义的。
    * **适用场景**：适用于模拟真实用户的操作场景，评估系统在不同并发用户数下的性能表现，例如登录、浏览、提交等操作。

‍

‍

### 性能测试中TPS上不去的几种原因

如果系统的**TPS**（Transactions Per Second，事务每秒）上不去，通常可能是由以下几种原因引起的：

1. **硬件资源瓶颈**：

    * 如果系统的服务器硬件资源（如CPU、内存、磁盘IO或网络带宽）不足，就可能成为性能瓶颈，限制系统的吞吐量。高并发请求会占用大量的硬件资源，导致TPS无法提升。
2. **数据库瓶颈**：

    * 数据库操作通常是影响性能的关键因素之一。如果数据库的查询、写入或事务处理不高效，或者数据库的连接池设置不合理，都会限制TPS。高并发时，数据库的锁竞争、索引不当、慢查询等都可能导致TPS的瓶颈。
3. **应用程序代码瓶颈**：

    * 如果应用程序的业务逻辑处理不高效，代码中存在不必要的延迟或锁竞争，都会影响TPS。比如，大量的同步调用、阻塞操作、重复的计算等都可能成为性能瓶颈。
4. **网络延迟或带宽不足**：

    * 如果网络延迟较高或带宽不足，可能导致请求和响应的传输时间过长，影响TPS。高并发时，网络不稳定或速度慢也会影响系统的吞吐能力。
5. **负载均衡配置不合理**：

    * 如果负载均衡器没有配置好，可能导致某些服务器节点的请求过多，而其他节点空闲，从而无法充分利用所有的服务器资源，导致TPS无法提升。

‍

‍

### 解释什么是集合点？设置集合点有什么意义？

**集合点**（Checkpoint）是性能测试中一种设置在特定位置的机制，用于控制负载测试中的并发用户数和请求的执行方式。通过设置集合点，可以在一定的时间点上暂停所有虚拟用户的执行，直到所有虚拟用户都到达该点为止，然后再一起继续执行后续的操作。集合点一般用于模拟用户在实际场景中的行为，比如多个用户同时提交订单或请求时，确保负载的同步性。

‍

**设置集合点的意义：**

1. **模拟实际用户行为**：  
    集合点能够让多个虚拟用户在同一时刻执行某一操作，模拟实际场景中多个用户并发访问某一功能或页面的情况。例如，多个用户同时访问结算页面进行支付，这种场景通过集合点来模拟。
2. **控制负载与并发**：  
    在性能测试中，集合点可以帮助设置在同一时刻发起请求的虚拟用户数量，这对于模拟高并发场景非常重要。例如，测试一个高并发的支付系统时，所有的虚拟用户可能需要同时点击“提交支付”按钮，集合点确保这些操作同步进行。
3. **检查系统在高并发下的表现**：  
    通过设置集合点，可以测试系统在突发性负载下的表现。比如，测试在系统需要同时处理多个请求时的响应时间、吞吐量、稳定性等。
4. **提高测试的准确性与可控性**：  
    集合点使得测试过程更为可控和一致。它确保了虚拟用户在执行请求时的同步性，有助于减少测试中由于用户间操作不一致导致的干扰因素，使得测试结果更加稳定和可信。

‍

### 性能测试里面如何确定并发用户数

‍

1. **分析业务需求和实际情况**：

    * 了解系统的预期负载和实际使用情况。例如，系统的日活跃用户数、每个用户的请求频率等。这可以通过历史数据、用户行为分析或者需求文档来推测。
2. **用户场景建模**：

    * 通过分析系统中不同类型用户的行为，设计出实际的并发用户模型。比如，不同的用户角色（如普通用户、管理员）有不同的操作频率和资源消耗。
3. **使用并发用户数的计算公式**：

    * 基于系统的日活跃用户数和每个用户的访问频率，推算出并发用户数。例如，如果预期每天有10000个活跃用户，每个用户每小时访问网站5次，计算出该系统的并发用户数。
4. **模拟负载测试**：

    * 从低并发逐步增加，进行负载测试，观察系统在不同并发数下的性能，最终确定实际能够承载的并发用户数。
5. **参考行业标准**：

    * 如果没有具体的业务数据，可以参考类似系统的行业标准或竞品的性能指标，作为初步的并发用户数的参考。

‍

‍

### 如何确定系统最大负载

通常，系统的最大负载是通过**压力测试**、**负载测试**和**性能基准测试**来确定的。以下是确定系统最大负载的常见方法：

1. **负载测试**：

    * 通过模拟逐渐增加的并发用户访问，测试系统在各种负载条件下的表现，最终找到系统开始变得不稳定或响应时间显著增加的负载阈值。
2. **压力测试**：

    * 在负载测试的基础上，进一步增加并发量，直至系统达到崩溃的临界点。此时，系统的最大负载即为其承受的极限负载。
3. **监控系统指标**：

    * 在负载和压力测试过程中，通过实时监控服务器的CPU、内存、磁盘和网络等资源，结合这些指标分析系统的瓶颈，并推测系统能够处理的最大负载。
4. **响应时间分析**：

    * 通过分析系统在不同负载下的响应时间，判断系统在达到某个负载时，响应时间是否超出了可接受范围。响应时间过长通常表示系统的最大负载已被接近或超过。
5. **吞吐量分析**：

    * 吞吐量指单位时间内系统能够处理的请求数量。通过分析吞吐量和负载的关系，能够找出系统的性能瓶颈和最大负载。

‍

### 什么是全链路压测

**全链路压测**（End-to-End Stress Testing）是指通过模拟真实用户的访问场景，全面测试系统在高并发、高负载情况下的表现，特别是对整个系统架构链路的性能进行测试。全链路压测涵盖了从用户请求开始，到服务层、数据库层，直到外部依赖服务的各个环节。其目的是确保在高并发的情况下，整个系统链路能够正常工作，帮助发现性能瓶颈并优化系统。

‍

1. **全链路压测的定义**：  
    全链路压测是对整个应用系统（包括前端、后端、数据库、第三方服务等）的压力进行全面测试。与传统的单点压力测试不同，全链路压测会覆盖整个请求链路，模拟用户在高并发、高负载情况下的真实操作场景。它的目的是模拟系统的实际使用情况，帮助评估和优化系统在生产环境下的处理能力，避免因为某一环节的瓶颈导致整个系统崩溃。
2. **为什么要做全链路压测**：

    * **多环节协作**：现代应用系统往往是由多个微服务或不同的组件构成，性能问题可能在任何一个环节出现，单纯的压力测试无法全面反映系统的负载能力。
    * **外部依赖**：许多系统依赖于外部的第三方服务（如支付、短信、日志服务等），这些依赖的性能也需要被考虑在内。如果第三方服务的性能不达标，可能会影响整个系统的响应能力。
    * **系统的复杂性**：随着系统架构的复杂化，性能瓶颈可能分布在多个层次和环节，传统的局部性能测试可能无法发现跨层的性能问题，而全链路压测能帮助揭示这些问题。
3. **全链路压测的流程**：

    * **需求和目标设定**：首先，明确全链路压测的目标和需求，例如并发用户数、响应时间要求、最大负载等。
    * **测试环境准备**：模拟真实环境，包括应用服务器、数据库、缓存、外部服务等都应该参与到测试中。此时，可能需要将一些生产环境中的配置复制到测试环境。
    * **压力模拟**：通过压力测试工具模拟大量用户并发访问系统，逐步增加请求量，查看系统在不同负载下的表现。
    * **性能监控和数据收集**：在测试过程中，需要监控系统各个组件的性能，包括服务器资源（如CPU、内存、磁盘I/O等）、应用程序的响应时间、数据库性能、外部服务的响应等。
    * **分析瓶颈**：通过测试结果来分析系统中存在的瓶颈，并确定是硬件资源不足、代码性能问题，还是外部依赖服务的问题。
    * **优化和调优**：根据测试结果进行优化，可能包括代码优化、数据库优化、增加缓存、提升硬件资源或改进外部服务的调用等。
4. **全链路压测的工具和方法**：

    * **压力测试工具**：例如 JMeter、Gatling、Locust 等，可以用来模拟高并发用户访问。
    * **监控工具**：如 Prometheus、Grafana、Zabbix、New Relic 等，可以用来实时监控系统的性能。
    * **日志分析**：结合 ELK Stack（Elasticsearch、Logstash、Kibana）等工具分析系统日志，快速定位性能瓶颈。
5. **实际应用中的挑战**：

    * **数据的一致性**：在模拟真实用户场景时，数据的一致性可能成为一个问题，尤其是当多个服务共享同一份数据时。需要确保在高并发的压力下数据不会丢失或产生不一致。
    * **外部服务依赖**：许多系统依赖外部API或第三方服务，而这些服务的性能通常不可控。因此，需要特别关注外部依赖服务的稳定性。
    * **测试环境和生产环境差异**：测试环境与生产环境之间的差异可能会影响测试的准确性。需要尽量确保两者的配置、数据和负载条件一致，以提高测试结果的代表性。

‍

‍

## 自动化测试

‍

### 什么是自动化测试

自动化测试是指使用自动化工具和脚本，代替人工执行软件测试的过程。它通过编写测试脚本自动执行测试用例、验证结果，能够提高测试效率、减少人为错误、增加测试的覆盖率，并帮助开发团队更早地发现问题，尤其适用于回归测试、接口测试和大规模测试场景。

‍

自动化测试是软件开发中不可或缺的一部分，尤其在现代的持续集成和敏捷开发过程中，自动化测试的价值更加突出。它主要通过使用自动化工具、框架和脚本，模拟用户操作，自动验证系统行为，并根据预期结果报告测试通过或失败的情况。自动化测试通常包括以下几个关键要素：

1. **自动化工具**  
    自动化测试工具用于编写、执行和管理测试用例。常见的自动化测试工具有：

    * **Selenium**：主要用于Web应用程序的UI自动化测试。
    * **JUnit、TestNG**：用于Java编程语言的单元测试，常与Selenium结合使用。
    * **Appium**：用于移动端应用程序的自动化测试。
    * **Postman**：用于接口测试，支持自动化执行API请求。

    这些工具能够帮助测试人员和开发人员快速执行测试，并得到自动化报告。
2. **自动化脚本**  
    自动化测试需要编写脚本，脚本是实现自动化测试的核心。通过编写脚本，测试人员可以自动化地模拟各种用户操作（如点击按钮、输入文本等），并对应用的功能进行验证。脚本可以编写成不同的语言，如Java、Python、JavaScript等。
3. **测试用例**  
    自动化测试的关键是测试用例，测试用例定义了测试的场景和期望的结果。在自动化测试中，测试用例不仅仅依赖人工执行，还可以通过脚本自动执行。这些测试用例可能包括功能测试、性能测试、回归测试、负载测试等。
4. **回归测试与重复测试**  
    自动化测试特别适合回归测试和重复测试。回归测试是指每次代码修改后，测试现有功能是否依然正常。由于回归测试通常需要多次执行相同的测试用例，自动化测试能够大幅提高效率，减少手动重复工作的压力。
5. **持续集成与自动化测试结合**  
    在持续集成（CI）的环境下，自动化测试与代码版本管理、构建工具等配合使用，每次开发人员提交代码时，自动化测试都会被触发，确保每次新版本的代码都能通过测试。这种快速反馈机制有助于在早期阶段发现并修复问题。
6. **报告与分析**  
    自动化测试工具通常会生成测试报告，报告中包含了测试的执行结果、失败的用例、错误信息等。通过这些报告，开发人员和测试人员可以及时了解哪些功能出现了问题，并迅速采取措施。
7. **降低成本**  
    尽管自动化测试的初期投资较高（如购买工具、编写脚本、配置环境等），但长期来看，它能够减少人工测试的时间和成本，尤其在回归测试、性能测试等重复性高的场景中，可以显著降低测试成本。
8. **提高测试覆盖率和准确性**  
    自动化测试能够覆盖更广泛的测试场景，尤其是一些边界条件、极限值等测试场景，这些往往很难通过手动测试覆盖。自动化测试能够确保测试不受人工限制，提高测试的全面性和准确性。

‍

‍

### 如何设计自动化测试用例

1. **明确测试目标**：首先要明确自动化测试的目标，是否是回归测试、功能测试还是性能测试。不同的目标决定了测试用例的设计侧重点。
2. **选择合适的测试工具和框架**：根据项目需求，选择合适的自动化测试工具（如Selenium、Appium、TestNG等），并设计合适的框架结构，如数据驱动框架、关键字驱动框架等。

    选择适合项目需求的测试工具和框架是非常重要的。常见的测试工具包括Selenium、Appium、Postman等。框架方面，可以选择**数据驱动框架**（Data-Driven Framework）、**关键字驱动框架**（Keyword-Driven Framework）、**行为驱动开发**（BDD）等。

    * **举个例子**：如果我们选择Selenium进行UI自动化测试，可以搭配TestNG进行测试管理，使用Maven作为构建工具，形成一个完善的自动化测试框架。
3. **测试用例的可维护性**：确保用例具有较高的可维护性，采用**Page Object Model**（POM）等设计模式将页面元素和操作封装，提高代码复用性和可维护性。
4. **覆盖全面**：确保测试用例能够覆盖所有的功能点，并考虑不同的输入数据和边界情况，设计正向和负向的测试用例。
5. **易于执行和调试**：确保测试用例执行流程简洁清晰，遇到问题时能提供详细的错误信息，便于定位和解决问题。
6. **考虑测试数据和环境**：合理管理测试数据，确保自动化测试脚本能够与不同的测试数据配合使用，避免数据依赖问题。

‍

‍

‍

### 自动化测试作用

1. **提高测试效率**：自动化测试可以快速执行大量测试用例，减少手动测试的时间和人力成本。
2. **提高测试覆盖率**：自动化测试可以覆盖更多的测试场景和边界条件，确保软件的各个方面都得到充分测试。
3. **提高测试准确性**：自动化测试可以避免人为错误，确保测试结果的准确性和一致性。
4. **支持持续集成和持续交付**：自动化测试可以集成到CI/CD管道中，确保每次代码变更都经过充分测试，减少引入新缺陷的风险。
5. **提高回归测试效率**：自动化测试可以快速执行回归测试，确保新代码不会破坏已有功能。
6. **提高开发效率**：自动化测试可以在开发过程中及时发现和修复缺陷，减少后期修复成本。

‍

#### 自动化测试的价值在哪里

自动化测试的价值在于提高测试效率、增强测试覆盖率、减少人为错误以及支持持续集成。它可以自动执行重复性测试任务，快速反馈软件问题，尤其在大规模的回归测试和复杂的功能验证中非常有效。通过自动化测试，团队可以节省时间、降低成本，并提高软件的质量。

‍

### 自动化测试平台实现原理

1. **测试用例管理**：管理和组织测试用例，包括创建、编辑、删除和分类等功能。
2. **测试执行**：调度和执行测试用例，收集测试结果。
3. **测试报告**：生成和展示测试报告，帮助分析测试结果。
4. **持续集成**：与持续集成工具（如 Jenkins）集成，实现自动化测试的持续执行。
5. **环境管理**：管理测试环境，包括配置和维护测试环境。

‍

‍

‍

### 自动化测试框架？介绍下（Selenium，pytest）

‍

Selenium

Selenium 是一个用于 Web 应用程序自动化测试的开源工具。它支持多种浏览器（如 Chrome、Firefox、Safari 等）和多种编程语言（如 Java、Python、C# 等）。Selenium 主要包括以下组件：

1. **Selenium WebDriver**：提供跨浏览器的自动化 API，允许编写直接与浏览器交互的测试脚本。
2. **Selenium IDE**：一个集成开发环境，用于录制和回放测试脚本，适合快速创建测试用例。
3. **Selenium Grid**：允许在分布式环境中并行运行测试，支持跨多个机器和浏览器的测试执行。

‍

pytest

pytest 是一个用于 Python 的测试框架，支持简单的单元测试和复杂的功能测试。它具有以下特点：

1. **简单易用**：使用简单的函数和断言语法编写测试用例。
2. **强大的插件系统**：支持多种插件，如 pytest-cov（代码覆盖率）、pytest-xdist（并行测试）等。
3. **自动发现测试**：自动发现以 `test_`​ 开头或结尾的测试文件和测试函数。

‍

‍

‍

### Java实现ui自动化的思路

在 Java 中实现 UI 自动化测试的常见思路如下：

‍

1. **选择自动化测试工具**：

    * 常用的工具有 Selenium、Appium 等。
2. **设置测试环境**：

    * 配置 WebDriver（如 ChromeDriver、GeckoDriver 等）。
    * 配置测试框架（如 JUnit、TestNG）。
3. **编写测试脚本**：

    * 使用 WebDriver API 编写测试脚本，模拟用户操作。
4. **执行测试**：

    * 运行测试脚本，捕获和报告测试结果。
5. **分析测试结果**：

    * 分析测试报告，找出问题并修复。

‍

### 一个接口已经有自动化用例了，但是它又被修改了，那直接用以前的回归还是说会再修改之前的自动化用例

‍

* **评估修改范围**：

  * 如果接口变更是非破坏性（例如增加了新字段，或调整了返回格式但业务逻辑不变），可以适当扩展现有回归用例。
  * 如果修改涉及业务逻辑、参数或返回值的核心变化，建议修改现有自动化用例，以确保测试覆盖新的接口行为。
* **维护自动化用例**：

  * 自动化用例应该随着产品变更而更新，确保测试脚本与接口设计保持同步。
  * 在修改过程中，最好与开发沟通，明确接口新规范，再调整测试用例，避免遗漏关键逻辑。

‍

‍

### Web自动化时，定位元素的方式有哪些

Web自动化测试中，定位元素的方式有多种，常见的有以下几种：

1. **ID**：通过元素的唯一标识符（id）进行定位。
2. **Name**：通过元素的名称属性（name）进行定位。
3. **Class Name**：通过元素的类名（class）进行定位。
4. **Tag Name**：通过元素的标签名称（如`div`​, `button`​等）进行定位。
5. **Link Text**：通过链接文本（link text）定位超链接元素。
6. **Partial Link Text**：通过链接文本的一部分来定位超链接元素。
7. **XPath**：通过XML路径语言（XPath）定位元素。
8. **CSS Selector**：通过CSS选择器定位元素。

‍

### 自动化用例的执行策略是什么？

自动化用例的执行策略包括合理规划测试的执行顺序、选择合适的执行环境、确保数据的准备和清理、实现并行执行、优化执行时间以及生成详细的测试报告。根据项目的需求，自动化测试的执行可以安排在不同的阶段，如持续集成（CI）阶段、回归测试阶段等。同时，需要定期回顾和优化执行策略，以适应软件的更新和变化。

‍

#### **执行顺序的规划**

自动化测试用例的执行顺序非常重要。根据用例的重要性、风险以及需求，可以将用例分为多个层次进行执行。

通常可以按照以下几种策略来规划执行顺序：

* **回归测试**：回归测试用例应该首先执行，确保新提交的代码不会破坏已有功能。回归测试可以分为基础回归测试和高级回归测试，基础回归测试优先执行，确保最重要的功能先被验证。
* **冒烟测试**：冒烟测试用例是最基础的测试，它验证软件的基本功能是否正常。在每次构建后，应该优先执行冒烟测试，确保软件能进行更深入的功能测试。
* **功能测试和性能测试**：功能测试可以在冒烟测试和回归测试通过后执行，确保系统功能完整且符合需求。性能测试则应根据项目需求，在适当的时机进行。

‍

**举例**：在一个在线商城的自动化测试中，可以首先执行登录、商品搜索等基本功能（冒烟测试），然后进行结账、支付等功能的回归测试

‍

#### **执行环境的配置**

‍

测试环境的配置需要确保执行用例时环境与生产环境尽可能一致。执行环境包括：

* **硬件配置**：确保硬件资源充足，支持并发执行和负载测试。
* **软件配置**：安装测试所需的软件版本、浏览器版本、数据库版本等。
* **网络配置**：确保网络环境稳定，尤其在进行性能测试时，网络波动可能导致测试结果不准确。

环境的配置可以通过容器化技术（如Docker）来保证环境的一致性，确保在不同机器上的测试结果相同。

‍

#### 数据准备与清理

自动化测试用例通常需要在特定的数据条件下执行，因此数据准备非常重要。执行策略中应包含：

* **数据驱动**：通过数据驱动的方式来执行用例，即将测试数据从外部源（如Excel、数据库、CSV文件）读取到自动化测试脚本中，确保测试用例可以覆盖多个数据场景。
* **数据清理**：测试执行完毕后，测试数据应及时清理，防止影响后续测试。数据清理工作可以通过自动化脚本实现，确保测试过程的独立性。

**举例**：在进行用户登录的测试时，可以提前准备多个用户数据，并在每次执行测试后清除测试数据，避免污染环境。

‍

#### **并行执行和分布式执行**

为了提高测试效率，可以通过并行执行和分布式执行策略来减少执行时间：

* **并行执行**：通过多线程或多进程方式，在同一台机器上同时运行多个测试用例，提高执行速度。
* **分布式执行**：通过分布式测试框架（如Selenium Grid、Appium等），在不同的机器上同时执行测试，进一步提高执行效率，特别是在大规模的回归测试中。

并行执行和分布式执行可以极大地缩短测试周期，提升测试效率。

**举例**：在Selenium WebDriver中，可以通过Selenium Grid在多台机器上并行执行测试，确保不同浏览器和操作系统上的兼容性。

‍

#### 定时执行与持续集成

自动化测试通常与持续集成（CI）工具结合，在每次代码变更后自动执行测试。定时执行策略包括：

* **代码提交后执行**：每次开发人员提交代码后，自动触发自动化测试，确保新代码没有引入新的问题。
* **定期执行**：定期执行自动化测试（例如每天、每周），确保持续的代码质量。

使用CI工具（如Jenkins、GitLab CI等）可以将自动化测试集成到开发流程中，自动化执行测试并反馈结果。

**举例**：当开发人员提交新的功能或修复bug后，CI工具可以自动触发回归测试，确保新提交的代码没有引发现有功能的错误。

‍

‍

#### 结果分析与报告生成

自动化测试执行后，生成详细的测试报告非常重要。测试报告应包括：

* **测试用例执行状态**：测试用例是否通过，哪些失败，失败原因是什么。
* **性能数据**：包括响应时间、吞吐量、系统负载等。
* **截图和日志**：尤其在UI自动化测试中，截图和日志帮助分析测试失败的原因。

**举例**：通过JUnit与Selenium集成时，测试报告中可以展示每个测试用例的执行结果，并附带每次失败的截图，以便调试。

‍

#### **执行优化**

自动化测试执行策略的优化非常重要。常见的优化措施包括：

* **减少冗余的测试**：删除不必要的测试用例，避免执行冗余的测试步骤，缩短测试周期。
* **优化等待时间**：避免在测试中使用硬编码的等待时间，改用动态等待（如WebDriverWait）来提高执行效率。
* **优化脚本执行顺序**：根据测试的重要性和优先级，调整执行顺序，确保关键功能优先得到验证。

‍

#### 如何实施自动化测试

实施自动化测试的步骤包括需求分析、选择合适的工具、设计和编写测试用例、创建测试脚本、配置测试环境、执行测试、生成报告和维护优化等。首先，团队需要明确自动化测试的目标和范围，然后选择合适的工具来满足需求。接下来，设计全面的测试用例并编写脚本，确保测试环境稳定。最后，执行测试并分析结果，持续优化测试流程和脚本。

‍

‍

## 压力测试

‍

‍

### 压测关键指标

在使用 Postman 等压测工具进行压力测试时，通常需要关注以下几个关键指标：

1. **响应时间（Response Time）** ：每个请求从发送到接收到响应所花费的时间。包括平均响应时间、最小响应时间、最大响应时间等。
2. **吞吐量（Throughput）** ：单位时间内处理的请求数量，通常以每秒请求数（Requests Per Second, RPS）来表示。
3. **并发用户数（Concurrent Users）** ：同时发起请求的用户数量。这个指标可以帮助评估系统在高并发情况下的表现。
4. **错误率（Error Rate）** ：请求失败的比例。包括 HTTP 错误码（如 4xx、5xx）和其他类型的错误。
5. **资源使用率（Resource Utilization）** ：服务器资源的使用情况，包括 CPU 使用率、内存使用率、磁盘 I/O 等。
6. **带宽（Bandwidth）** ：网络带宽的使用情况，通常以每秒传输的数据量（如 Mbps）来表示。
7. **延迟（Latency）** ：请求在网络中传输的时间，包括网络延迟和服务器处理延迟。

‍

‍

‍

## 覆盖率

‍

### 介绍下各种编程语言的代码覆盖率工具

我就说**JaCoCo**就行

‍

1. **Java**：

    * **JaCoCo**：一个流行的Java代码覆盖率工具，支持行覆盖率、分支覆盖率、方法覆盖率等多种覆盖率指标。它可以与JUnit结合使用，生成详细的报告。
    * **Cobertura**：另一款Java代码覆盖率工具，支持代码行覆盖和分支覆盖。它可以集成到Maven、Ant等构建工具中。
2. **Python**：

    * **coverage.py**：最常用的Python代码覆盖率工具，支持多种覆盖率指标（如行覆盖率、分支覆盖率）。它可以生成HTML报告、XML报告等，便于与CI/CD集成。
    * **pytest-cov**：与pytest测试框架集成的插件，能够实时生成代码覆盖率报告，适合自动化测试环境使用。
3. **JavaScript**：

    * **Istanbul**：JavaScript中的广泛使用的代码覆盖率工具，支持行覆盖率、分支覆盖率等。它可以与Mocha等测试框架配合使用，生成详细的覆盖率报告。
    * **Jest**：Facebook开发的JavaScript测试框架，内置了代码覆盖率功能，支持与Istanbul集成，自动生成测试覆盖率报告。
4. **Go**：

    * **go test -cover**：Go语言内置的测试命令，支持生成代码覆盖率报告，适用于快速查看代码的覆盖情况。它生成的覆盖率报告可以在命令行中显示。

‍

‍

### 如何保证测试用例的覆盖度

‍

1. **需求分析**：确保所有的需求都被测试用例覆盖。可以使用需求追踪矩阵（RTM）来跟踪需求和测试用例的对应关系。
2. **代码覆盖率工具**：使用代码覆盖率工具（如 JaCoCo、Cobertura）来分析测试用例对代码的覆盖情况。目标是尽量提高代码覆盖率，包括行覆盖率、分支覆盖率等。
3. **测试用例审查**：定期对测试用例进行审查，确保测试用例的完整性和有效性。可以邀请开发人员、测试人员和业务分析师参与审查。
4. **自动化测试**：尽量使用自动化测试工具来执行测试用例，提高测试效率和覆盖率
5. **执行回归测试**：确保在每次修改后，通过回归测试验证新增功能没有影响已有功能，避免遗漏
6. **重视边界值和特殊情况**：设计针对边界值、极限条件以及系统异常行为的测试用例，以确保系统在各种极端情况下能正常工作
7. **边界测试**：设计边界测试用例，确保覆盖所有可能的边界情况
8. **异常测试**：设计异常测试用例，确保系统在异常情况下的处理
9. **考虑非功能性需求**：除了功能性需求，还要设计测试用例覆盖性能、安全性、兼容性等非功能性要求，确保全面性

‍

> IDEA也有覆盖率测试内容

‍

‍

#### 全新项目，如何保证测试的覆盖率？

在全新项目中，保证测试的覆盖率通常需要以下几个关键步骤：

1. **明确需求与功能点**：首先，确保所有的需求文档和功能设计文档都清晰、详细，了解项目的功能模块和核心流程。
2. **设计全面的测试用例**：根据需求和设计文档，编写详细的测试用例，确保覆盖所有功能模块、边界值、异常情况和用户场景。
3. **优先级划分**：将测试用例按优先级进行划分，确保关键功能、最常见的使用场景和潜在风险区域得到充分的测试。
4. **使用静态分析工具**：利用代码覆盖率工具（如JaCoCo、Cobertura等）分析测试过程中代码的覆盖情况，识别未被测试到的部分。
5. **进行回归测试**：随着项目的逐步开发，进行回归测试，确保新代码没有影响到已有功能的正常运行，增加覆盖率。
6. **引入自动化测试**：对核心功能和常规流程进行自动化测试，以提高覆盖效率和频率。
7. **验证非功能性需求**：除了功能性测试外，还要覆盖性能、安全性、兼容性等非功能性需求，确保项目的各项标准都符合预期。

‍

‍

### 代码覆盖率有哪些的指标？

代码覆盖率通常包括以下几种主要指标：

1. **行覆盖率（Line Coverage）** ：表示测试用例执行过程中，测试到的代码行占总代码行数的比例。行覆盖率是最常见的代码覆盖率指标，目标是尽量提高被测试执行的代码行数。
2. **语句覆盖率（Statement Coverage）** ：与行覆盖率类似，但它关注的是代码中的每个独立语句是否执行过。语句覆盖率与行覆盖率很相似，但语句覆盖率对某些复杂代码可能没有行覆盖率那么详细。
3. **分支覆盖率（Branch Coverage）** ：衡量代码中每个条件分支（如if、switch语句）是否都被测试。分支覆盖率保证了代码中的每个判断条件都得到验证，能够识别代码中未执行的条件分支。
4. **路径覆盖率（Path Coverage）** ：指测试用例覆盖了代码中可能的所有执行路径。路径覆盖率比分支覆盖率更为严格，它确保所有可能的路径都被走到，包括循环结构和嵌套判断。
5. **条件覆盖率（Condition Coverage）** ：确保代码中所有的条件表达式在测试中都至少为真或假一次。与分支覆盖率不同，条件覆盖率更关注代码中的单独条件，而不仅仅是条件分支。
6. **函数覆盖率（Function Coverage）** ：确保程序中所有函数或方法都至少执行一次。它关注的是函数的执行，而不是函数内部的代码行或语句。
7. **决策覆盖率（Decision Coverage）** ：衡量测试用例是否覆盖了所有的决策点（如if语句中的条件判断）。决策覆盖率关注的是判断条件的结果是否为真或假，确保所有判断的结果都被验证。

‍

‍

‍

## 性能调优

‍

### 如何检测、定位项目的性能瓶颈在哪

在进行压测和测试时，检测和定位项目的性能瓶颈可以通过以下步骤进行：

1. **监控系统资源**：使用监控工具来观察 CPU、内存、磁盘 I/O 和网络带宽等系统资源的使用情况。
2. **分析应用日志**：查看应用程序的日志文件，寻找异常或错误信息。 (看看提示输出, 使用/搜素)
3. **使用性能分析工具**：使用性能分析工具（如 Java 的 VisualVM、JProfiler 或 YourKit）来分析应用程序的性能。
4. **数据库监控**：监控数据库的性能，包括查询执行时间、连接数和锁等待时间等。
5. **代码分析**：检查代码中的潜在性能问题，如不必要的循环、复杂的算法和资源泄漏等。(不好)

‍

‍

‍

### 除了CPU占用率外，还有什么能判断是否到底瓶颈？

‍

1. **内存使用率**：高内存使用率可能导致内存不足，从而影响系统性能。
2. **磁盘 I/O**：高磁盘读写操作可能导致 I/O 瓶颈，影响系统响应速度。
3. **网络带宽**：网络带宽不足可能导致数据传输延迟，影响系统性能。
4. **数据库连接数**：过多的数据库连接可能导致数据库性能下降。
5. **线程数**：过多的线程可能导致线程调度开销增加，影响系统性能。

‍

可以使用以下工具来监控这些指标：

* **内存使用率**：`free`​、`top`​、`htop`​
* **磁盘 I/O**：`iostat`​、`iotop`​
* **网络带宽**：`iftop`​、`nload`​
* **数据库连接数**：数据库自带的监控工具，如 MySQL 的 `SHOW PROCESSLIST`​
* **线程数**：`top`​、`htop`​

‍

‍

### 有没有做过二次封装？封装了哪些方法？

我确实做过**二次封装**，主要是封装了常见的操作方法，以提高自动化测试的复用性和稳定性。以下是封装的几类方法：

1. **元素查找方法**：封装了常用的元素查找方法（如`findElement`​和`findElements`​），通过传入不同的定位方式和定位值，简化了元素查找过程。
2. **等待方法**：封装了显式等待的常见操作，如等待元素可见、可点击等，避免每次都编写重复的等待代码。
3. **操作方法**：封装了常见的UI操作，如点击、输入、获取文本等，使得代码更加简洁和可维护。
4. **浏览器相关方法**：封装了浏览器启动、切换窗口、截图等操作，提供了统一的接口调用。

通过这些二次封装，可以使得测试代码更加简洁，提高了测试的复用性和可维护性。

‍

‍

‍

## CICD相关

‍

‍

### 持续集成环境中实施自动化测试的经验

‍

1. **选择CI工具**：选择适合的CI工具，如 Jenkins、GitLab CI、Travis CI 等。
2. **配置CI管道**：在 CI 工具中配置管道（Pipeline），定义各个阶段（Stages）和步骤（Steps），包括代码构建、测试、部署等。
3. **编写测试脚本**：编写自动化测试脚本，确保测试覆盖率高。可以使用 JUnit、TestNG 等测试框架进行单元测试和集成测试。
4. **集成测试工具**：将测试工具集成到 CI 管道中，确保每次代码提交后自动运行测试。
5. **报告和通知**：配置测试报告和通知机制，确保测试结果及时反馈给开发团队。

‍

‍

### 如何进行接口测试持续集成

**接口测试持续集成**（CI）是将接口测试自动化，并在每次代码更改后自动执行，确保API的功能持续符合预期。要实现接口测试的持续集成，可以结合CI工具（如Jenkins、GitLab CI、CircleCI等）和自动化测试框架（如Postman、JUnit、RestAssured等）来实现

‍

#### 1. **准备自动化接口测试脚本**

* 使用Postman、RestAssured、JMeter等工具编写接口测试脚本。确保测试脚本可以在命令行或CI工具中执行，而不需要人工干预。
* 如果使用Postman，可以将API测试集合导出为JSON文件，并通过Postman的命令行工具（Newman）来运行测试

‍

#### 2. **选择CI工具**

选择一个适合团队的持续集成工具，如：

* **Jenkins**：最常用的CI/CD工具，支持丰富的插件和自动化。
* **GitLab CI**：GitLab的CI/CD功能，适合与GitLab仓库结合使用。
* **CircleCI**：云原生的CI工具，集成速度较快。

‍

#### 3. **配置CI环境**

配置CI工具，确保每次代码更新后自动触发接口测试。以下是Jenkins为例的配置步骤：

1. 在Jenkins中创建一个新的流水线项目。
2. 配置代码仓库（如GitHub、GitLab）。
3. 编写Jenkinsfile来描述CI流程。Jenkinsfile中可以包含运行接口测试的步骤

‍

#### 4. **集成自动化测试脚本**

* 将接口测试脚本（如Postman、RestAssured、JUnit等）与CI工具集成，确保每次代码更改后自动运行这些脚本。
* 通过CI工具中的构建触发器（如GitHub的Webhook）来监控代码的变化，自动执行接口测试。

‍

#### 5. **报告与反馈**

* 在接口测试执行后，CI工具会生成测试报告。例如，Newman（Postman的命令行工具）可以生成HTML格式的测试报告。
* 配置CI工具在测试失败时自动发送通知（如Slack、电子邮件等），提醒开发人员修复问题。

‍

#### 6. **定期执行**

* 设置定时任务，定期运行接口测试，确保在长期开发过程中API始终保持稳定。例如，Jenkins可以设置为每日或每周定时运行接口测试。

‍

‍

‍

## JaCoCo

JaCoCo（Java Code Coverage）是一个免费的 Java 代码覆盖率库，提供测试运行期间执行的代码的详细信息。主要特点包括：

* **覆盖率报告**：JaCoCo 生成详细的报告，显示测试期间执行的代码部分，帮助识别未测试的部分。
* **集成**：JaCoCo 可以与 Maven 和 Gradle 等构建工具以及 Jenkins 等 CI 服务器集成。
* **指标**：提供行覆盖率、分支覆盖率和复杂度等各种指标，帮助提高测试质量。

‍

#### 示例代码

以下是使用 JaCoCo 进行代码覆盖率分析的示例：

‍

##### 1. 添加 JaCoCo 依赖

在 `pom.xml`​ 中添加 JaCoCo 插件：

```xml
<build>
    <plugins>
        <plugin>
            <groupId>org.jacoco</groupId>
            <artifactId>jacoco-maven-plugin</artifactId>
            <version>0.8.7</version>
            <executions>
                <execution>
                    <goals>
                        <goal>prepare-agent</goal>
                    </goals>
                </execution>
                <execution>
                    <id>report</id>
                    <phase>test</phase>
                    <goals>
                        <goal>report</goal>
                    </goals>
                </execution>
            </executions>
        </plugin>
    </plugins>
</build>
```

‍

##### 2. 运行测试并生成覆盖率报告

使用以下命令运行测试并生成覆盖率报告：

```sh
mvn clean test
mvn jacoco:report
```

‍

##### 3. 查看覆盖率报告

生成的覆盖率报告通常位于 `target/site/jacoco/index.html`​，可以在浏览器中打开查看详细的覆盖率信息。

通过这些方法，可以有效地保证测试用例的覆盖度，提高软件的质量和稳定性。

‍

‍

‍

## Jmeter

‍

### Jmeter调试时候怎么进行密码校验

‍

1. **添加 HTTP 请求**：在测试计划中添加一个 HTTP 请求采样器，用于发送登录请求。
2. **添加 HTTP Header Manager 信息头管理器**：在 HTTP 请求采样器中添加一个 HTTP Header Manager，用于设置请求头信息（如 Content-Type）。
3. **添加 User Defined Variables 用户定义变量**：在测试计划中添加一个 User Defined Variables 元素，用于定义用户名和密码变量。
4. **添加 Post Processor 后置处理器**：在 HTTP 请求采样器中添加一个 Post Processor（如 JSON Extractor 或 Regular Expression Extractor），用于提取响应中的认证令牌或其他验证信息。
5. **添加断言 响应断言**：在 HTTP 请求采样器中添加一个断言（如 Response Assertion），用于验证响应中是否包含预期的认证信息。

‍

‍

### Jmeter 为什么要参数化

**为什么要参数化 JMeter 脚本？**

1. **模拟真实用户行为**：真实用户在访问系统时，往往会提供不同的输入参数，如用户名、密码、商品信息等。参数化可以使得 JMeter 脚本模拟多个用户使用不同数据访问系统，从而更真实地反映实际负载。
2. **避免缓存和重复数据的影响**：如果测试脚本中使用固定的输入数据，会导致服务器缓存相同的请求和响应，从而影响测试的结果。通过参数化，每次请求都会使用不同的数据，从而避免缓存的干扰。
3. **提高测试的覆盖度**：通过参数化，可以在不同的测试场景下使用不同的数据，进而提高测试的全面性。例如，模拟不同地区的用户登录、不同商品的购买等，从而验证系统在多种条件下的表现。
4. **增强脚本的复用性**：当脚本需要进行多次执行时，参数化允许你使用不同的数据集，从而避免编写多个不同的脚本，提升了脚本的灵活性和复用性。

‍

#### Jmeter参数化有哪几种方法

在 JMeter 中，**参数化**是指通过动态变化的输入数据来模拟不同用户的行为，从而更真实地反映实际使用情况。JMeter 提供了多种参数化方法，常见的有：

1. **CSV Data Set Config**：

    * 使用 CSV 文件作为数据源，通过 CSV Data Set Config 元件将 CSV 文件中的数据逐行读取，并替换测试计划中的变量。
2. **User Defined Variables**：

    * 在测试计划或线程组中定义用户自定义变量，可以用来存储不同的参数值。通过这些变量，可以灵活地在测试中传递数据。
3. **__Random函数**：

    * 使用 JMeter 内置的随机函数，可以生成随机值并应用于请求中，通常用于模拟不同的输入数据。
4. **__StringFromFile 函数**：

    * 从文件中读取数据并在测试中作为变量使用。每次执行时，JMeter 会按顺序读取文件中的内容。
5. **__P 函数**：

    * 使用 JMeter 的参数化函数 \_\_P 读取系统参数或环境变量，以动态地传递测试数据。

‍

### 如何用Jmeter录制脚本？

1. **启动 JMeter**：

    * 打开 JMeter，进入主界面。
2. **添加录制控制器**：

    * 在测试计划下，右键点击“测试计划” \> “添加” \> “非测试元素” \> “HTTP(S) Test Script Recorder”。这会在测试计划中添加一个录制器组件，用于捕获 HTTP 请求。
3. **设置录制器**：

    * 在“HTTP(S) Test Script Recorder”中，设置端口号（默认是 8888）。确保 JMeter 监听的端口与浏览器的代理设置端口相同。
    * 可以选择是否保存请求的文件夹。
4. **配置浏览器代理**：

    * 打开浏览器，进入设置，设置代理服务器为本地代理（通常是 `127.0.0.1`​）并使用与 JMeter 设置的端口号相同的端口（如 8888）。
    * 在浏览器中访问需要录制的网页，JMeter 会自动捕获并显示请求。
5. **录制请求**：

    * 在浏览器中执行你需要录制的操作（如登录、浏览网页、提交表单等）。JMeter 会自动捕获这些操作并生成 HTTP 请求。
    * 完成后，可以停止录制。
6. **查看生成的请求**：

    * 在 JMeter 中，可以在“HTTP(S) Test Script Recorder”下的“请求”中查看录制的 HTTP 请求。每个请求会被添加到线程组中，按顺序执行。
7. **保存脚本**：

    * 最后，保存整个测试计划，确保所有录制的请求都被保存下来。

‍

‍

### Jmeter 用户定义的变量和用户参数的区别

在 **JMeter** 中，用户定义的变量和用户参数是用来存储数据的，但它们的使用场景和作用有所不同。具体区别如下：

1. **用户定义的变量（User Defined Variables）** ：

    * **作用**：用户定义的变量用于在测试计划或线程组中全局存储一些常量值。它们通常用于存储在测试中需要重复使用的静态值。
    * **定义位置**：可以在 **Test Plan** 或 **Thread Group** 级别定义，定义后，在该级别及其子组件中都可以访问。
    * **使用场景**：适用于存储不随请求变化的静态数据，例如 URL 地址、常用的测试参数等。
2. **用户参数（User Parameters）** ：

    * **作用**：用户参数主要用于动态传递不同用户请求的参数，可以为每个用户（线程）设置不同的值。它们更灵活，可以在运行时动态改变。
    * **定义位置**：通常在 **Thread Group** 或 **Test Plan** 中使用 **User Parameters** 元件来定义，可以设置每个线程使用的特定参数。
    * **使用场景**：适用于当你需要为每个用户生成不同的输入值时，例如为每个虚拟用户生成唯一的登录名、密码等。

‍

‍

### 什么是Jemter采样器（Samplers）和线程组（Thread group）

1. **采样器（Samplers）** ：

    * **作用**：采样器用于发送请求到服务器，并收集响应数据。每个采样器代表一个请求类型，比如 HTTP 请求、FTP 请求、JDBC 请求等。采样器通过发送请求，模拟用户与服务器的交互。
    * **常见类型**：

      * **HTTP Request**：发送 HTTP 请求，可以用于测试 Web 应用。
      * **JDBC Request**：发送数据库查询请求。
      * **FTP Request**：发送 FTP 请求。
      * **SMTP/POP3 Request**：发送邮件协议请求。
2. **线程组（Thread Group）** ：

    * **作用**：线程组用于模拟用户的行为。它定义了虚拟用户（线程）的数量、每个用户的启动延迟、执行次数等信息。每个线程组中可以包含多个采样器和其他控制器（如定时器、断言等），线程组会按照设定的规则启动并执行其中的所有操作。
    * **配置项**：

      * **线程数（Number of Threads）** ：表示虚拟用户的数量，JMeter 会根据这个数值创建相应数量的线程来模拟并发用户。
      * **循环次数（Loop Count）** ：每个线程（用户）将执行的次数，可以设置为无限次，也可以设置为固定次数。
      * **Ramp-up 时间（Ramp-up Period）** ：表示 JMeter 启动所有线程的时间，通常用于模拟负载逐渐增加的情况。

‍

### 什么是Jemter预置处理器元件？列出一些预处理器元件

在 **JMeter** 中，**预置处理器（Pre-Processors）**  是一种用于在每个采样器（请求）执行之前执行某些操作的元素。它们可以用来设置参数、进行数据准备、或者修改请求的内容。预置处理器可以帮助我们在发送请求之前进行一些必要的设置或修改，以确保测试数据的准确性或模拟更真实的用户行为。

‍

### 常见的预处理器元件

1. **User Defined Variables**：可以在请求之前定义全局变量，这些变量可以在测试中使用。
2. **BeanShell PreProcessor**：允许使用 BeanShell 脚本在请求前进行一些自定义的操作，如修改请求的参数、提取值等。
3. **JSR223 PreProcessor**：类似于 BeanShell PreProcessor，但支持多种脚本语言（如 Groovy、JavaScript、Jython 等）。它比 BeanShell 更加高效，适用于复杂的脚本处理。
4. **CSV Data Set Config**：读取 CSV 文件中的数据并将其作为变量传递给后续的请求。通常用于参数化测试，确保每个请求使用不同的数据。

‍

### 什么是JMeter中的断言？断言的类型有哪些？

在 **JMeter** 中，**断言（Assertions）**  用于验证服务器返回的响应是否符合预期。断言允许你检查响应的数据内容、响应时间、响应代码等，帮助确认系统是否按预期工作。通过断言，能够自动化地验证测试是否成功，减少手动验证的工作量。

‍

类型

1. **响应断言（Response Assertion）** ：验证返回的响应是否包含特定的内容、匹配某种模式或符合预期的条件。
2. **大小断言（Size Assertion）** ：用于验证响应内容的大小（字节数）。
3. **XML断言（XML Assertion）** ：用于验证XML响应的格式是否正确，检查XML中的元素是否满足条件。
4. **JSON断言（JSON Assertion）** ：验证返回的JSON格式是否符合预期，检查某些键值对是否存在。
5. **断言失败时的动作（Assertion Results）** ：允许你查看断言的结果，如果断言失败，则显示错误信息。
6. **超时断言（Duration Assertion）** ：检查响应时间是否在规定的时间内，适用于性能测试中的响应时效性验证。

‍

详细

1. **响应断言（Response Assertion）** ：

    * **功能**：响应断言用于验证返回的响应是否符合特定的规则，可以检查响应体、响应代码或响应消息是否符合预期。
    * **使用场景**：例如，验证网页返回的 HTML 页面中是否包含特定的关键字，或验证 HTTP 状态码是否为 200。
    * **示例**：你可以验证 HTTP 响应码是否为 200 OK，或者验证网页响应中是否包含 “Login successful” 字样。

    这个断言有多个检查条件：

    * **响应文本**：检查响应体中是否包含指定的文本（如成功、失败消息等）。
    * **正则表达式**：通过正则表达式匹配响应体，确保其符合特定格式。
    * **响应代码**：检查 HTTP 状态码，如 200、404 等。
2. **大小断言（Size Assertion）** ：

    * **功能**：用于验证返回的响应内容的大小。通常用于检查返回内容的字节数是否在预期范围内。
    * **使用场景**：可以用来验证文件上传或下载时返回的文件大小是否合适，或者验证响应的 HTML 文件是否过大。
    * **示例**：假设你期望一个文件的下载响应大小为 100KB，你可以使用大小断言来验证实际的响应大小是否符合要求。
3. **XML断言（XML Assertion）** ：

    * **功能**：这个断言用于验证响应是否为有效的 XML 格式，并且可以进一步检查 XML 数据中的特定元素是否存在或符合条件。
    * **使用场景**：适用于处理 Web 服务返回的 XML 数据，比如 SOAP Web 服务的响应验证。
    * **示例**：你可以验证 XML 响应中是否包含某个特定的 `<status>success</status>`​ 元素，或者验证 XML 是否符合特定的结构。
4. **JSON断言（JSON Assertion）** ：

    * **功能**：用于验证返回的 JSON 数据是否有效，并检查特定键的值是否符合预期。
    * **使用场景**：适用于验证 JSON 格式的响应，特别是在 RESTful API 测试中。
    * **示例**：你可以检查 JSON 响应中是否包含 `{"status":"success"}`​，或者验证 `userId`​ 是否为特定值。
5. **断言失败时的动作（Assertion Results）** ：

    * **功能**：在断言失败时，可以通过 **断言结果** 来输出错误信息，帮助调试和分析问题。这可以记录错误的响应内容或失败的原因。
    * **使用场景**：在性能测试中，使用断言来确保请求的响应符合预期，如果断言失败，则能够在报告中查看失败的详细信息，方便后续分析和调试。
6. **超时断言（Duration Assertion）** ：

    * **功能**：用于检查响应时间是否在规定的时间内。它能够设置最大允许的响应时长，若超过这个时间，则标记为失败。
    * **使用场景**：在性能测试中，使用此断言可以确保请求的响应时间不超过指定的最大值。例如，如果你希望每个请求的响应时间不超过 2 秒，可以使用超时断言来验证。

‍

‍

### JMeter中的计时器是什么，计时器的类型是什么

在 **JMeter** 中，**计时器（Timers）**  是用于控制请求之间延迟的元素。它允许你在每个请求之间设置一定的时间间隔，模拟用户的等待行为，从而更准确地模拟实际的用户操作。计时器可以帮助我们模拟更自然的请求发送，避免过于快速的请求造成服务器压力过大或者不符合真实情况。

‍

常见的计时器类型：

1. **Constant Timer（固定计时器）** ：在每个请求之间插入一个固定的延迟时间。
2. **Gaussian Random Timer（高斯随机计时器）** ：在请求之间插入一个具有正态分布（高斯分布）的随机延迟时间。
3. **Uniform Random Timer（均匀随机计时器）** ：在请求之间插入一个均匀分布的随机延迟时间。
4. **Constant Throughput Timer（恒定吞吐量计时器）** ：根据目标吞吐量（请求速率）插入延迟，使得请求速率保持恒定。
5. **Poisson Random Timer（泊松随机计时器）** ：使用泊松分布的延迟时间来模拟请求之间的随机间隔，适用于模拟用户行为时更具随机性的场景。

‍

‍

### 如何在JMeter中执行尖峰测试（Spike testing）

在 **JMeter** 中执行 **尖峰测试（Spike Testing）**  主要是通过突然增加负载，观察系统在高负荷下的响应情况。尖峰测试旨在模拟瞬时高峰流量，评估系统的承载能力以及系统在高负载下的表现，特别是对系统崩溃、性能下降和资源消耗的影响。

‍

**基本步骤：**

1. **创建线程组（Thread Group）** ：在线程组中设置合适的用户数。
2. **增加负载**：快速增加请求负载，比如在短时间内将并发线程数从常规值提升到峰值。
3. **配置定时器（Timer）** ：可以使用定时器控制请求的发送频率，模拟负载的突增。
4. **监控性能**：使用 **监听器**（如图形结果、聚合报告等）来查看响应时间、吞吐量、失败率等指标。
5. **逐步减少负载**：测试过程中可以逐渐减少负载，模拟请求量的快速降低。

‍

### 如何在JMeter中捕获身份验证窗口的脚本

[如何在JMeter中捕获身份验证窗口的脚本？-帅地玩编程 (iamshuaidi.com)](https://www.iamshuaidi.com/50313.html)

在 **JMeter** 中，捕获身份验证窗口的脚本通常涉及 **HTTP 请求** 和 **HTTP 请求的身份验证** 配置。JMeter 支持多种身份验证机制，常见的有 **基本身份验证（Basic Authentication）**  和 **表单身份验证（Form Authentication）** 。如果是 **弹出式身份验证窗口**（例如 Basic Auth 对话框），JMeter 可以通过 **HTTP Authorization Manager** 来模拟身份验证。

‍

捕获身份验证窗口的基本步骤：

1. **配置 HTTP Authorization Manager**：用于管理身份验证的凭据。
2. **录制脚本**：使用 **HTTP(s) Test Script Recorder** 录制请求。
3. **检查和设置请求**：在请求中正确设置身份验证信息。

‍

‍

### 列出几个JMeter监听器

**JMeter** 提供了多种 **监听器（Listeners）**  用于收集和展示测试过程中的性能数据。常用的监听器包括：

1. **聚合报告（Aggregate Report）** ：

    * 汇总所有请求的统计信息，包括吞吐量、平均响应时间、最小/最大响应时间、错误率等。
2. **图形结果（Graph Results）** ：

    * 以图表的形式展示响应时间和吞吐量随时间的变化，帮助识别性能瓶颈。
3. **查看结果树（View Results Tree）** ：

    * 显示每个请求的详细响应结果，包括请求数据、响应数据和断言结果，方便调试。
4. **总结报告（Summary Report）** ：

    * 提供每个请求的汇总数据，类似于聚合报告，但更加简洁。
5. **响应时间分布（Response Time Graph）** ：

    * 展示每个请求的响应时间分布情况，帮助分析性能波动。
6. **执行结果日志（Simple Data Writer）** ：

    * 将测试结果输出到指定文件，以便进行后续分析。
7. **持久性图（Persistent Data Writer）** ：

    * 以持久化的方式记录结果，支持数据存储到数据库或外部文件中。

‍

### JMeter中的远程测试是什么？如何进行远程测试？

JMeter中的远程测试是指通过分布式模式进行负载测试，即使用多台机器来共同执行测试任务。这样可以模拟更高并发的用户负载。远程测试的过程通常是将JMeter的测试计划（Test Plan）分发到多台机器上运行，并通过主控机器（Master）来管理和监控测试过程，生成结果。

要进行远程测试，需要：

1. 在所有参与的机器上安装JMeter。
2. 配置远程测试的主控和被控机器。
3. 在主控机器上设置远程测试的配置（通过JMeter的GUI或命令行）。
4. 启动远程测试，主控机器会将测试任务分发到各个被控机器上执行。
5. 执行完毕后，收集各机器的测试结果进行分析。

‍

### JMeter中的分布式测试是什么？如何进行分布式测试？

JMeter中的分布式测试是指使用多台计算机来共同执行负载测试，以提高测试的并发能力和覆盖范围。通过将测试任务分发到多个从机（Slave）上进行执行，可以模拟更多的虚拟用户，并确保测试能够处理大规模的负载。

进行分布式测试的步骤如下：

1. 在主机和所有从机上安装JMeter。
2. 配置从机，启动JMeter服务器。
3. 在主机上配置远程测试设置，并设置要启动的从机。
4. 在主机上启动测试，主机将任务分发到所有从机。
5. 执行完成后，收集各个从机的测试结果，进行汇总和分析。

‍

‍

‍

## Junit

‍

‍

### 覆盖率测试

使用覆盖率模式运行, 看左边的进入边界条件的情况.

‍

‍

‍

### 写单元测试用例 + Mock

@BeforeEach    每一个测试方法调用前必执行的方法

@AfterEach    每一个测试方法调用后必执行的方法

@BeforeAll  所有测试方法调用前执行一次，在测试类没有实例化之前就已被加载，需用static修饰

@AfterAll   所有测试方法调用后执行一次，在测试类没有实例化之前就已被加载，需用static修饰

‍

为了制造假数据, 使用Mock进行

‍

两个注解

* @MockBean    如果没有指定规则的话，统统返回默认值，对象为null，数字为零; 制定了mock规则就按照规则走
* @SpyBean    有规则按照规则走，没有规则走真实

‍

搭配

```java
when(memberService2.add(3)).thenReturn("ok");//不真的进入数据库/MQ，不落盘，改变return
```

‍

```java
 Mock注入
 * 实现原理：使用打桩(Stub)技术动态替换原来的程序
 * 直接跑Java代码，验证逻辑，不太关心真实数据落盘，不需要启动微服务和连接真实数据库
 * 模拟一切操作步骤，逻辑通过，不执行SQL，可以指定返回任意值。优点如下：
 * 1 完全脱离数据库/MQ等，避免废数据产生
 * 2 只针对一个小方法逻辑，不启动其它避免其它因素产生的干扰。
```

‍

```java
@SpringBootTest
class MemberServiceTest //类似MemberController
{
    //如果没有指定规则的话，统统返回默认值，对象为null，数字为零;制定了mock规则就按照规则走

    @MockBean
    private MemberService memberService2;
    @Test
    void m2_WithMockRule()
    {
        when(memberService2.add(3)).thenReturn("ok");//不真的进入数据库/MQ，不落盘，改变return

        String result = memberService2.add(3);
        assertEquals("ok",result);

        System.out.println("----m2_WithMockRule over");
    }

    //==========第三组==========
    //@Resource //真实调用
    //@MockBean//如果没有指定规则的话，统统返回默认值，对象为null，数字为零;制定了mock规则就按照规则走
    //@SpyBean //有规则按照规则走，没有规则走真实
    private MemberService memberService3;

    @Test
    void m3()
    {
        when(memberService3.add(2)).thenReturn("ok");
        String result = memberService3.add(2);
        System.out.println("----add result:  "+result);
        assertEquals("ok",result);

        int result2 = memberService3.del(3);
        System.out.println("----del result2:  "+result2);
        assertEquals(3,result2);


        System.out.println("----over");
    }
}

 
```

‍

‍

### Mock提升

示例

‍

#### Controller

```java
@RunWith(SpringRunner.class)
@SpringBootTest
// 会自动注入 mockmvc对象
@AutoConfigureMockMvc
public class UserControllerTest {

    @Autowired
    private MockMvc mockMvc;

    // 标记service表示无效,走我们的service
    @MockBean
    private UserService service;

    @Test
    public void findAddr() throws Exception {
        // 当访问service的 verify方法时. 任何对象都返回true
        when(service.verify(any(UserDto.class))).thenReturn(true);
        MvcResult result = mockMvc.perform(
                post("/v1/user/login")
                // application/x-www-form-urlencoded , 一般都是选择这个
                .contentType(MediaType.APPLICATION_FORM_URLENCODED_VALUE)
                // 输入参数
                .param("username", "root")
                .param("password", "123456")
        )
            // 打印一些必要的请求信息.
                .andDo(MockMvcResultHandlers.print())
                .andReturn();

        String content = result.getResponse().getContentAsString();
        Assert.assertEquals(content, "true");
    }
}
```

这里的 `@MockBean`​ 是Spring提供的. 同时还有, mock官方提供了 `@Mock`​ 和 `@InjectMocks `​ , 具体可以看看这篇文章 , 怕使用的时候出错误 , 记录一下 : [https://www.jianshu.com/p/c68ee5d08fdd](https://www.jianshu.com/p/c68ee5d08fdd)

‍

‍

#### Service

往往依赖于dao层

‍

##### 1. 关闭web 环境

所以解决方式也是 , 但是不是web层, 所以往往不需要web环境. `@SpringBootTest(webEnvironment = SpringBootTest.WebEnvironment.NONE)`​ 这个可以关闭web环境.

```java
@RunWith(SpringRunner.class)
@SpringBootTest(webEnvironment = SpringBootTest.WebEnvironment.NONE)
public class UserServiceTest {

    @Autowired
    UserService service;
  
    @Test
    public void testUser() {
        service.verify(UserDto.builder().password("111").build());
    }
}
```

‍

##### 2. 测试用例 , 每个都需要单独的context

```java
@RunWith(SpringRunner.class)
@SpringBootTest(webEnvironment = SpringBootTest.WebEnvironment.NONE)
public class UserServiceTest {

    @Autowired
    private UserService service;

    @DirtiesContext
    @Test
    public void test1() {
        System.out.println("test-1 " + service.hashCode());
    }

    @DirtiesContext
    @Test
    public void test2() {
        System.out.println("test-2 " + service.hashCode());
    }
}
```

我们发现输出的是

```java
test-1 2125470482
test-2 1846539844
```

这个 `@DirtiesContext`​ 有两种

第一种就是方法隔离 .

第二种就是类隔离. 使用场景的话各有差异 ..

测试用例的运行 . 可以直接通过 mvn test 就可以了 , 重定向输出可以用 >> log.log ,这里切记一点. `mvn test -Dtest=*Test -DfailIfNoTests=true`​ , 这个是默认的缺省值. 也就是只会监测到Test后缀的文件进行测试.

‍

#### DAO层面

其中 `@MockBean`​ 可以帮助我们实例化一个mapper对象, 同时我们可以直接调用. 此时就可以直接隔离数据库访问层面.

```java
@RunWith(SpringRunner.class)
@SpringBootTest(webEnvironment = SpringBootTest.WebEnvironment.NONE)
public class UserMapperTest {

    @MockBean
    private UserMapper mapper;

    @Test
    public void findAddr() {
        when(mapper.findAddr(anyInt())).thenReturn("北京");
        String addr = mapper.findAddr(100);
        assertEquals("北京", addr);
        verify(mapper).findAddr(100);
    }
}
```

‍

##### H2数据库 方便测试

加入依赖,

```xml
<!-- 方便测试 -->
<dependency>
    <groupId>com.h2database</groupId>
    <artifactId>h2</artifactId>
    <version>1.4.197</version>
    <scope>test</scope>
</dependency>
```

配置文件的话, 很简单, 我是用的是JPA.

```properties
# H2的配置. 就两行其实. 其他我多余加上的. 
spring.datasource.url=jdbc:h2:mem:jpa;DB_CLOSE_DELAY=-1;DB_CLOSE_ON_EXIT=FALSE;MODE=MYSQL
spring.datasource.driver-class-name=org.h2.Driver
spring.datasource.continue-on-error=false
spring.datasource.hikari.minimum-idle=2

# 没次启动创建, 每次结束进程删除
spring.jpa.hibernate.ddl-auto=create-drop
# SQL打印开启
spring.jpa.show-sql=true
# 数据库引擎修改一下
spring.jpa.database-platform=org.hibernate.dialect.MySQL5InnoDBDialect
```

这个就是一个完整的测试环境. soeasy .

‍

‍

### MockITO 的使用

第一种 使用 , 很方便

```java
public class MockTest {
    @Test
    public void test(){
        UserMapper mapper = mock(UserMapper.class);
        UserDo tom = UserDo.builder().userId(1L).username("tom").password("123").build();
        when(mapper.findByUserId(anyLong())).thenReturn(tom);
        // 第一次调用
        System.out.println(mapper.findByUserId(1L));
        // 第二次调用
        System.out.println(mapper.findByUserId(2L));

        // 校验
        verify(mapper).findByUserId(1L);
        verify(mapper).findByUserId(2L);
    }
}
```

第二种 , 更方便

```java
@RunWith(MockitoJUnitRunner.class)
public class MockTest {

    @Mock
    private UserMapper mapper;

    @Test
    public void test(){
        UserDo byUserId = mapper.findByUserId(1L);
        System.out.println(byUserId);
    }
}
```

第三种 , 一般吧, 其实和第一种相似 .

```java
public class MockTest {

    @Mock
    private UserMapper mapper;

    public MockTest() {
        mapper = Mockito.mock(UserMapper.class);
    }

    @Test
    public void test(){
        UserDo byUserId = mapper.findByUserId(1L);
        System.out.println(byUserId);
    }
}
```

‍

‍

‍

‍

## 其他工具

‍

### 使用过的监控和分析工具，各自有什么特点

‍

1. **Zabbix**：

    * **特点**：Zabbix是一个开源的企业级监控解决方案，可以监控服务器、网络设备、应用程序等。它支持实时数据收集、告警和自动化操作。Zabbix通过代理和无代理两种方式获取数据，具有灵活的可扩展性。
    * **优势**：功能强大，支持自定义监控项；支持图形和报告，适合复杂的企业环境。
2. **Prometheus + Grafana**：

    * **特点**：Prometheus用于收集和存储时序数据，Grafana则用于数据的可视化展示。Prometheus使用拉取方式获取指标，支持自动化发现服务和强大的查询语言PromQL。
    * **优势**：适合微服务架构，支持实时监控和报警；Grafana提供强大的数据可视化功能，可以非常灵活地定制仪表盘。
3. **New Relic**：

    * **特点**：New Relic是一款商业化的应用性能管理（APM）工具，能够深入到代码级别进行性能监控，支持多种编程语言和框架。它提供了详细的性能分析、慢查询分析、错误跟踪等功能。
    * **优势**：对应用性能的监控非常细致，支持端到端的监控；界面简洁，适合开发和运维人员进行实时分析。
4. **ELK Stack (Elasticsearch, Logstash, Kibana)** ：

    * **特点**：ELK Stack主要用于日志管理和分析。Elasticsearch负责日志数据的存储和查询，Logstash负责日志数据的收集和处理，Kibana则用于展示和可视化分析。
    * **优势**：强大的日志聚合和分析能力；适合大规模的日志数据处理，尤其对于微服务架构中的日志分析非常有用。
5. **Nagios**：

    * **特点**：Nagios是一款经典的开源监控工具，适用于监控服务器、网络设备、应用等。它提供了强大的报警功能，可以在出现异常时发送通知。
    * **优势**：配置灵活，支持插件扩展；适用于小型到中型环境的监控。

‍

### 介绍常用抓包工具以及一般抓包的流程

‍

#### 常用的抓包工具

1. **Wireshark**：一个开源的网络协议分析工具，可以捕获和分析网络流量。
2. **Fiddler**：一个强大的HTTP调试代理工具，适用于捕获和分析HTTP/HTTPS流量。
3. **Charles**：一个HTTP代理/HTTP监视器/反向代理，适用于调试和分析HTTP/HTTPS流量。
4. **tcpdump**：一个命令行抓包工具，适用于捕获和分析网络流量。

‍

#### 一般抓包的流程

1. **选择抓包工具**：根据需求选择合适的抓包工具。
2. **配置抓包工具**：配置抓包工具以捕获所需的网络流量。例如，配置代理设置、过滤规则等。
3. **启动抓包**：启动抓包工具，开始捕获网络流量。
4. **执行操作**：在被测系统上执行需要分析的操作，以生成网络流量。
5. **停止抓包**：完成操作后，停止抓包工具，保存捕获的数据。
6. **分析数据**：使用抓包工具分析捕获的数据，查找和诊断问题。

‍

‍

#### 示例

以下是使用Wireshark进行抓包的简单示例：

1. **启动Wireshark**：打开Wireshark应用程序。
2. **选择网络接口**：选择要捕获流量的网络接口（如以太网、Wi-Fi）。
3. **开始捕获**：点击“Start”按钮开始捕获网络流量。
4. **执行操作**：在被测系统上执行需要分析的操作。
5. **停止捕获**：点击“Stop”按钮停止捕获网络流量。
6. **分析数据**：在Wireshark中查看和分析捕获的数据包。

通过这些步骤，可以使用抓包工具捕获和分析网络流量，帮助诊断和解决网络问题。

‍

### 最常用的API文档模板

1. **OpenAPI（Swagger）** ：一种标准化的API描述格式，广泛用于RESTful API文档的生成。它采用YAML或JSON格式，支持生成交互式API文档，方便开发者进行测试。
2. **RAML（RESTful API Modeling Language）** ：一种用于RESTful API的建模语言，结构化的定义使得API文档可读性强，便于团队协作。
3. **Postman**：不仅是API测试工具，还提供了API文档功能。通过Postman，可以生成交互式文档，支持API调用和响应的实时展示。
4. **Apiary**：提供基于API Blueprint的文档生成工具，可以快速定义、测试和发布API文档。它支持Markdown格式，便于开发人员编辑。
5. **Redoc**：一个流行的OpenAPI文档生成工具，专注于清晰、易读的API文档展示，常用来与Swagger配合使用。

‍

‍

### 最常用的BUG管理工具

1. **JIRA**：由Atlassian公司开发，是全球最流行的项目管理和Bug跟踪工具之一，特别适合敏捷开发团队。
2. **Bugzilla**：由Mozilla开发，是一款开源的Bug跟踪工具，功能强大，适用于大型项目和开源社区。
3. **Redmine**：一款开源的项目管理工具，支持Bug管理、任务追踪、时间跟踪等功能，适合中小型团队。
4. **MantisBT**：轻量级的开源Bug跟踪工具，适合小型和中型团队，简单易用且灵活。
5. **YouTrack**：由JetBrains开发，适合敏捷开发团队，支持灵活的查询语言和高效的Bug追踪。
6. **Trello**：虽然主要是项目管理工具，但也被很多团队用来作为Bug跟踪工具，使用看板方式管理Bug。
7. **Trac**：一款开源的项目管理和Bug跟踪工具，支持Wiki和版本控制系统，适合小型团队。
8. **Pivotal Tracker**：专注于敏捷开发的项目管理工具，集成了Bug跟踪功能，适合开发团队。
9. **Asana**：一款流行的团队协作工具，也可以用于Bug跟踪，界面简洁，适合小型团队。
10. **TestRail**：主要用于测试管理的工具，但也具备Bug跟踪功能，特别适合与测试流程紧密结合的团队。

‍

‍

### 如何对IE浏览器进行网页调试

‍

1. **打开开发者工具**：

    * 按下 **F12** 键或右键点击网页并选择  **“检查元素”** （Inspect Element）来打开开发者工具。
2. **使用开发者工具的不同面板**：

    * **DOM（元素）面板**：查看和编辑网页的HTML结构。
    * **Console（控制台）面板**：查看JavaScript输出、错误和日志信息。
    * **Network（网络）面板**：查看和分析所有网络请求，包括页面加载的资源。
    * **Debugging（调试）面板**：调试JavaScript代码，设置断点，单步执行代码。
    * **Memory（内存）面板**：查看页面的内存使用情况。
    * **Performance（性能）面板**：监控页面加载和渲染的时间，帮助优化页面性能。
3. **调试JavaScript**：

    * 在 **Debugging** 面板中，可以设置断点、单步执行代码、查看调用栈以及变量值。
4. **检查和修改CSS**：

    * 在 **Elements** 面板中，你可以查看网页元素的CSS样式，并在实时编辑时查看修改效果。
5. **查看网络请求**：

    * 在 **Network** 面板中，你可以分析所有的HTTP请求，查看资源加载情况，以及分析响应的时间和内容。

‍

‍

‍

### Chrome开发者工具中，常用的面板有哪几个？

‍

1. **Elements**：用于查看和编辑页面的HTML和CSS。

    * 该面板允许你实时查看页面的DOM结构，并且能够修改HTML和CSS样式，帮助调试和优化网页的布局。
2. **Console**：用于输出日志、错误信息以及与网页的JavaScript交互。

    * 可以查看JavaScript错误、警告和日志输出，还可以直接在控制台输入和执行JavaScript代码。
3. **Network**：用于监控网络请求和响应。

    * 可以查看页面加载过程中的所有网络请求，包括HTTP请求、文件下载等，并且可以分析加载时间、响应时间以及请求的内容。
4. **Performance**：用于分析网页的性能，包括页面加载时间、JavaScript执行时间和页面渲染过程。

    * 该面板帮助你识别性能瓶颈，查看每一帧的渲染时间以及JavaScript的执行情况，帮助优化页面响应速度。
5. **Memory**：用于分析页面的内存使用情况。

    * 可以检测内存泄漏、查看内存的分配情况，帮助优化页面的内存使用。
6. **Application**：用于查看和管理网页存储的数据，如Cookies、LocalStorage、SessionStorage、IndexedDB等。

    * 你可以查看和编辑浏览器中的存储数据，调试Service Workers等内容。
7. **Security**：用于查看网页的安全信息，特别是SSL/TLS连接。

    * 该面板显示关于网站的安全状态，包括SSL证书、加密协议等信息，帮助分析安全性问题。
8. **Lighthouse**：用于进行网页性能、可访问性、SEO等方面的自动化审计。

    * 提供了关于网页的详细报告，包括页面加载速度、响应能力、可访问性以及SEO优化等建议。
9. **Sources**：用于查看、调试和编辑网页的源代码。

    * 你可以在该面板中查看JavaScript文件、设置断点进行调试，调试脚本的执行流程。

‍

‍

‍

## 测试场景

‍

‍

‍

### 测试一支笔

1. **检查外观**：确保笔的外观没有损坏，笔帽、笔夹等部件完好无损。
2. **书写测试**：在纸上书写，检查笔的流畅度、墨水的均匀性和颜色的清晰度。
3. **耐久性测试**：长时间书写，观察笔的耐用性和墨水的持续性。
4. **干燥时间**：测试墨水的干燥时间，确保不会轻易弄脏纸张。
5. **特殊功能**：如果笔有特殊功能（如橡皮擦、荧光功能等），测试这些功能是否正常工作。

‍

‍

‍

### 针对QQ这种聊天发送信息设计测试用例

设计针对QQ聊天发送信息的测试用例时，可以考虑以下几个方面：

1. **功能测试**：验证基本的发送信息功能是否正常。
2. **边界测试**：验证信息长度的边界情况。
3. **性能测试**：验证在高并发情况下发送信息的性能。
4. **异常测试**：验证在网络异常或服务器异常情况下的处理。

‍

示例测试用例

#### 功能测试

1. **发送普通文本信息**

    * **步骤**：输入普通文本信息，点击发送按钮。
    * **预期结果**：信息成功发送，接收方收到信息。
2. **发送表情**

    * **步骤**：选择一个表情，点击发送按钮。
    * **预期结果**：表情成功发送，接收方收到表情。
3. **发送图片**

    * **步骤**：选择一张图片，点击发送按钮。
    * **预期结果**：图片成功发送，接收方收到图片。

‍

‍

#### 边界测试

1. **发送空信息**

    * **步骤**：不输入任何内容，点击发送按钮。
    * **预期结果**：提示不能发送空信息。
2. **发送超长信息**

    * **步骤**：输入超过最大长度限制的文本信息，点击发送按钮。
    * **预期结果**：提示信息过长，不能发送。

‍

‍

#### 性能测试

1. **高并发发送信息**

    * **步骤**：模拟多个用户同时发送信息。
    * **预期结果**：系统能够正常处理高并发请求，信息发送成功率高。

‍

‍

#### 异常测试

1. **网络断开**

    * **步骤**：在发送信息时断开网络连接。
    * **预期结果**：提示网络异常，信息发送失败。
2. **服务器异常**

    * **步骤**：在发送信息时模拟服务器异常。
    * **预期结果**：提示服务器异常，信息发送失败。

这些测试用例可以帮助全面验证QQ聊天发送信息功能的正确性和稳定性。

‍

‍

### 设计百度搜索框的测试用例

设计百度搜索框的测试用例时，可以考虑以下几个方面：功能性测试、性能测试、兼容性测试和安全性测试

‍

#### 功能性测试

1. **输入有效关键词**

    * 输入：`"GitHub"`​
    * 预期结果：显示与`"GitHub"`​相关的搜索结果。
2. **输入无效关键词**

    * 输入：`"asdkjfhaskjdfh"`​
    * 预期结果：显示没有找到相关结果的提示。
3. **输入空字符串并搜索**

    * 输入：`""`​
    * 预期结果：提示用户输入关键词。
4. **自动补全功能**

    * 输入：`"Git"`​
    * 预期结果：显示与`"Git"`​相关的自动补全建议。
5. **搜索历史记录**

    * 输入：之前搜索过的关键词`"Java"`​
    * 预期结果：显示搜索历史记录中的`"Java"`​。

‍

‍

#### 性能测试

1. **响应时间测试**

    * 输入：`"GitHub"`​
    * 预期结果：搜索结果在2秒内显示。
2. **高并发测试**

    * 模拟多个用户同时搜索`"GitHub"`​
    * 预期结果：系统能够正常处理并发请求，响应时间在合理范围内。

‍

‍

#### 兼容性测试

1. **不同浏览器测试**

    * 在Chrome、Firefox、Safari、Edge等浏览器中输入`"GitHub"`​并搜索
    * 预期结果：在所有浏览器中显示一致的搜索结果。
2. **不同设备测试**

    * 在PC、手机、平板等设备上输入`"GitHub"`​并搜索
    * 预期结果：在所有设备上显示一致的搜索结果。

‍

‍

#### 安全性测试

1. **输入SQL注入攻击字符串**

    * 输入：`"'; DROP TABLE users; --"`​
    * 预期结果：系统能够处理并拒绝恶意输入，不会执行任何SQL命令。
2. **输入XSS攻击字符串**

    * 输入：`"<script>alert('XSS')</script>"`​
    * 预期结果：系统能够处理并拒绝恶意输入，不会执行任何脚本。

这些测试用例可以帮助确保百度搜索框的功能正常、性能优越、兼容性好并且安全可靠。

‍

‍

‍

### 登陆测试用例设计

1. **正常登录**：使用正确的用户名和密码，验证是否能够顺利登录。
2. **用户名错误**：输入错误的用户名，系统应该给出明确的错误提示。
3. **密码错误**：输入正确的用户名和错误的密码，验证系统是否能给出合适的错误提示。
4. **空字段**：用户名和/或密码为空时，系统应该提示用户填写必要的信息。
5. **密码明文/加密显示**：验证密码是否在输入时以明文或加密显示，测试加密显示的正确性。
6. **多次错误登录**：输入错误的用户名或密码多次，检查系统是否有锁定账户或验证机制（如验证码、延时）。
7. **特殊字符测试**：使用特殊字符、空格、非ASCII字符等进行测试，确保系统能正确处理这些字符。
8. **自动跳转**：登录成功后，是否正确跳转到用户主页或指定页面。
9. **会话保持**：登录后退出，检查是否存在会话保持问题，验证用户是否会被自动登出。

‍

‍

### 微信扫码支付的测试

1. **正常支付**：通过扫描二维码，使用已绑定的微信账户成功完成支付。
2. **账户余额不足**：账户余额不足时，系统应提示“余额不足”并取消支付。
3. **二维码过期**：扫描过期二维码时，系统应提示“二维码已过期”。
4. **支付密码验证**：在支付过程中，输入错误的支付密码时，系统应提示密码错误并不给予支付。
5. **支付时网络中断**：在支付过程中如果网络中断，系统应能够正确处理并给出错误提示。
6. **重复支付**：多次扫描同一二维码时，系统应防止重复支付。
7. **扫码支付金额与实际金额不符**：支付金额与扫码时显示金额不一致时，系统应给出提示并终止支付。
8. **支付超时**：支付过程中如果超过一定时间未完成支付，系统应提示“支付超时”。
9. **不同支付方式切换**：支付时，用户切换至其他支付方式（如银行卡支付、信用卡支付等），验证系统是否能正确处理。
10. **扫码支付环境测试**：在不同操作系统（iOS、Android）、不同版本的微信中，扫码支付是否能够正常工作。

‍

‍

### 上传图片与导出文件测试点

1. **正常上传**：上传符合要求的图片文件（如支持的格式、大小等），确保上传成功。
2. **格式限制**：上传不符合格式要求的文件，如不支持的图片格式（如GIF、BMP等），系统应提示错误。
3. **大小限制**：上传超过文件大小限制的图片时，系统应提示文件过大，并且不允许上传。
4. **空文件上传**：上传空的图片文件（如0KB文件），系统应给出相应提示。
5. **多文件上传**：测试批量上传多张图片，系统是否能够处理多个文件的上传，并正确显示每个文件的上传状态。
6. **上传中断**：测试在图片上传过程中断开网络连接，验证系统是否能够处理上传失败的情况，并给出相关提示。
7. **重复上传**：上传相同的文件，系统是否会提示重复文件，或者能够自动覆盖或重命名。
8. **导出文件格式**：导出文件时，系统应支持所需的格式（如PDF、Excel、Word等），并能正确生成文件。
9. **文件内容完整性**：导出的文件内容是否与系统中展示的数据一致，确保没有遗漏或格式错乱。
10. **导出文件大小限制**：验证系统能否正确处理导出大文件的情况，是否有文件大小限制，并且在超出时给出提示。
11. **导出多种格式**：检查是否可以选择不同的文件格式导出，并确保每种格式正确无误。
12. **权限限制**：确保只有具有相应权限的用户才能上传图片或导出文件，防止未授权操作。

‍

‍

### 电商购物车测试

‍

1. **商品添加与删除功能**：

    * 测试用户是否能够顺利将商品添加到购物车，并能在购物车中删除不需要的商品。
    * 验证商品数量的增加和减少是否有效。
2. **购物车商品展示**：

    * 测试商品在购物车中的展示是否准确，包括商品名称、价格、数量和总金额等信息。
    * 验证商品图片、描述等是否与实际商品一致。
3. **购物车结算功能**：

    * 确保用户能够顺利从购物车进入结算页面，查看所有商品信息并确认订单。
    * 验证结算页面展示的商品清单是否与购物车一致。
4. **优惠券和促销活动**：

    * 测试优惠券、折扣、满减等促销活动是否能正确应用到购物车中的商品。
    * 验证促销活动的计算是否准确，并且符合条件。
5. **库存验证**：

    * 当购物车中商品的数量超过可用库存时，系统应给出提示，并且用户无法继续添加超出库存的商品。
    * 测试商品库存的实时更新，确保购物车和商品库存的一致性。
6. **购物车持久化**：

    * 测试购物车数据是否在用户登录、退出或页面刷新后得以保存。
    * 验证在不同设备或浏览器间，购物车信息是否能同步。
7. **跨页面数据一致性**：

    * 在用户跳转至其他页面后，购物车中的商品数据应能保持一致，并能够正常更新。
    * 测试商品数量和总价的实时更新，确保购物车反映最新的购物信息。
8. **用户身份与购物车关联**：

    * 测试未登录用户和已登录用户的购物车是否能正确处理。已登录用户的购物车数据应与账号关联，未登录用户的购物车数据应在登录后保存。
9. **清空购物车功能**：

    * 确保用户可以一键清空购物车，并且清空后购物车页面更新正确。
10. **兼容性测试**：

     * 测试购物车功能在不同设备、操作系统、浏览器上的兼容性，确保用户在任何设备上都能正常使用购物车。

‍

‍

### 电商限时秒杀如何设计测试核心点

‍

‍

1. **秒杀活动的开始与结束**：

    * 确保秒杀活动按预定时间开始和结束，活动开始前、结束后，商品的库存状态和价格应符合预期。
    * 测试秒杀开始时，商品是否能正常显示为秒杀商品，结束时，商品应恢复正常价格或下架。
2. **秒杀商品库存验证**：

    * 确保秒杀商品的库存数量与活动开始时设置的一致。
    * 测试秒杀商品被抢购完时，库存数量是否正确更新，且不能超卖。
3. **抢购速度与并发测试**：

    * 测试高并发情况下系统是否能够处理大量用户同时抢购秒杀商品，确保秒杀页面不崩溃，商品信息展示及时。
    * 验证秒杀商品在秒杀开始的瞬间是否有较长的响应时间，确保能快速加入购物车并完成支付。
4. **秒杀商品的页面显示**：

    * 测试秒杀商品页面是否能够正确显示商品的秒杀价格、剩余时间、库存等信息。
    * 确保商品信息在秒杀进行中不会错乱，且在抢购后，秒杀商品的信息能及时更新。
5. **用户参与条件验证**：

    * 验证只有符合条件的用户才能参与秒杀，例如限购数量、VIP用户优先等条件的验证。
    * 测试秒杀商品的限购数量，确保一个用户只能购买指定的数量。
6. **支付与订单生成**：

    * 测试秒杀订单是否能够及时生成，支付流程是否顺畅。
    * 验证秒杀商品的支付过程中的价格、库存是否正确，且秒杀商品的订单能正常提交。
7. **系统抗压能力**：

    * 进行压力测试，模拟大量用户同时访问秒杀页面和进行购买，确保系统在高并发场景下不崩溃，页面加载速度不会受到影响。
    * 测试秒杀过程中，系统能否正确处理超卖、库存不足等异常情况。
8. **秒杀结束后的库存恢复与展示**：

    * 测试秒杀结束后，商品的库存是否正确恢复（如秒杀未成功、库存超卖等情形）。
    * 验证秒杀结束后，用户是否还能看到秒杀商品并进行常规购买。
9. **优惠券和其他活动的冲突验证**：

    * 测试是否允许用户在秒杀商品上使用优惠券或其他折扣。
    * 验证在秒杀商品的购买过程中，优惠券是否会导致不合理的折扣情况。
10. **异常情况与错误处理**：

     * 测试秒杀过程中可能发生的各种异常情况，例如库存超卖、支付失败、秒杀页面崩溃等，并验证系统的错误提示是否清晰且及时。

‍
