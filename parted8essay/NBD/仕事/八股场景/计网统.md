> 计算机网络 + 操作系统

‍

‍

## 整体分层

‍

OSI

1. 物理
2. 数据链路
3. 网络
4. 运输
5. 会话
6. 表示
7. 应用

‍

TCP/IP

1. 网络接口    物理 + 数据链路
2. 网际    IP
3. 运输    TCP/UDP
4. 应用

‍

五层协议

1. 物理层
2. 数据链路层
3. 网络层
4. 运输层
5. 应用层

‍

‍

### OSI七层模型

* 网络七层模型是一个标准，而非实现。
* 网络四层模型是一个实现的应用模型。
* 网络四层模型由七层模型简化合并而来。

‍

* 物理层：底层数据传输，如网线；网卡标准。    数据被称为**比特流**（Bits）
* 数据链路层：定义数据的基本格式，如何传输，如何标识；如网卡MAC地址。    数据被称为**帧**（Frames）
* 网络层：定义IP编址，定义路由功能；如不同设备的数据转发。    数据被称做**包**（Packages）
* 传输层：端到端传输数据的基本功能；如 TCP、UDP。    数据被称作**段**（Segments）
* 会话层：控制应用程序之间会话能力；如不同软件数据分发给不同软件。
* 表示层：数据格式标识，基本压缩加密功能。
* 应用层：各种应用软件，包括 Web 应用。

‍

‍

### **TCP/IP四层模型**

实际应用中广泛使用的网络模型。TCP/IP 模型将 OSI 七层模型简化为四层，每层的功能如下：

‍

* **网络接口层**：物理网络上的数据传输（帧）。
* **网络层**：IP 地址和路由选择（包）。
* **传输层**：端到端的数据传输（段）。
* **应用层**：应用程序之间的通信（数据）。

‍

1. **网络接口层（Network Interface Layer）** ：

    * 负责在物理网络上发送和接收数据。
    * 包括 OSI 模型的物理层和数据链路层。
    * 数据被称为 **帧**（Frames）。
2. **网络层（Internet Layer）** ：

    * 负责逻辑地址（IP 地址）的分配和路由选择。
    * 包括 OSI 模型的网络层。
    * 数据被称为 **包**（Packets）。
3. **传输层（Transport Layer）** ：

    * 负责端到端的数据传输和流量控制。
    * 包括 OSI 模型的传输层。
    * 数据被称为 **段**（Segments）。
    * 主要协议有 TCP 和 UDP。
4. **应用层（Application Layer）** ：

    * 负责应用程序之间的通信。
    * 包括 OSI 模型的会话层、表示层和应用层。
    * 数据被称为 **数据**（Data）。
    * 主要协议有 HTTP、FTP、SMTP 等。

‍

‍

‍

#### PDU :比特、帧、数据包、数据段、报文

PDU：Protocol data unit，协议数据单元，指对等层协议之间交换的信息单元

PDU 再往上就是数据（data）

‍

在 OSI 模型里，PDU 和底下四层相关

* 物理层———**比特（Bit）**
* 数据链路层———**帧（Frame）**
* 网络层———**分组、数据包（Packet）**
* 传输层———**数据段（Segment）**

第五层或以上为**数据（data）**

也有一种说法是，应用层的信息称为**消息、报文（message）** ，表示完整的信息

‍

‍

‍

## <span data-type="text" style="color: var(--b3-font-color9);">物理/数链</span>

> 物理层考虑的是怎样才能在连接各种计算机的传输介质上传输数据比特流。现有的计算机网络中的硬件设备和传输媒体(介质)的种类非常多，而通信手段也有许多不同方式。物理层的作用正是要尽可能地屏蔽掉这些传输媒体和硬件设备的差异，使物理层上面的数据链路层感觉不到这些差异，这样就可使数据链路层只考虑如何完成本层的协议和服务，而不必考虑网络具体的传输媒体和通信手段是什么。

‍

> 数据链路层研究的是分组怎样从一台主机传送到另一台主机，但并不经过路由器转发。从整个互联网来看，局域网仍属于数据链路层的范围。数据传送单位是帧。
>
> 1、在两个相邻节点之间传送数据时，数据链路层将网络层交下来的**IP 数据报**添加首部和尾部组装成帧 **，在两个相邻节点间的链路上传送帧。** 每一帧包括数据和必要的控制信息【如同步信息，地址信息，差错控制等】。
>
> 2、数据链路层在收到一个帧后，通过控制信息检测收到的帧中是否有差错，如果没有就可从中提出**IP数据报**部分，上交给网络层 **。** 如果发现差错，数据链路层就简单地丢弃这个出了差错的帧，以避免继续在网络中传送下去白白浪费网络资源 【在接收数据时，控制信息使接收端能够知道一个帧从哪个比特开始和到哪个比特结束。控制信息还使接收端能够检测到所收到的帧中有误差错。如果发现差错，数据链路层就简单地丢弃这个出了差错的帧，以避免继续在网络中传送下去白白浪费网络资源。如果需要改正数据在链路层传输时出现差错（这就是说，数据链路层不仅要检错，而且还要纠错），那么就要采用可靠性传输协议来纠正出现的差错。这种方法会使链路层的协议复杂些】

‍

‍

#### 硬件: 集线器、网桥、交换机、路由器

* 网线是物理层的硬件
* 集线器（Hub）是**物理层**的硬件，连接所有的线路，广播所有信息
* 网桥（Bridge）是**数据链路层**的硬件。网桥隔离两个端口，不同的端口形成单独的冲突域，减少网内冲突。网桥在不同或相同类型的 LAN 之间存储并转发数据帧，根据 MAC 头部来决定转发端口，显然是数据链路层的设备
* 交换机（Switch）是**数据链路层**的硬件，相当于多端口的网桥。交换机内部存储 MAC 表，只会将数据帧发送到指定的目的地址
* 路由器（Router）是**网络层**的硬件，根据 IP 地址进行寻址，不同子网间的数据传输隔离

‍

‍

### ARP 协议应该属于哪一层？

一种说法是属于网络层，因为 IP 协议使用 ARP 协议

另一种说法是属于数据链路层，因为 MAC 地址是数据链路层的内容。

‍

在 OSI 模型中，ARP 协议属于链路层；而在 TCP/IP 模型中，ARP 协议属于网络层

‍

我同意后者说法, 是数据链路层的东西, 而不是IP那个

‍

‍

‍

## <span data-type="text" style="color: var(--b3-font-color9);">网络层</span>

> 选择合适的网间路由和交换结点，确保数据及时传送。网络层向上只提供简单灵活的、无连接的、尽最大努力交付的IP数据报服务. 在发送分组时不需要先建立连接，没有给分组进行上编号，所传送的分组可能出错、丢失、重复和失序。如果主机（即端系统）中的进程之间的通信需要是可靠的，那么就由网络的主机中的运输层负责（包括差错处理、流量控制等）

‍

‍

### 常见协议

IP（Internet Protocol，网际协议）：TCP/IP 协议中最重要的协议之一，主要作用是定义数据包的格式、对数据包进行路由和寻址，以便它们可以跨网络传播并到达正确的目的地。目前 IP 协议主要分为两种，一种是过去的 IPv4，另一种是较新的 IPv6，目前这两种协议都在使用，但后者已经被提议来取代前者。

‍

ARP（Address Resolution Protocol，地址解析协议）：ARP 协议解决的是网络层地址和链路层地址之间的转换问题。因为一个 IP 数据报在物理上传输的过程中，总是需要知道下一跳（物理上的下一个目的地）该去往何处，但 IP 地址属于逻辑地址，而 MAC 地址才是物理地址，ARP 协议解决了 IP 地址转 MAC 地址的一些问题。

‍

ICMP（Internet Control Message Protocol，互联网控制报文协议）：一种用于传输网络状态和错误消息的协议，常用于网络诊断和故障排除。例如，Ping 工具就使用了 ICMP 协议来测试网络连通性。

‍

NAT（Network Address Translation，网络地址转换协议）：NAT 协议的应用场景如同它的名称——网络地址转换，应用于内部网到外部网的地址转换过程中。具体地说，在一个小的子网（局域网，LAN）内，各主机使用的是同一个 LAN 下的 IP 地址，但在该 LAN 以外，在广域网（WAN）中，需要一个统一的 IP 地址来标识该 LAN 在整个 Internet 上的位置。

‍

OSPF（Open Shortest Path First，开放式最短路径优先） ）：一种内部网关协议（Interior Gateway Protocol，IGP），也是广泛使用的一种动态路由协议，基于链路状态算法，考虑了链路的带宽、延迟等因素来选择最佳路径。

‍

RIP(Routing Information Protocol，路由信息协议）：一种内部网关协议（Interior Gateway Protocol，IGP），也是一种动态路由协议，基于距离向量算法，使用固定的跳数作为度量标准，选择跳数最少的路径作为最佳路径。

‍

BGP（Border Gateway Protocol，边界网关协议）：一种用来在路由选择域之间交换网络层可达性信息（Network Layer Reachability Information，NLRI）的路由选择协议，具有高度的灵活性和可扩展性。

‍

‍

‍

## <span data-type="text" style="color: var(--b3-font-color9);">运输层</span>

> 负责向两台主机的进程之间提供通用的数据传输服务。
>
> 复用和分用。这里的“复用"是指在发送方不同的应用进程都可以使用同一个运输层协议传送数据(当然需要加上适当的首部)，而“分用”是指接收方的运输层在剥去报文的首部后能够把这些数据正确交付目的应用进程
>
> 向高层用户屏蔽了下面网络核心的细节【如网络拓扑、所采用的路由选择协议等】，**它使应用进程看见的就是好像在两个运输层实体之间有一条端到端的逻辑通信信道**。但这条逻辑通信信道对上层的表现却因运输层使用的不同协议而有很大的差别。当运输层釆用面向连接的TCP协议时，尽管下面的网络层是不可靠的（只提供尽最大努力服务），但这种逻辑通信信道就相当于一条全双工的可靠信道。但当运输层采用无连接的udp协议时，这种逻辑通信信道仍然是一条不可靠信道。

‍

‍

### TCP

> TCP（Transmission Control Protocol，传输控制协议）是一个面向连接的、可靠的、基于字节流的传输层协议。从它的概念中我们可以看出 TCP 的三个特点：
>
> * **面向连接**
> * **可靠性**
> * **面向字节流**。

‍

‍

‍

#### TCP对应的应用层协议

HTTP, 超文本传输协议

FTP, 文件传输协议

SMTP, 简单邮件传输协议，用来发送电子邮件

SSH 安全外壳协议,用于加密安全登陆

‍

‍

#### TCP头部信息

* 序号（32bit）：传输方向上字节流的字节编号。初始时序号会被设置一个随机的初始值（ISN），之后每次发送数据时，序号值 = ISN + 数据在整个字节流中的偏移。假设A -> B且ISN = 1024，第一段数据512字节已经到B，则第二段数据发送时序号为1024 + 512。用于解决网络包乱序问题。
* 确认号（32bit）：接收方对发送方TCP报文段的响应，其值是收到的序号值 + 1 (期望收到这个)
* 首部长（4bit）：标识首部有多少个4字节 * 首部长，最大为15，即60字节。
* 标志位（6bit）：

  * URG：标志紧急指针是否有效。
  * ACK：标志确认号是否有效（确认报文段）。用于解决丢包问题。
  * PSH：提示接收端立即从缓冲读走数据。
  * RST：表示要求对方重新建立连接（复位报文段）。
  * SYN：表示请求建立一个连接（连接报文段）。
  * FIN：表示关闭连接（断开报文段）。
* 窗口（16bit）：接收窗口。用于告知对方（发送方）本方的缓冲还能接收多少字节数据。用于解决流控。
* 校验和（16bit）：接收端用CRC检验整个报文段有无损坏。

‍

Sequence Number

> 称为「序列号」。用于 TCP 通信过程中某一传输方向上字节流的每个字节的编号，为了确保数据通信的有序性，避免网络中乱序的问题。接收端根据这个编号进行确认，保证分割的数据段在原始数据包的位置。初始序列号由自己定，而后绪的序列号由对端的 ACK 决定：SN_x = ACK_y (x 的序列号 = y 发给 x 的 ACK)。

Acknowledge Number

> 称为「确认序列号」。确认序列号是接收确认端所期望收到的下一序列号。确认序号应当是上次已成功收到数据字节序号加1，只有当标志位中的 ACK 标志为 1 时该确认序列号的字段才有效。主要用来解决不丢包的问题。

‍

‍

#### MSL、TTL、RTT

‍

MSL（Maximum segment lifetime）：报文最大生存时间。

任何 TCP 报文在网络上存在的最长时间，超过这个时间报文将被丢弃。实际应用中常用的设置是 30 秒，1 分钟和 2 分钟。

* 应用场景：TCP 四次挥手时，需要在 TIME-WAIT 状态等待 2MSL 的时间，可以保证本次连接产生的所有报文段都从网络中消失。

‍

TTL（Time to live）：IP 数据报在网络中可以存活的总跳数，称为“生存时间”，但并不是一个真正的时间。该域由源主机设置初始值，每经过一个路由器，跳数减 1，如果减至 0，则丢弃该数据包，同时发送 ICMP 报文通知源主机。取值范围 1-255，如果设置的 TTL 值小于传输过程中需要经过的路由器数量，则该数据包在传输中就会被丢弃。

‍

RTT（Round trip time）：客户端到服务端往返所花时间。RTT 受网络传输拥塞的变化而变化，由 TCP **动态地估算**。

‍

‍

#### 序列号和确认号变化

万能公式

发送的 TCP 报文：

* 公式一：序列号 = **上一次发送的序列号 + len（数据长度）** 。特殊情况，如果上一次发送的报文是 SYN 报文或者 FIN 报文，则改为 上一次发送的序列号 + 1。
* 公式二：确认号 = **上一次收到的报文中的序列号 + len（数据长度）** 。特殊情况，如果收到的是 SYN 报文或者 FIN 报文，则改为上一次收到的报文中的序列号 + 1。

‍

‍

#### 三次握手

‍

TCP 三次握手的执行流程

* SYN（Synchronize Sequence Numbers），同步序列编号；
* ACK（Acknowledge Character），确认字符；
* SEQ（Sequence Number），序列号。

‍

‍

##### 为什么三次

‍

* 第一次握手：客户端发送网络包，服务端收到了。 这样服务端就能得出结论：客户端的发送能力、服务端的接收能力是正常的。
* 第二次握手：服务端发包，客户端收到了。 这样客户端就能得出结论：服务端的接收、发送能力，客户端的接收、发送能力是正常的。不过此时服务器并不能确认客户端的接收能力是否正常。
* 第三次握手：客户端发包，服务端收到了。 这样服务端就能得出结论：客户端的接收、发送能力正常，服务器自己的发送、接收能力也正常。

因此，需要三次握手才能确认双方的接收与发送能力是否正常。

‍

> 其实就是非常简单的 "你能听到我讲话吗?" "我能听到, 你能听到我讲话吗?" "我能听到你讲话, 我们开始吧"

* 两次握手**只能保证单向连接是畅通的**。只有经过第三次握手，才能确保双向都可以接收到对方的发送的数据。两次握手接收方这里不能确定自己的的发送是正常的，发送方的接收是正常的。
* 防止已经失效的连接请求报文突然又传送到了服务器，从而产生错误。

‍

##### SIN/FIN不包含数据却要消耗序列号

凡是需要对端确认的，一定消耗TCP报文的序列号。SYN和FIN需要对端的确认，因此需要消耗一个序列号。

SYN作为三次握手的确认。FIN作为四次挥手的确认。如果没有序列号，会导致SYN请求多次重发，服务端多次处理，造成资源浪费

‍

‍

#### 四次挥手

‍

##### 为什么四次

因为只有在客户端和服务端都**没有数据要发送**的时候才能断开TCP。

而客户端发出FIN报文时只能保证客户端没有数据发了，服务端还有没有数据发客户端是不知道的。而服务端收到客户端的FIN报文后只能先回复客户端一个确认报文来告诉客户端我服务端已经收到你的FIN报文了，但我服务端还有一些数据没发完，等这些数据发完了服务端才能给客户端发FIN报文(所以**不能一次性将确认报文和FIN报文发给客户端**，就是这里多出来了一次)。

‍

> 任何一方都可以在数据传送结束后发出连接释放的通知，待对方确认后进入半关闭状态。当另一方也没有数据再发送的时候，则发出连接释放通知，对方确认后就完全关闭了TCP连接。
>
> A 和 B 打电话，通话即将结束后，A 说“我没啥要说的了”，B回答“我知道了”，但是 B 可能还会有要说的话，最后 B 说“我说完了”，A 回答“知道了”，这样通话才算结束。

‍

‍

##### 四次挥手过程

建立一个连接需要三次握手，而终止一个连接要经过四次挥手（也有将四次挥手叫做四次握手的）。这由TCP的**半关闭**（half-close）造成的。所谓的半关闭，其实就是TCP提供了 连接的一端在结束它的发送后 还能接收来自另一端数据 的能力。

TCP 的连接的拆除需要发送四个包，因此称为四次挥手(Four-way handshake)，客户端或服务器均可主动发起挥手动作。

‍

* **初始化状态**：客户端和服务端都在连接状态，接下来开始进行四次分手断开连接操作。
* **第一次分手**：第一次分手无论是客户端还是服务端都可以发起，因为 TCP 是全双工的。

> 假如客户端发送的数据已经发送完毕，发送 FIN = 1 **告诉服务端，客户端所有数据已经全发完了**，**服务端你可以关闭接收了**，但是如果你们服务端有数据要发给客户端，客户端照样可以接收的。此时客户端处于FIN = 1等待服务端确认释放连接状态。

* **第二次分手**：服务端接收到客户端的释放请求连接之后，**知道客户端没有数据要发给自己了**，**然后服务端发送ACK =**  **1告诉客户端收到你发给我的信息**，此时服务端处于 CLOSE_WAIT 等待关闭状态。（服务端先回应给客户端一声，我知道了，但服务端的发送数据能力即将等待关闭，于是接下来第三次就来了。）
* **第三次分手**：此时服务端向客户端把所有的数据发送完了，然后发送一个FIN = 1，**用于告诉客户端，服务端的所有数据发送完毕**，**客户端你也可以关闭接收数据连接了**。此时服务端状态处于LAST_ACK状态，来等待确认客户端是否收到了自己的请求。（服务端等客户端回复是否收到呢，不收到的话，服务端不知道客户端是不是挂掉了还是咋回事呢，所以服务端不敢关闭自己的接收能力，于是第四次就来了。）
* **第四次分手**：此时如果客户端收到了服务端发送完的信息之后，就发送ACK = 1，告诉服务端，客户端已经收到了你的信息。**有一个 2 MSL 的延迟等待**。

‍

‍

‍

##### 释放连接时等待2MSL的意义?

> **MSL**是Maximum Segment Lifetime的英文缩写，可译为“最长报文段寿命”，它是任何报文在网络上存在的最长时间，超过这个时间报文将被丢弃。

为了保证客户端发送的**最后一个ACK报文段能够到达服务器**。因为这个ACK有可能丢失，从而导致处在LAST-ACK状态的服务器收不到对FIN-ACK的确认报文。服务器会超时重传这个FIN-ACK，接着客户端再重传一次确认，重新启动时间等待计时器。最后客户端和服务器都能正常的关闭。假设客户端不等待2MSL，而是在发送完ACK之后直接释放关闭，一但这个ACK丢失的话，服务器就无法正常的进入关闭连接状态。

‍

‍

1. 保证客户端发送的最后一个ACK报文段能够到达服务端。 这个ACK报文段有可能丢失，使得处于LAST-ACK状态的B收不到对已发送的FIN+ACK报文段的确认，服务端超时重传FIN+ACK报文段，而客户端能在2MSL时间内收到这个重传的FIN+ACK报文段，接着客户端重传一次确认，重新启动2MSL计时器，最后客户端和服务端都进入到CLOSED状态，若客户端在TIME-WAIT状态不等待一段时间，而是发送完ACK报文段后立即释放连接，则无法收到服务端重传的FIN+ACK报文段，所以不会再发送一次确认报文段，则服务端无法正常进入到CLOSED状态。
2. 防止“**已失效的连接请求报文段**”出现在本连接中。 客户端在发送完最后一个ACK报文段后，再经过2MSL，就可以使本连接持续的时间内所产生的所有报文段都从网络中消失，使下一个新的连接中不会出现这种旧的连接请求报文段。

‍

‍

##### CLOSE-WAIT和TIME-WAIT存在的意义

Close-wait存在的意义:

 就是服务端还有数据要发送，这个时间内就是服务端发送完最后的数据

‍

Time-wait存在的意义

* 第一，这里同样是要考虑丢包的问题，如果第四次挥手的报文丢失，服务端没收到确认ack报文就会重发第三次挥手的报文，这样报文一去一回最长时间就是2MSL，所以需要等这么长时间来确认服务端确实已经收到了
* 第二，防止类似与“三次握手”中提到了的“已经失效的连接请求报文段”出现在本连接中。客户端发送完最后一个确认报文后，在这个2MSL时间中，就可以使本连接持续的时间内所产生的所有报文段都从网络中消失。这样新的连接中不会出现旧连接的请求报文。

‍

‍

‍

‍

#### 流量控制原理

* 目的是接收方通过**TCP头窗口字段**告知发送方本方可接收的最大数据量，用以解决发送速率过快导致接收方不能接收的问题。所以流量控制是点对点控制。
* TCP是双工协议，双方可以同时通信，所以发送方接收方各自维护一个发送窗和接收窗。

  * 发送窗：用来限制发送方可以发送的数据大小，其中发送窗口的大小由接收端返回的TCP报文段中窗口字段来控制，接收方通过此字段告知发送方自己的缓冲（受系统、硬件等限制）大小。
  * 接收窗：用来标记可以接收的数据大小。
* TCP是流数据，发送出去的数据流可以被分为以下四部分：已发送且被确认部分 | 已发送未被确认部分 | 未发送但可发送部分 | 不可发送部分，其中发送窗 = 已发送未确认部分 + 未发但可发送部分。接收到的数据流可分为：已接收 | 未接收但准备接收 | 未接收不准备接收。接收窗 = 未接收但准备接收部分。
* 发送窗内数据只有当接收到接收端某段发送数据的ACK响应时才移动发送窗，左边缘紧贴刚被确认的数据。接收窗也只有接收到数据且最左侧连续时才移动接收窗口。

‍

‍

#### 拥塞控制&流量控制4个家伙的区别

‍

> 为了进行拥塞控制，TCP 发送方要维持一个 拥塞窗口(cwnd) 的状态变量。拥塞控制窗口的大小取决于网络的拥塞程度，并且动态变化。发送方让自己的发送窗口取为拥塞窗口和接收方的接受窗口中较小的一个。**慢开始和拥塞避免都是基于窗口的拥塞控制**

‍

==区别 2 -&gt; 4==

拥塞控制

* **慢开始：**  慢开始算法的思路是当主机开始发送数据时，如果立即把大量数据字节注入到网络，那么可能会引起网络阻塞，因为现在还不知道网络的符合情况。经验表明，较好的方法是先探测一下，即由小到大逐渐增大发送窗口，也就是由小到大逐渐增大拥塞窗口数值。cwnd初始值为1，每经过一个传播轮次，cwnd加倍
* **拥塞避免：**  拥塞避免算法的思路是让拥塞窗口cwnd缓慢增大，即每经过一个往返时间RTT就把发送放的cwnd加 1

  拥塞避免算法：让拥塞窗口cwnd缓慢地增大，即每经过一个往返时间RTT就把发送方的拥塞窗口cwnd加1，而不是加倍。这样拥塞窗口cwnd按线性规律缓慢增长，比慢开始算法的拥塞窗口增长速率缓慢得多。

‍

流量控制

* **快重传**

  要求接收方每收到一个失序的报文段后就立即发出重复确认（为的是使发送方及早知道有报文段没有到达对方）而不要等到自己发送数据时才进行捎带确认
* **快恢复**

  发送方知道现在只是丢失了个别的报文段。于是不启动慢开始，而是执行**快恢复算法**。这时，发送方调整门限值，同时设置拥塞窗口，并开始执行拥塞避免算法

‍

‍

#### 可靠传输? 

‍

补充

1. 数据包校验
2. 对失序数据包重新排序（TCP报文具有序列号）
3. 丢弃重复数据
4. 应答机制：接收方收到数据之后，会发送一个确认（通常延迟几分之一秒）；
5. 超时重发：发送方发出数据之后，启动一个定时器，超时未收到接收方的确认，则重新发送这个数据；
6. 流量控制：确保接收端能够接收发送方的数据而不会缓冲区溢出

‍

##### 第一种回答

* **确认和重传**：接收方收到报文就会确认，发送方发送一段时间后没有收到确认就会重传。
* **数据校验**：TCP报文头有校验和，用于校验报文是否损坏。
* **数据合理分片和排序**：tcp会按最大传输单元(MTU)合理分片，接收方会缓存未按序到达的数据，重新排序后交给应用层。而UDP：IP数据报大于1500字节，大于MTU。这个时候发送方的IP层就需要分片，把数据报分成若干片，是的每一片都小于MTU。而接收方IP层则需要进行数据报的重组。由于UDP的特性，某一片数据丢失时，接收方便无法重组数据报，导致丢弃整个UDP数据报。
* **流量控制**：当接收方来不及处理发送方的数据，能通过滑动窗口，提示发送方降低发送的速率，防止包丢失。
* **拥塞控制**：当网络拥塞时，通过拥塞窗口，减少数据的发送，防止包丢失。

‍

‍

##### 第二种回答

* 建立连接（标志位）：通信前确认通信实体存在。
* 序号机制（序号、确认号）：确保了数据是按序、完整到达。
* 数据校验（校验和）：CRC校验全部数据。
* 超时重传（定时器）：保证因链路故障未能到达数据能够被多次重发。
* 窗口机制（窗口）：提供流量控制，避免过量发送。
* 拥塞控制：同上。

‍

‍

##### 第三种回答

‍

**首部校验**  
这个校验机制能够确保数据传输不会出错吗？ 答案是不能。

‍

**原因**

TCP协议中规定，TCP的首部字段中有一个字段是校验和，发送方将伪首部、TCP首部、TCP数据使用累加和校验的方式计算出一个数字，然后存放在首部的校验和字段里，接收者收到TCP包后重复这个过程，然后将计算出的校验和和接收到的首部中的校验和比较，如果不一致则说明数据在传输过程中出错。

这就是TCP的数据校验机制。 但是这个机制能够保证检查出一切错误吗？**显然不能**。

因为这种校验方式是累加和，也就是将一系列的数字（TCP协议规定的是数据中的每16个比特位数据作为一个数字）求和后取末位。 但是小学生都知道A+B=B+A，假如在传输的过程中有前后两个16比特位的数据前后颠倒了（至于为什么这么巧合？我不知道，也许路由器有bug？也许是宇宙中的高能粒子击中了电缆？反正这个事情的概率不为零，就有可能会发生），那么校验和的计算结果和颠倒之前是一样的，那么接收端肯定无法检查出这是错误的数据。

‍

**解决方案**

传输之前先使用**MD5加密数据**获得摘要，跟数据一起发送到服务端，服务端接收之后对数据也进行MD5加密，如果加密结果和摘要一致，则认为没有问题

‍

‍

‍

#### 粘包现象

**TCP粘包**是指发送方发送的若干包数据到接收方接收时粘成一包，从接收缓冲区看，后一包数据的头紧接着前一包数据的尾。

不是所有的粘包现象都需要处理，若传输的数据为**不带结构的连续流数据**（如文件传输）- 流式传输，则不必把粘连的包分开（简称分包）。但在实际工程应用中，传输的数据一般为带结构的数据，这时就需要做分包处理。分包一般难度较大, 所以尽量避免粘包

‍

‍

##### 原因

一个完整的业务可能会被TCP拆分成多个包进行发送，也有可能把多个小的包封装成一个大的数据包发送，这个就是TCP的拆包和粘包问题。

‍

1.UDP协议的保护消息边界使得每一个消息都是独立的

2.而tcp是基于流的传输，流传输却把数据当作一串数据流，他不认为数据是一个一个的消息

3.发送端需要等缓冲区满才发送出去，造成粘包

4.接收方不及时接收缓冲区的包，造成多个包粘包

‍

‍

具体点：**分发送方和接收方来看**

（1）发送方引起的粘包是由TCP协议本身造成的，TCP为提高传输效率，发送方往往要收集到足够多的数据后才发送一包数据。若连续几次发送的数据都很少，通常TCP会根据优化算法把这些数据合成一包后一次发送出去，这样接收方就收到了粘包数据。

（2）接收方引起的粘包是由于接收方用户进程不及时接收数据，从而导致粘包现象。这是因为接收方先把收到的数据放在系统接收缓冲区，用户进程从该缓冲区取数据，若下一包数据到达时前一包数据尚未被用户进程取走，则下一包数据放到系统接收缓冲区时就接到前一包数据之后，而用户进程根据预先设定的缓冲区大小从系统接收缓冲区取数据，这样就一次取到了多包数据。

‍

1、应用程序写入数据的字节大小大于套接字发送缓冲区的大小.

2、进行MSS大小的TCP分段。( MSS=TCP报文段长度-TCP首部长度)

3、以太网的payload大于MTU进行IP分片。（ MTU指：一种通信协议的某一层上面所能通过的最大数据包大小。）

‍

* 由TCP**连接复用**造成的粘包问题。
* 因为TCP默认会使用**Nagle算法**，此算法会导致粘包问题。

  * 只有上一个分组得到确认，才会发送下一个分组；
  * 收集多个小分组，在一个确认到来时一起发送。
* **数据包过大**造成的粘包问题。
* 流量控制，**拥塞控制**也可能导致粘包。
* **接收方不及时接收缓冲区的包，造成多个包接收**

---

‍

‍

##### 解决方法

（1）对于发送方引起的粘包现象，用户可通过编程设置来避免，TCP提供了强制数据立即传送的操作指令push，TCP程序收到该操作指令后，就**立即将本段数据发送出去**，而不必等待发送缓冲区满；

（2）对于接收方引起的粘包，则可通过优化程序设计、精简接收进程工作量、提高接收进程优先级等措施，使其及时接收数据，从而尽量避免出现粘包现象；

（3）由接收方控制，将一包数据按结构字段，人为控制分多次接收，然后合并，通过这种手段来避免粘包。

‍

1. **Nagle算法**问题导致的，需要结合应用场景适当关闭该算法
2. 尾部标记序列。通过特殊标识符表示数据包的边界，例如\n\r，\t，或者一些隐藏字符。
3. 头部标记分步接收。在TCP报文的头部加上表示数据长度。
4. 应用层发送数据时**定长**发送。

‍

‍

‍

‍

### UDP

‍

#### UDP对应的应用层协议

DHCP协议：动态主机配置协议，动态配置IP地址

NTP协议：网络时间协议，用于网络时间同步

RIP（路由选择协议）

DNS

‍

‍

#### 怎么让UDP可靠？（QUIC、HTTPS 3.0等）

‍

* **UDP特性与挑战**

  * UDP本身是无连接、无保证交付的协议，不保证数据包顺序、完整性和重复数据删除。
  * 为了在UDP上实现可靠性，需要在应用层或协议层增加额外机制。
* **QUIC协议**

  * **背景**：由Google提出，基于UDP构建，旨在提供与TCP相当的可靠传输，同时减少连接建立延迟。
  * **特性**：

    * 内置拥塞控制、流控制和重传机制；
    * 采用0-RTT/1-RTT连接建立，减少延迟；
    * 支持多路复用和数据包加密。
  * **应用**：HTTP/3基于QUIC协议，利用UDP实现低延迟和高可靠性的数据传输。
* **其他实现思路**

  * **自定义重传机制**：在UDP上添加序号、确认应答（ACK）和超时重传机制，确保数据包正确传输；
  * **FEC（前向纠错）** ：通过在数据中增加冗余校验码，允许接收端在丢包时重构数据；
  * **结合TLS/DTLS**：为UDP传输加密和认证层，确保数据安全和传输可靠性。

‍

总结来说，通过在UDP协议上增加传输控制、错误检测、重传和拥塞控制等机制，可以实现类似于TCP的可靠传输，而QUIC就是在这种理念下诞生的下一代传输协议，已经应用于HTTP/3中

‍

‍

‍

### TCP和UDP的区别

1、TCP面向连接（如打电话要先拨号建立连接）;UDP是无连接的，即发送数据之前不需要建立连接

2、TCP提供可靠的服务。也就是说，通过TCP连接传送的数据，无差错，不丢失，不重复，且按序到达;UDP尽最大努力交付，即不保证可靠交付

3、TCP面向字节流，实际上是TCP把数据看成一连串无结构的字节流;UDP是面向报文的

UDP没有拥塞控制，因此网络出现拥塞不会使源主机的发送速率降低（对实时应用很有用，如IP电话，实时视频会议等）

4、每一条TCP连接只能是点到点的; UDP支持一对一，一对多，多对一和多对多的交互通信

5、TCP首部开销20字节; UDP的首部开销小，只有8个字节

6、TCP的逻辑通信信道是全双工的可靠信道，UDP则是不可靠信道

7、UDP是面向报文的，发送方的UDP对应用层交下来的报文，不合并，不拆分，只是在其上面**加上首部**后就交给了下面的网络层，论应用层交给UDP多长的报文，它统统发送，一次发送一个。而对接收方，接到后直接去除首部，交给上面的应用层就完成任务了。因此，它需要应用层控制报文的大小

TCP是面向字节流的，它把上面应用层交下来的数据看成无结构的字节流会发送，可以想象成流水形式的，发送方TCP会将数据放入“蓄水池”（缓存区），等到可以发送的时候就发送，不能发送就等着TCP会根据当前网络的拥塞状态来确定每个报文段的大小。

‍

简单点

* **可靠性**，TCP 有“状态性”和“可控制性”可以保证消息不重复、按顺序、不丢失的发送和接收，而 UDP 则不能保证消息的可靠性；
* **连接**，TCP 是面向连接的传输层协议，传输数据前先要建立连接，而 UDP 发送数据之前无需建立连接；
* **服务对象**，TCP 服务的对象为一对一的双端应用，而 UDP 可以应用于一对一、一对多和多对多的通信场景；
* **效率**，TCP 的传输效率较低，而 UDP 的传输效率较 高；
* **流量控制**，TCP 有滑动窗口可以用来控制流量，而 UDP 则不具备流量控制的能力；
* **报文**，TCP 是面向字节流的传输层协议，而 UDP 是面向报文的传输层协议；
* **应用场景**，TCP 的应用场景是对消息准确性和顺序要求较高的场景，而 UDP 则是应用于对通信效率较高、准确性要求相对较低的场景。

‍

‍

‍

## <span data-type="text" style="color: var(--b3-font-color9);">应用层</span>

> 不同的网络应用的应用进程之间，还需要有不同的通信规则. 每个应用层协议都是为了解决某一类应用问题， (而问题的解决又必须通过位于不同主机中的多个应用进程之间的通信和协同工作来完成) 。应用进程之间的这种通信必须遵循严格的规则。应用层的具体内容就是精确定义这些通信规则。  
> 运输层是两台主机间进程的交互。应用层是为了更加细化不同网络应用的交互规则。

‍

‍

### 常用协议

应用层常见协议：

HTTP（Hypertext Transfer Protocol，超文本传输协议）：基于 TCP 协议，是一种用于传输超文本和多媒体内容的协议，主要是为 Web 浏览器与 Web 服务器之间的通信而设计的。当我们使用浏览器浏览网页的时候，我们网页就是通过 HTTP 请求进行加载的。

‍

FTP（File Transfer Protocol，文件传输协议） : 基于 TCP 协议，是一种用于在计算机之间传输文件的协议，可以屏蔽操作系统和文件存储方式。

> 不安全的协议，因为它在传输过程中不会对数据进行加密。建议在传输敏感数据时使用更安全的协议，如 SFTP

‍

SSH（Secure Shell Protocol，安全的网络传输协议）：基于 TCP 协议，通过加密和认证机制实现安全的访问和文件传输等业务

‍

DNS（Domain Name System，域名管理系统）: 基于 UDP 协议，用于解决域名和 IP 地址的映射问题。

‍

‍

### DNS

DNS（Domain Name System，域名系统），因特网上作为**域名和IP地址相互映射**的一个**分布式数据库**，能够使用户更方便的访问互联网，而不用去记住能够被机器直接读取的IP数串。

通过主机名，最终得到该主机名对应的IP地址的过程叫做**域名解析**（或主机名解析）

将主机域名转换为ip地址，使用UDP传输(快啊)

‍

**过程**

> 1） 当用户输入域名时，**浏览器**先检查自己的缓存中是否有 这个域名映射的ip地址，有解析结束
>
> 2） 若没命中，则检查**操作系统缓存**（如Windows的hosts）中有没有解析过的结果，有解析结束
>
> 3） 若无命中，则请求**本地域名服务器解析**（LDNS）。 (作代理进行后续遍历)
>
> 4） 若LDNS没有命中就直接跳到**根域名服务器**请求解析。根域名服务器返回给LDNS一个 主域名服务器地址。
>
> 5） 此时LDNS再发送请求给上一步返回的gTLD（通用顶级域）， 接受请求的gTLD查找并返回这个域名对应的Name Server的地址
>
> 6） Name Server根据映射关系表找到目标ip，返回给LDNS
>
> 7） LDNS缓存这个域名和对应的ip， 把解析的结果返回给用户，用户根据TTL值缓存到本地系统缓存中，域名解析过程至此结束

‍

‍

**总结**

浏览器缓存，系统缓存，路由器缓存，IPS服务器缓存，根域名服务器缓存，顶级域名服务器缓存，主域名服务器缓存。

主机向本地域名服务器的查询一般都是采用递归查询。

本地域名服务器向根域名服务器的查询的迭代查询。

‍

‍

‍

#### 负载均衡策略

当一个网站有足够多的用户的时候，假如每次请求的资源都位于同一台机器上面，那么这台机器随时可能会蹦掉

‍

处理办法就是用DNS负载均衡技术

‍

原理是在**DNS服务器中为同一个主机名配置多个IP地址,在应答DNS查询时,DNS服务器对每个查询将以DNS文件中主机记录的IP地址按顺序返回不同的解析结果,将客户端的访问引导到不同的机器上去,使得不同的客户端访问不同的服务器**, 从而达到负载均衡的目的

‍

‍

### 服务器缓存

* 缓解服务器压力；
* 降低客户端获取资源的延迟：缓存通常位于内存中，读取缓存的速度更快。并且缓存服务器在地理位置上也有可能比源服务器来得近，例如浏览器缓存。

‍

‍

**实现方法**

* 让代理服务器进行缓存
* 让客户端浏览器进行缓存

‍

‍

### URI 和 URL 的区别?

* URI(Uniform Resource Identifier) 是统一资源标志符，可以唯一标识一个资源。
* URL(Uniform Resource Locator) 是统一资源定位符，可以提供该资源的路径。它是一种具体的 URI，即 URL 可以用来标识一个资源，而且还指明了如何 locate 这个资源。

URI 的作用像身份证号一样，URL 的作用更像家庭住址一样。URL 是一种具体的 URI，它不仅唯一标识资源，而且还提供了定位该资源的信息。

‍

URL=协议+服务器地址+URI

应用协议://服务器地址信息/请求资源的抽象路径

‍

‍

`

## JavaWeb日常

‍

### GET 和 POST 的区别，你知道哪些？

‍

1. 语义上get是获取数据，post是修改数据
2. get把请求的数据放在url上， 以?分割URL和传输数据，参数之间以&相连，所以get不太安全。而post把数据放在**HTTP的包体**内（requrest body）

    格式：GET 请求的参数通常放在 URL 中，形成查询字符串（querystring），而 POST 请求的参数通常放在请求体（body）中，可以有多种编码格式，如 application/x-www-form-urlencoded、multipart/form-data、application/json 等。GET 请求的 URL 长度受到浏览器和服务器的限制，而 POST 请求的 body 大小则没有明确的限制。不过，实际上 GET 请求也可以用 body 传输数据，只是并不推荐这样做，因为这样可能会导致一些兼容性或者语义上的问题。
3. get提交的数据最大是2k（限制实际上取决于浏览器）， post理论上没有限制。
4. GET产生一个TCP数据包，浏览器会把http header和data一并发送出去，服务器响应200(返回数据); POST产生两个TCP数据包，浏览器先发送header，服务器响应100 continue，浏览器再发送data，服务器响应200 ok(返回数据)。
5. 缓存：由于 GET 请求是幂等的，它可以被浏览器或其他中间节点（如代理、网关）缓存起来，以提高性能和效率。而 POST 请求则不适合被缓存，因为它可能有副作用，每次执行可能需要实时的响应。GET请求会被**浏览器主动缓存**，而POST不会，除非手动设置。
6. 安全性：GET 请求和 POST 请求如果使用 HTTP 协议的话，那都不安全，因为 HTTP 协议本身是**明文传输**的，必须使用 HTTPS 协议来加密传输数据。另外，GET 请求相比 POST 请求更容易泄露敏感数据，因为 GET 请求的参数通常放在 URL 中
7. **本质区别：GET是幂等的，而POST不是幂等的**

    > 这里的幂等性：幂等性是指一次和多次请求某一个资源应该具有同样的副作用。简单来说意味着对同一URL的多个请求应该返回同样的结果。
    >

‍

正因为它们有这样的区别，所以不应该且**不能用get请求做数据的增删改这些有副作用的操作**。因为get请求是幂等的，**在网络不好的隧道中会**​**==尝试重试==**。如果用get请求增数据，会有**重复操作**的风险，而这种重复操作可能会导致副作用（浏览器和操作系统并不知道你会用get请求去做增操作）

‍

‍

‍

### Cookie

‍

HTTP 协议是**无状态**的，主要是为了让 HTTP 协议尽可能简单，使得它能够处理大量事务，HTTP/1.1 引入 Cookie 来保存状态信息。

Cookie 是**服务器发送到用户浏览器并保存在本地的一小块数据**，它会在浏览器之后向同一服务器再次发起请求时被携带上，用于告知服务端两个请求是否来自同一浏览器。由于之后每次请求都会需要携带 Cookie 数据，因此会带来额外的性能开销（尤其是在移动环境下）。

‍

新的浏览器 API 已经允许开发者直接将数据存储到本地，如使用 Web storage API（本地存储和会话存储）或 IndexedDB

‍

‍

### Session

利用 Session 存储在服务器端，存储在**服务器端的信息更加安全**。

‍

我项目这样做的: 

==使用 Session 维护用户登录状态的过程==

1. 用户进行登录时，用户提交包含用户名和密码的表单，放入 HTTP 请求报文中；
2. 服务器验证该用户名和密码，如果正确则把用户信息存储到 Redis 中，它在 Redis 中的 Key 称为 Session ID；
3. 服务器返回的响应报文的 Set-Cookie **首部字段包含了这个 Session ID**，客户端收到响应报文之后将该 **Cookie 值存入浏览器中**；
4. 客户端之后对同一个服务器进行请求时会**包含该 Cookie 值**，服务器收到之后提取出 Session ID，从 Redis 中取出用户信息，继续之前的业务操作。

‍

抽象地概括一下：一个 cookie 可以认为是一个「变量」，形如 name=value，存储在浏览器；一个 session 可以理解为一种数据结构，多数情况是「映射」（键值对），存储在服务器上。

‍

‍

‍

### Cookie | Session 区别

‍

Cookie：

* 存在浏览器里 (可以看到)，可以设置过期时间
* 每次访问服务器时，浏览器会自动在 header 中携带 cookie
* 如果浏览器禁用了 cookie，可以使用 **URL 重写机制**，将信息保存在 URL 里

‍

Session:

* 存在服务端，由服务器维护，一段时间后 session 就失效了
* **本质上，session 还是通过 cookie 实现的**。浏览器的 cookie 中只保存一个 `sessionId`​，所有其他信息均保存在服务端，由 `sessionId`​ 标识
* Session 失效，其实是服务器设置了失效时间。如果用户长时间不和服务器交互（比如 30 分钟），那么 session 就会被销毁；交互的话，就会刷新 session

‍

‍

#### 如何去选择 （适用场景）

* Cookie 只能存储 ASCII 码字符串，而 Session 则可以存储任何类型的数据，因此在考虑**数据复杂性**时首选 Session
* Cookie 存储在浏览器中，容易被恶意查看。如果非要将一些隐私数据存在 Cookie 中，可以将 Cookie 值进行加密，然后在服务器进行解密；
* 对于大型网站，如果用户所有的信息都存储在 Session 中，那么**开销是非常大的**，因此不建议将所有的用户信息都存储到 Session 中

‍

‍

### 端口有效范围是多少到多少？

0-1023为知名端口号，比如其中HTTP是80，FTP是20（数据端口）、21（控制端口）

UDP和TCP报头使用两个字节存放端口号，所以端口号的有效范围是从0到65535。动态端口的范围是从 1024 到 65535 (2^16-1)

‍

‍

### 绕过 Cookie 继续运行 Session

**默认情况下禁用 Cookie 后，Session 是无法正常使用的**。

> 这是因为大多数 Web 服务器都是依赖于 Cookie 来传递 Session 的会话 ID 的。客户端浏览器禁用 Cookie 时，服务器将无法把会话 ID 发送给客户端，客户端也无法在后续请求中携带会话 ID 返回给服务器，从而导致服务器无法识别用户会话。

‍

两种解决方案

* **URL 中携带 SessionID**：可以通过 URL 重写的方式将 Session ID 添加到所有的 URL 中。服务器生成 Session ID 后，将其作为 URL 的一部分传递给客户端，客户端在后续的请求中将 Session ID 带在 URL 中。服务器端需要相应地解析 URL 来获取 Session ID，并维护用户的会话状态。
* **隐藏表单字段传递 SessionID**：将 Session ID 添加到 HTML 表单的隐藏字段中。在每个表单中添加一个隐藏的字段，保存 Session ID，客户端提交表单时会将 Session ID 随表单数据一起发送到服务器，服务器通过解析表单数据中的 Session ID 来获取用户的会话状态。

‍

‍

## 网络行为

‍

### 301、302、307 重定向的原理

返回的 Header 中有一个 `Location`​ 字段指向目标 URL，浏览器会重定向到这个 URL

‍

### 重定向和转发区别

**重定向过程**: 第一次，客户端request一个网址, 服务器响应，并response回来，告诉浏览器，你应该去别一个网址

‍

1、重定向是两次请求，转发是一次请求。因此转发的速度要快于重定向  

2、重定向之后地址栏上的地址会发生变化，变化成第二次请求的地址，转发之后地址栏上的地址不会变化，还是第一次请求的地址

3、**转发是服务器行为**，**重定向是客户端行为**

4、重定向时的网址可以是任何网址，转发的网址必须是本站点的网址

‍

详细的:

‍

#### 定义不同

**请求转发（Forward）：发生在服务端程序内部，当服务器端收到一个客户端的请求之后，会先将请求，转发给目标地址，再将目标地址返回的结果转发给客户端。**  而客户端对于这一切毫无感知的，这就好比，张三（客户端）找李四（服务器端）借钱，而李四没钱，于是李四又去王五那借钱，并把钱借给了张三，整个过程中张三只借了一次款，剩下的事情都是李四完成的，这就是请求转发。

**请求重定向（Redirect）：请求重定向指的是服务器端接收到客户端的请求之后，会给客户端返回了一个临时响应头，这个临时响应头中记录了，客户端需要再次发送请求（重定向）的 URL 地址，客户端再收到了地址之后，会将请求发送到新的地址上，这就是请求重定向。**  这就好像张三（客户端）找李四（服务器端）借钱，李四没钱，于是李四就告诉张三，“我没钱，你去王五那借“，于是张三又去王五家借到了钱，这就是请求重定向

‍

#### 请求方不同

从上面请求转发和请求重定向的定义，我们可以看出：**请求转发是服务器端的行为**，服务器端代替客户端发送请求，并将结果返回给客户端；**而请求重定向是客户端的行为**

‍

‍

#### 数据共享不同

请求转发是服务器端实现的，所以整个执行流程中，客户端（浏览器端）只需要发送一次请求，因此整个交互过程中使用的都是同一个 Request 请求对象和一个 Response 响应对象，所以整个请求过程中，请求和返回的数据是共享的；而请求重定向客户端发送两次完全不同的请求，所以两次请求中的数据是不同的

‍

#### 最终 URL 地址不同

请求转发是服务器端代为请求，再将结果返回给客户端的，所以整个请求的过程中 URL 地址是不变的；而请求重定向是服务器端告诉客户端，“你去另一个地访问去”，所以浏览器会重新再发送一次请求，因此客户端最终显示的 URL 也为最终跳转的地址，而非刚开始请求的地址，所以 URL 地址发生了改变。

‍

#### 代码实现不同

SpringBoot 中

‍

请求转发的实现代码

```java
@RequestMapping("/fw")
public void forward(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException {
    request.getRequestDispatcher("/index.html").forward(request, response);
}
```

‍

而请求重定向的实现代码

```java
@RequestMapping("/rt")
public void redirect(HttpServletRequest request, HttpServletResponse response) throws IOException {
    response.sendRedirect("/index.html");
}
```

‍

‍

‍

### 两台主机通信过程(纯计网无JavaWeb)

‍

‍

#### 不同局域网的两台主机

以主机 ping 一个域名为例，过程如下：

‍

* [主机] [应用层] 通过 DNS 协议获取域名的 IP 地址；
* [主机] [网络层] 构造 IP 数据包，源 IP 为本机 IP，目的 IP 为域名 IP；
* [主机] [网络层] 根据路由表，选择下一跳的 IP 地址，即当前局域网的网关；
* [主机] [链路层] 根据 ARP 表，查找网关 IP 的 MAC 地址；在 IP 数据包外面包一层 MAC 地址；
* ‍
* [局域网] 根据 MAC 地址，上一步的报文最终会发送到当前局域网的网关；
* [网关] [网络层] 网关查看数据包的目的 IP 地址，重复上述 2～3 步，继续发给下一跳；
* ‍
* [互联网] 中间经过若干个下一跳主机，最终数据包发送到域名所在的网络中心的网关；
* ‍
* [网关] [网络层] 网络中心的网关查看数据包的目的 IP 地址，根据路由表发现目的 IP 对应的 `Gateway`​ 为 0.0.0.0 ，这表明目的机器和自己位于同一个局域网内；
* [网关] [链路层] 根据 ARP 表，查找目的 IP 的 MAC 地址，构造链路层报文；
* ‍
* [局域网] 根据 MAC 地址，上一步的报文最终会发送到目的主机；
* [目的主机] [网络层] 目的主机查看数据包中的目的 IP，发现是给自己的，解析其内容，过程结束。

‍

‍

#### 同局域网内的两台主机

两台主机通过网线、网桥或者交换机连接，就构成了一个局域网

网桥或交换机的作用是连接多台主机，隔离不同的端口，每个端口形成单独的冲突域。当主机连接到网桥或交换机端口的时候，这些设备会要求主机上报 MAC 地址，并在设备内保存 MAC 地址与端口的对应关系。

同局域网内的两台主机进行通信时，只需要根据 ARP 协议获取目的主机的 MAC 地址，构造链路层报文。报文会经过网桥或交换机，后两者根据目的 MAC 地址，在 MAC 地址表里查询目的端口，然后将报文从目的端口转发给对应的主机。

‍

‍

**注意**：

(1) 交换机是链路层的设备，主要根据 MAC 地址进行转发、隔离冲突域；不具有路由功能，不记录路由表。这类设备也称为**二层交换机**。如果只使用二层交换机、不使用路由器来构建局域网，需要为交换机和每台主机分配同属于一个子网的静态 IP

(2) 路由器工作在 OSI 的第三层网络层，记录路由表，并以此控制数据传输过程

(3) 有些交换机也具有路由功能，记录了路由表，能够简化路由过程、实现高速转发。这类设备也称为**三层交换机**

‍

‍

‍

### URL到页面过程(无JavaWeb)

如果是纯计网, 最好也要加入, 浏览器(客户端) + 操作系统(客户端) + 服务器(服务端) 的三者角色

‍

![15826961451953](assets/net-img-15826961451953-20241022133709-95kxvv4.jpg)​

描述

1. 用户在某个标签页输入 URL 并回车后，浏览器主进程会新开一个网络线程，发起 HTTP 请求
2. 浏览器会进行 DNS 查询，将域名解析为 IP 地址
3. 浏览器获得 IP 地址(以及默认的端口号)后，首先尝试http然后调用Socket建立TCP连接
4. 经过三次握手成功建立连接后，开始传送数据，如果正是http协议的话，就返回就完事了
5. 如果不是http协议，服务器会返回一个5开头的的重定向消息，告诉我们用的是https，那就是说IP没变，但是端口号从80变成443了，好了，再四次挥手关闭TCP，完事. 除了上述的端口号从80变成443之外，还会采用SSL的加密技术来保证传输数据的安全性，保证数据传输过程中不被修改或者替换之类的: 这次依然是三次握手，沟通好双方使用的认证算法，加密和检验算法，在此过程中也会检验对方的CA安全证书
6. 浏览器向服务器发起 HTTP 请求
7. 服务器处理请求，返回 HTTP 响应, css, html文件回来
8. 浏览器的渲染进程解析数据并绘制页面
9. 如果遇到 JS/CSS/图片 等静态资源的引用链接，重复上述过程，向服务器请求这些资源

‍

简单些

* 根据域名，进行DNS域名解析
* 拿到解析的IP地址，建立TCP连接
* 向IP地址，发送HTTP请求
* 服务器处理请求
* 返回响应结果
* 关闭TCP连接
* 浏览器解析HTML
* 浏览器布局渲染
* 补充资源(如果需要)

‍

‍

#### **用到的协议**

* TCP:与服务器建立TCP连接
* IP: 建立TCP协议时，需要发送数据，发送数据在网络层使用IP协议
* OPSF: IP数据包在路由器之间，路由选择使用OPSF协议
* ARP: 路由器在与服务器通信时，需要将ip地址转换为MAC地址，需要使用ARP协议
* HTTP: 在TCP建立完成后，使用HTTP协议访问网页

‍

‍

### 网络延迟包含的部分

‍

* **传播延迟（Propagation Delay）** ：信号在介质上传播所需的时间，与物理距离和介质传播速度有关。
* **传输延迟（Transmission Delay）** ：数据包在发送端完全发送到链路上的所需时间，取决于数据包大小和链路带宽。
* **处理延迟（Processing Delay）** ：路由器、交换机等网络设备处理数据包（如查找路由、校验等）所花费的时间。
* **排队延迟（Queuing Delay）** ：数据包在网络设备等待处理或排队发送时产生的延迟。
* **应用层延迟**：包括DNS解析时间、TCP握手时间、SSL/TLS建立连接的时间以及服务器端的业务处理时间。

‍

‍

### 客户端和服务端距离很远，建立连接后，客户端长时间不发消息，连接会怎么样

‍

* 会发送数据包保活，超时会断开连接
* tcp 的keep-alive机制
* 应用层心跳机制
* 中间设备（如防火墙，NAT）的超时机制
* 配置连接超时
* 使用长连接协议

‍

‍

### 海外业务数据传输延迟较大如何改善

* **网络链路优化**

  * 与运营商协作优化跨境专线，采用更优的路由；
  * 部署海外中转节点或使用边缘服务器，缩短数据传输路径。
* **协议与传输层优化**

  * 考虑采用基于 UDP 的 QUIC 协议（HTTP/3 基于 QUIC），降低 TCP 握手和队头阻塞带来的延迟；
  * 数据压缩、分片传输等手段也有助于降低延迟。
* **异步传输与缓存**

  * 采用消息队列或异步传输，避免同步等待；
  * 在海外节点部署缓存（如 Redis、CDN）降低跨境实时请求的影响。
* **综合方案**

  * 结合网络链路、协议优化、边缘部署及异步处理，多方面降低整体延迟。

‍

‍

‍

## HTTP

‍

### 完整的HTTP请求过程

‍

* 建立起客户机和服务器连接
* 建立连接后，客户机发送一个请求给服务器
* 服务器收到请求给予响应信息
* 客户端浏览器将返回的内容解析并呈现，断开连接

‍

‍

‍

### 如何禁用缓存？如何确认缓存？

HTTP/1.1 通过 Cache-Control 首部字段来控制缓存。

‍

**禁止进行缓存 no-store**

no-store 指令规定不能对请求或响应的任何一部分进行缓存

```html
Cache-Control: no-store
```

‍

**强制确认缓存 no-cache**

no-cache 指令规定 缓存服务器需要先向源服务器验证缓存资源的有效性，**只有当缓存资源有效时** 才能使用该缓存对客户端的请求进行响应。

```html
Cache-Control: no-cache
```

‍

‍

### HTTP Header 中常见的字段有哪些

‍

|请求头字段名|说明|示例|
| ---------------------| ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------| ----------------------------------------------------------------------------------|
|Accept|能够接受的回应内容类型（Content-Types）|Accept: text/plain|
|Accept-Charset|能够接受的字符集|Accept-Charset: utf-8|
|Accept-Datetime|能够接受的按照时间来表示的版本|Accept-Datetime: Thu, 31 May 2007 20:35:00 GMT|
|Accept-Encoding|能够接受的编码方式列表。参考 HTTP 压缩|Accept-Encoding: gzip, deflate|
|Accept-Language|能够接受的回应内容的自然语言列表|Accept-Language: en-US|
|**Authorization**|用于超文本传输协议的认证的认证信息|Authorization: Basic QWxhZGRpbjpvcGVuIHNlc2FtZQ==|
|Cache-Control|用来指定在这次的请求/响应链中的所有缓存机制 都必须 遵守的指令|Cache-Control: no-cache|
|Connection|该浏览器想要优先使用的连接类型|Connection: keep-alive Connection: Upgrade|
|Content-Length|以 八位字节数组 （8 位的字节）表示的请求体的长度|Content-Length: 348|
|Content-MD5|请求体的内容的二进制 MD5 散列值，以 Base64 编码的结果|Content-MD5: Q2hlY2sgSW50ZWdyaXR5IQ==|
|Content-Type|请求体的 多媒体类型 （用于 POST 和 PUT 请求中）|Content-Type: application/x-www-form-urlencoded|
|**Cookie**|之前由服务器通过 Set- Cookie （下文详述）发送的一个 超文本传输协议 Cookie|Cookie: $Version=1; Skin=new;|
|Date|发送该消息的日期和时间(按照 RFC 7231 中定义的"超文本传输协议日期"格式来发送)|Date: Tue, 15 Nov 1994 08:12:31 GMT|
|Expect|表明客户端要求服务器做出特定的行为|Expect: 100-continue|
|From|发起此请求的用户的邮件地址|From: [user@example.com](mailto:user@example.com)|
|Host|服务器的域名(用于虚拟主机 )，以及服务器所监听的传输控制协议端口号。如果所请求的端口是对应的服务的标准端口，则端口号可被省略。|Host: [en.wikipedia.org:80open in new window](http://en.wikipedia.org/)|
|If-Match|仅当客户端提供的实体与服务器上对应的实体相匹配时，才进行对应的操作。主要作用时，用作像 PUT 这样的方法中，仅当从用户上次更新某个资源以来，该资源未被修改的情况下，才更新该资源。|If-Match: “737060cd8c284d8af7ad3082f209582d”|
|If-Modified-Since|允许在对应的内容未被修改的情况下返回 304 未修改（ 304 Not Modified ）|If-Modified-Since: Sat, 29 Oct 1994 19:43:31 GMT|
|If-None-Match|允许在对应的内容未被修改的情况下返回 304 未修改（ 304 Not Modified ）|If-None-Match: “737060cd8c284d8af7ad3082f209582d”|
|If-Range|如果该实体未被修改过，则向我发送我所缺少的那一个或多个部分；否则，发送整个新的实体|If-Range: “737060cd8c284d8af7ad3082f209582d”|
|If-Unmodified-Since|仅当该实体自某个特定时间已来未被修改的情况下，才发送回应。|If-Unmodified-Since: Sat, 29 Oct 1994 19:43:31 GMT|
|Max-Forwards|限制该消息可被代理及网关转发的次数。|Max-Forwards: 10|
|Origin|发起一个针对 跨来源资源共享 的请求。|Origin: [http://www.example-social-network.comopen in new window](http://www.example-social-network.com/)|
|Pragma|与具体的实现相关，这些字段可能在请求/回应链中的任何时候产生多种效果。|Pragma: no-cache|
|Proxy-Authorization|用来向代理进行认证的认证信息。|Proxy-Authorization: Basic QWxhZGRpbjpvcGVuIHNlc2FtZQ==|
|Range|仅请求某个实体的一部分。字节偏移以 0 开始。参见字节服务。|Range: bytes=500-999|
|Referer|表示浏览器所访问的前一个页面，正是那个页面上的某个链接将浏览器带到了当前所请求的这个页面。|Referer: [http://en.wikipedia.org/wiki/Main_Pageopen in new window](https://en.wikipedia.org/wiki/Main_Page)|
|TE|浏览器预期接受的传输编码方式：可使用回应协议头 Transfer-Encoding 字段中的值；|TE: trailers, deflate|
|Upgrade|要求服务器升级到另一个协议。|Upgrade: HTTP/2.0, SHTTP/1.3, IRC/6.9, RTA/x11|
|**User-Agent**|浏览器的浏览器身份标识字符串|User-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:12.0) Gecko/20100101 Firefox/21.0|
|Via|向服务器告知，这个请求是由哪些代理发出的。|Via: 1.0 fred, 1.1 [example.comopen in new window](http://example.com/) (Apache/1.1)|
|Warning|一个一般性的警告，告知，在实体内容体中可能存在错误。|Warning: 199 Miscellaneous warning|

‍

‍

#### 发送post请求时，可以发送二进制数据吗

请求头里面 Content-Type 设置为 application/octet-stream 就可以

‍

‍

### 一个TCP连接可以对应几个HTTP请求？

如果维持连接，一个 TCP 连接是可以发送多个 HTTP 请求的。

（比如一起发三个请求，再三个响应一起接收）

‍

HTTP/1.1 存在一个问题，单个 TCP **连接在同一时刻只能处理一个请求**，意思是说：两个请求的生命周期不能重叠，任意两个 HTTP 请求从开始到结束的时间在同一个 TCP 连接里不能重叠

在 HTTP/1.1 存在 Pipelining 技术可以完成这个多个请求同时发送，但是由于浏览器默认关闭，所以可以认为这是不可行的

‍

那么在 HTTP/1.1 时代，浏览器是如何提高页面加载效率的呢？主要有下面两点：

* 维持和服务器已经建立的 TCP 连接，在同一连接上顺序处理多个请求。
* 和服务器建立多个 TCP 连接。

‍

而在 HTTP2 中由于 Multiplexing 特点的存在，多个 HTTP 请求可以在同一个 TCP 连接中并行进行

‍

‍

### 浏览器对同一 Host 建立 TCP 连接到的数量有没有限制？

HTTP/1.1 时代，那个时候没有多路传输，**有**

**Chrome 最多允许对同一个 Host 建立六个 TCP 连接。不同的浏览器有一些区别**

‍

如果图片都是 HTTPS 连接并且在同一个域名下，那么浏览器在 SSL 握手之后会和服务器商量==能不能用 HTTP2==，如果能的话就使用 **Multiplexing** 功能在这个连接上进行**多路传输**。不过也未必会所有挂在这个域名的资源都会使用一个 TCP 连接去获取，但是可以确定的是 Multiplexing 很可能会被用到。

‍

如果发现用不了 HTTP2 呢？或者用不了 HTTPS（现实中的 HTTP2 都是在 HTTPS 上实现的，所以也就是只能使用 HTTP/1.1）。那浏览器就会在一个 HOST 上建立多个 TCP 连接，连接数量的最大限制取决于**浏览器设置**，这些连接会在空闲的时候被浏览器用来发送新的请求，如果所有的连接都正在发送请求呢？那其他的请求就只能等等了。

‍

‍

### HTTP 2.0

HTTP 2.0 的三大特性是**头部压缩、服务端推送、多路复用**。

‍

HTTP 1.1 允许通过同一个连接发起多个请求。但是由于 HTTP 1.X 使用**文本格式**传输数据，服务端必须按照客户端请求到来的顺序，**串行返回**数据。此外，HTTP 1.1 中浏览器会限制同时发起的最大连接数，超过该数量的连接会被**阻塞**

‍

HTTP 2 引入了**多路复用 MP**，允许通过同一个连接发起多个请求，并且可以**并行传输**数据。HTTP 2 使用**二进制数据帧**作为传输的最小单位，每个**帧**标识了自己属于哪个流，每个流对应一个请求。服务端可以并行地传输数据，而接收端可以根据帧中的顺序标识，自行合并数据。

‍

在 HTTP 1.1 中，由于浏览器限制每个域名下最多同时有 6 个请求，其余请求会被阻塞，因此我们通常使用**多个域名**（比如 CDN）来提高浏览器的下载速度。HTTP 2 就不再需要这样优化了。

同理，在 HTTP 1.1 中，我们会将多个 JS 文件、CSS 文件等打包成一个文件，将多个小图片合并为雪碧图，目的是减少 HTTP 请求数。HTTP 2 也不需要这样优化了。

‍

‍

### HTTP 的长连接与短连接

**HTTP/1.0 默认使用的是短连接**。也就是说，浏览器每请求一个静态资源，就建立一次连接，任务结束就中断连接

**HTTP/1.1 默认使用的是长连接**。长连接是指在一个网页打开期间，所有网络请求都使用同一条已经建立的连接。当没有数据发送时，双方需要发检测包以维持此连接。长连接不会永久保持连接，而是有一个保持时间。实现长连接要客户端和服务端都支持长连接。

‍

长连接的优点：TCP 三次握手时会有 1.5 RTT 的延迟，以及建立连接后慢启动（slow-start）特性，当请求频繁时，建立和关闭 TCP 连接会浪费时间和带宽，而重用一条已有的连接性能更好。

长连接的缺点：长连接会占用服务器的资源。

‍

长连接适用于操作频繁、点对点通讯，如数据库的连接

短连接常用于一点对多点通讯，如web网站的http服务. 长连接对于服务端来说会耗费一定的资源，而像 WEB 网站这么频繁的成千上万甚至上亿客户端的连接用短连接会更省一些资源

‍

‍

### HTTP缓存是怎么实现去获取资源？以及他有一个字段是缓存资源的过期时间的？

HTTP缓存都是从第二次请求开始的

Expires：指定资源的过期时间

‍

第一次请求资源时，服务器返回资源，并在response header中回传资源的缓存策略；`Cache-Control`​

第二次请求时，浏览器判断这些请求参数，击中强缓存就直接200，否则就把请求参数加到request header头中传给服务器，看是否击中协商缓存，击中则返回304，否则服务器会返回新的资源

‍

‍

### HTTP的缺点有哪些？

* 使用明文进行通信，内容可能会被窃听；
* 不验证通信方的身份，通信方的身份有可能遭遇伪装；
* 无法证明报文的完整性，报文有可能遭篡改。

‍

‍

### HTTP几大版本的优化总结

‍

简要

* **HTTP/0.9**

  * **特性**：最初级的版本，仅支持简单的 GET 请求，不支持请求头、状态码和多媒体传输。
  * **优化**：设计简单，但功能极其有限。
* **HTTP/1.0**

  * **特性**：引入了请求头和状态码，实现了静态资源传输。
  * **缺陷与优化点**：

    * 每个请求建立一个TCP连接，连接建立和关闭的开销较大；
    * 无持久连接，导致性能低下。
* **HTTP/1.1**

  * **特性**：

    * **持久连接**：引入了 Keep-Alive 机制，可以在一个 TCP 连接上发送多个请求；
    * **管道化**：允许客户端同时发送多个请求（虽然存在队头阻塞问题）；
    * **缓存控制**：增加了更多缓存相关头部；
    * **Host头**：支持在同一 IP 上托管多个域名。
  * **优化点**：大幅减少连接建立/关闭开销，但依然存在队头阻塞等问题
* **HTTP/2**

  * **特性**：

    * **二进制分帧**：将数据转换成二进制帧传输，降低解析成本；
    * **多路复用**：在单一连接上同时传输多个请求和响应，消除了HTTP/1.1队头阻塞问题；
    * **头部压缩**：使用 HPACK 算法压缩头部信息，降低带宽开销；
    * **服务器推送**：服务器可以主动推送资源给客户端。
  * **优化点**：提高了并发传输效率和带宽利用率，改善了延迟表现。
* **HTTP/3**

  * **特性**：

    * 基于 UDP 和 QUIC 协议，摆脱了 TCP 的队头阻塞；
    * 提供 0-RTT 连接建立、内建加密和更好的拥塞控制。
  * **优化点**：在高延迟或丢包环境下更稳定，连接建立更快，进一步提高了传输效率。

‍

详细

* HTTP 0.9版本

  * HTTP协议的第一个版本，功能简单，**已弃用**
  * 仅支持纯文本数据的传输，虽然支持HTML，但是不支持图片插入
  * 仅支持GET请求方式，且不支持请求头
  * 无状态，短连接。没有对用户状态的管理；每次请求建立一个TCP连接，响应之后关闭TCP连接。
* HTTP 1.0版本

  * 支持POST、GET、HEAD三种方法
  * **支持长连接keep-alive**（但**默认还是使用短连接**：浏览器每一次请求建立一次TCP连接，请求处理完毕之后断开）。
  * 服务器不跟踪用户的行为也不记录用户过往请求。
* HTTP 1.1版本

  * 新增PUT、DELETE、CONNECT、TRACE、OPTIONS方法，是现今**使用最多**的版本。
  * 支持长连接，在一次TCP连接中可以发送多个请求或响应，且默认使用长连接。
  * 支持宽带优化、断点续传。请求的对象部分数据，可以不必发送整个对象；文件上传下载支持续传。
  * 因为长连接产生的问题：队头阻塞。长连接中，发送请求和响应都是串行化的，前面的消息会造成后面的消息也阻塞。解决方法是创建多个TCP连接，这样就可以基本保证了可用性，浏览器**默认的最大TCP连接数是6个**。
* HTTP 2.0版本

  * 二进制分帧，所有帧都是用二进制编码，节省了空间
  * 多路复用：HTTP 2.0中所有的连接都是持久化的。相比1.1版本可以不用维护更多的TCP连接，在处理并发请求的时候，可以将多个数据流中**互不依赖的帧**可以**乱序发送**，同时还支持**优先级**。接收方接收之后可以根据帧头部信息将帧组合起来。（解决了1.1版本中的队头阻塞问题）
  * 头部压缩：1.1版本每次传输都需要传输一份首部，2.0让双方各自缓存一份首部字段表，达到更快传输的目标。
* HTTP 3.0版本

  * 基于UDP的**QUIC多路复用**：在一个QUIC中可以并发发送多个HTTP请求Stream，且如果各个Stream互不依赖，那么就不会造成**使用TCP带来的队头阻塞问题**。这个问题源头上是因为TCP连接，TCP连接的性质决定了重传会影响队后的数据发送，所以干脆选用UDP来解决这个方案。
  * 0RTT建链：RTT表示Round-Trip Time，3.0可以实现0RTT建链。一般来说HTTPS协议要建立完整链接包括**TCP握手**和**TLS握手**，总计需要至少2-3个RTT，普通的HTTP协议也需要至少1个RTT才可以完成握手。基于UDP的QUIC协议可以在第一次发送包的时候直接发送业务数据。但是由于首次连接需要发送公钥数据，所以首次连接并不使用这一方法。

‍

## HTTPS

HTTPS（HyperText Transfer Protocol Secure）协议是基于HTTP协议和SSL/TLS协议实现的。以下是它的主要组成部分：

1. **HTTP**：==超文本传输协议==，用于在客户端和服务器之间传输超文本数据（如HTML、CSS、JavaScript等）
2. **SSL/TLS**：==安全套接字层==（SSL）和==传输层安全==（TLS）协议，用于在传输层提供加密和数据完整性保护

‍

HTTPS 并不是新协议，而是让 **HTTP 先和 SSL（Secure Sockets Layer）通信，再由 SSL 和 TCP 通信，也就是说 HTTPS 使用了隧道进行通信**。通过使用 SSL，HTTPS 具有了加密（防窃听）、认证（防伪装）和完整性保护（防篡改）

‍

### HTTPS精简版

HTTPS 并不是新协议，而是让 HTTP 先和 SSL（Secure Sockets Layer）通信，再由 SSL 和 TCP 通信，也就是说 HTTPS 使用了隧道进行通信。

通过使用 SSL，HTTPS 具有了加密（防窃听）、认证（防伪装）和完整性保护（防篡改）。

‍

‍

#### 1.加密(对称加密和非对称加密)

加密分为对称密匙加密和非对称密匙加密2种

‍

**对称密钥加密**

对称密钥加密是指加密和解密使用同一个密钥的方式，这种方式存在的最大问题就是密钥发送问题，即如何安全地将密钥发给对方。

‍

优点: 对称加密算法的优点是算法公开、计算量小、加密速度快、加密效率高。

缺点: 没有非对称加密安全.

‍

用途： 一般用于保存用户手机号、身份证等敏感但能解密的信息。

常见的对称加密算法有: AES、HS256

‍

‍

**非对称加密**

非对称加密是指使用一对非对称密钥，即公钥和私钥，公钥可以随意发布，但私钥只有自己知道。发送密文的一方使用对方的公钥进行加密处理，对方接收到加密信息后，使用自己的私钥进行解密。

‍

优点: 非对称加密与对称加密相比，其安全性更好；

缺点: 非对称加密的缺点是加密和解密花费时间长、速度慢，只适合对少量数据进行加密。 用途： 一般用于签名和认证。私钥服务器保存, 用来加密, 公钥客户拿着用于对于令牌或者签名的解密或者校验使用.

‍

‍

常见的非对称加密算法有： RSA、DSA（数字签名用）、ECC（移动设备用）、RS256 (采用SHA-256 的 RSA 签名)

‍

HTTPS 采用混合的加密机制：

* 使用非对称密钥加密方式，传输对称密钥加密方式所需要的 Secret Key，从而保证安全性
* 获取到 Secret Key 后，再使用对称密钥加密方式进行通信，从而保证效率

‍

‍

#### 2.认证

通过使用 证书 来对通信方进行认证

‍

数字证书认证机构（CA，Certificate Authority）是客户端与服务器双方都可信赖的第三方机构

服务器的运营人员向 CA 提出公开密钥的申请，CA 在判明提出申请者的身份之后，会对已申请的公开密钥做数字签名，然后分配这个已签名的公开密钥，并将该公开密钥放入公开密钥证书后绑定在一起。

进行 HTTPS 通信时，服务器会把证书发送给客户端。客户端取得其中的公开密钥之后，先使用数字签名进行验证，如果验证通过，就可以开始通信了。

‍

‍

#### 3.完整性保护SSL

SSL 提供报文摘要功能来进行完整性保护。

HTTP 也提供了 MD5 报文摘要功能，但不是安全的。例如报文内容被篡改之后，同时重新计算 MD5 的值，通信接收方是无法意识到发生了篡改。

HTTPS 的报文摘要功能之所以安全，是因为它结合了加密和认证这两个操作。试想一下，加密之后的报文，遭到篡改之后，也很难重新计算报文摘要，因为无法轻易获取明文。

‍

‍

### 和 HTTP 的区别

Http协议运行在TCP之上，明文传输(不安全)，客户端与服务器端都无法验证对方的身份；Https是身披SSL(Secure Socket Layer)外壳的Http，运行于SSL上，SSL运行于TCP之上，是添加了加密和认证机制的HTTP

‍

二者之间存在如下不同

Https通信需要证书，而证书一般需要向认证机构购买

‍

‍

#### **端口**

连接方式, 端口

HTTP的URL由“http://”起始且默认使用端口80，而HTTPS的URL由“https://”起始且默认使用端口443

‍

#### **安全性**

HTTP协议运行在TCP之上，所有传输的内容都是明文，客户端和服务器端都无法验证对方的身份。

HTTPS是运行在SSL/TLS之上的HTTP协议，SSL/TLS 运行在TCP之上。所有传输的内容都经过加密，**加密采用对称加密，但对称加密的密钥**用服务器方的证书进行了非对称加密。

‍

#### 性能

HTTPS 比HTTP耗费更多服务器资源 (加密处理)

‍

‍

### HTTPS连接建立过程

[说一下HTTPS执行流程？ | Javaᶜⁿ 面试突击 (javacn.site)](https://javacn.site/interview/net/httpsexecute.html)

‍

HTTPS在传输的过程中会涉及到三个密钥

* 服务器端的公钥和私钥，用来进行**非对称加密**
* 客户端生成的随机密钥，用来进行**对称加密**

‍

一个HTTPS请求实际上包含了**两次HTTP传输**，可以细分为8步

1. 客户端向服务器发起HTTPS请求，连接到服务器的443端口
2. 服务器端有一个密钥对，即公钥和私钥，是用来进行非对称加密使用的，服务器端保存着私钥，不能将其泄露，公钥可以发送给任何人
3. 服务器将自己的公钥发送给客户端
4. 客户端收到服务器端的公钥之后，会对公钥进行检查，验证其合法性，如果发现发现公钥有问题，那么HTTPS传输就无法继续

    > 【严格的说，这里应该是验证服务器发送的**数字证书的合法性**，关于客户端如何验证数字证书的合法性，下文会进行说明】如果公钥合格，那么客户端会生成一个随机值，这个随机值就是用于进行对称加密的密钥，我们将该密钥称之为client key，即客户端密钥，这样在概念上和服务器端的密钥容易进行区分。然后用服务器的公钥对客户端密钥进行非对称加密，这样客户端密钥就变成密文了，至此，HTTPS中的第一次HTTP请求结束。
    >
5. 客户端会发起HTTPS中的第二个HTTP请求，将加密之后的客户端密钥发送给服务器
6. 服务器接收到客户端发来的密文之后，会用自己的私钥对其进行非对称解密，解密之后的明文就是客户端密钥，然后用客户端密钥对数据进行对称加密，这样数据就变成了密文
7. 然后服务器将加密后的密文发送给客户端。
8. 客户端收到服务器发送来的密文，用客户端密钥对其进行对称解密，得到服务器发送的数据。这样HTTPS中的第二个HTTP请求结束，整个HTTPS传输完成。

‍

‍

简单版

1. **客户端发起请求**：客户端向服务器发起HTTPS请求
2. **服务器响应并发送证书**：服务器响应请求并发送SSL/TLS证书给客户端
3. **客户端验证证书**：客户端验证服务器的证书是否合法
4. **协商加密算法**：客户端和服务器协商使用的加密算法和会话密钥
5. **建立加密通道**：使用协商好的会话密钥建立加密通道
6. **传输数据**：在加密通道中传输HTTP数据

‍

HTTPS 请求示意图

```plaintext
客户端  -->  服务器
  |            |
  |  发起请求   |
  |----------->|
  |            |
  |  发送证书   |
  |<-----------|
  |            |
  |  验证证书   |
  |            |
  |  协商加密   |
  |<---------->|
  |            |
  |  建立加密   |
  |            |
  |  传输数据   |
  |<---------->|
```

‍

‍

‍

### HTTPS是如何保证数据传输的安全，整体的流程是什么？（SSL是怎么工作保证安全的）

HTTPS是基于HTTP的上层添加了一个叫做TLS的安全层，对数据的加密等操作都是在这个安全层中进行处理的，其底层还是应用的HTTP。用对称加密来传送消息，但对称加密所使用的密钥是通过非对称加密的方式发送出去。

‍

（1）客户端向服务器端发起SSL连接请求

（2）服务器把公钥发送给客户端，并且服务器端保存着唯一的私钥

（3）客户端用公钥对双方通信的对称秘钥进行加密，并发送给服务器端

（4）服务器利用自己唯一的私钥对客户端发来的对称秘钥进行解密

（5）进行数据传输，服务器和客户端双方用公有的相同的对称秘钥对数据进行加密解密，可以保证在数据收发过程中的安全，即是第三方获得数据包，也无法对其进行加密，解密和篡改

‍

因为数字签名、摘要是证书防伪非常关键的武器。 “摘要”就是对传输的内容，通过hash算法计算出一段固定长度的串。然后，通过发送方的私钥对这段摘要进行加密，加密后得到的结果就是“数字签名”

SSL/TLS协议的基本思路是采用公钥加密法，也就是说，客户端先向服务器端索要公钥，然后用公钥加密信息，服务器收到密文后，用自己的私钥解密。

‍

‍

#### 什么是SSL/TLS ？

SSL 代表安全套接字层。它是一种用于加密和验证应用程序（如浏览器）和Web服务器之间发送的数据的协议。 身份验证，加密Https的加密机制是一种共享密钥加密和公开密钥加密并用的混合加密机制。

SSL/TLS协议作用：认证用户和服务，加密数据，维护数据的完整性的应用层协议加密和解密需要两个不同的密钥，故被称为非对称加密；加密和解密都使用同一个密钥的

‍

‍

‍

#### HTTPS采用的加密方式有哪些？是对称还是非对称？

HTTPS 采用混合的加密机制，使用**非对称密钥加密用于传输对称密钥来保证传输过程的安全性**，之后使用**对称密钥加密进行通信来保证通信过程的效率**。

‍

确保传输安全过程（其实就是rsa原理）

1. Client给出协议版本号、一个客户端生成的随机数（Client random），以及客户端支持的加密方法。
2. Server确认双方使用的加密方法，并给出数字证书、以及一个服务器生成的随机数（Server random）。
3. Client确认数字证书有效，然后生成一个新的随机数（Premaster secret），并使用数字证书中的公钥，加密这个随机数，发给Server。
4. Server使用自己的私钥，获取Client发来的随机数（Premaster secret）
5. Client和Server根据约定的加密方法，使用前面的三个随机数，生成”对话密钥”（session key），用来加密接下来的整个对话过程。

‍

‍

‍

‍

‍

## 网安

‍

### 对称加密算法和非对称加密算法

**对称加密**：密钥只有一个，加密解密为同一个密码，且加解密速度快，典型的对称加密算法有DES、AES等； 速度快

> 这种方式存在的最大问题就是密钥发送问题，即如何安全地将密钥发给对方,对称加密所使用的密钥我们可以通过非对称加密的方式发送出去。

‍

**非对称加密**：密钥成对出现（且根据公钥无法推知私钥，根据私钥也无法推知公钥），加密解密使用不同密钥（公钥加密需要私钥解密，私钥加密需要公钥解密），相对对称加密速度较慢，典型的非对称加密算法有 ==RSA、DSA==等。

> 非对称加密算法实现机密信息交换的基本过程是:
>
> 甲方生成一对密钥并将其中的一把作为公用密钥向其它方公开;得到该公用密钥的乙方使用该密钥对机密信息进行加密后再发送给甲方;甲方再用自己保存的另一把专用密钥对加密后的信息进行解密

‍

> 例如与熊对话这种思路, 双方都是讲英文, 但是套在秘密私聊软件里面进行公开加密, 我们自己对自己的对话进行英译中

公开密钥所有人都可以获得，**通信发送方获得接收方的公开密钥之后，就可以使用公开密钥进行加密**，**接收方收到通信内容后使用私有密钥解密**。

非对称密钥除了用来加密，还可以用来进行签名。因为私有密钥无法被其他人获取，因此通信发送方使用其私有密钥进行签名，通信接收方使用发送方的公开密钥对签名进行解密，就能判断这个签名是否正确。

* 优点：可以更安全地将公开密钥传输给通信发送方；
* 缺点：运算速度慢。

‍

‍

#### SSL中的认证中的证书是什么？了解过吗？

通过使用 **证书** 来对通信方进行认证。

数字证书认证机构（CA，Certificate Authority）是客户端与服务器双方都可信赖的第三方机构。

服务器的运营人员向 CA 提出公开密钥的申请，CA 在判明提出申请者的身份之后，会对已申请的公开密钥做数字签名，然后分配这个已签名的公开密钥，并将该公开密钥放入公开密钥证书后绑定在一起。

进行 HTTPS 通信时，服务器会把证书发送给客户端。客户端取得其中的公开密钥之后，先使用数字签名进行验证，如果验证通过，就可以开始通信了。

‍

‍

#### 如何保证公钥不被篡改？

将公钥放在数字证书中。只要证书是可信的，公钥就是可信的

‍

‍

#### 公钥加密计算量太大，如何减少耗用的时间？

每一次对话（session），客户端和服务器端都生成一个"**对话密钥**"（session key），用它来加密信息。由于"对话密钥"是对称加密，所以运算速度非常快，而服务器公钥只用于加密"对话密钥"本身，这样就减少了加密运算的消耗时间。

‍

（1） 客户端向服务器端索要并验证公钥

（2） 双方协商生成"对话密钥"

（3） 双方采用"对话密钥"进行加密通信。上面过程的前两步，又称为"握手阶段"（handshake）

‍

‍

‍

### DDos分布式拒绝服务攻击

‍

TCP的内容

客户端向服务端发送请求链接数据包，服务端向客户端发送确认数据包，客户端不向服务端发送确认数据包，服务器一直等待来自客户端的确认  
没有彻底根治的办法，除非不使用TCP

‍

治理

* 限制同时打开SYN半链接的数目
* 缩短SYN半链接的 Time out 时间
* 关闭不必要的服务

‍

‍

#### DDOS - SYN攻击

TCP内容

‍

**服务器端的资源分配是在二次握手时分配的，而客户端的资源是在完成三次握手时分配的**，所以服务器容易受到SYN洪泛攻击。

SYN攻击就是Client在短时间内伪造大量不存在的IP地址，并向Server不断地发送SYN包，**Server则回复确认包**，并等待Client确认，由于源地址不存在，因此Server需要不断重发直至超时，这些伪造的SYN包将长时间占用未连接队列，导致正常的SYN请求因为队列满而被丢弃，从而引起网络**拥塞**甚至系统瘫痪。SYN 攻击是一种典型的 DoS/DDoS 攻击。

检测 SYN 攻击非常的方便，当你在服务器上看到大量的半连接状态时，特别是源IP地址是随机的，基本上可以断定这是一次SYN攻击

‍

常见的防御 SYN 攻击的方法同上

* 缩短超时（SYN Timeout）时间
* 增加最大半连接数
* 过滤网关防护
* SYN cookies技术

‍

‍

‍

### 防止CSRF跨站点请求伪造攻击

‍

跨站点请求伪造，指攻击者通过跨站请求，以**合法的用户**的身份进行非法操作。

可以这么理解CSRF攻击：攻击者**盗用你的身份**，以你的名义向第三方网站发送恶意请求。

‍

‍

治理

* **安全框架**，例如Spring Security
* **token机制**。在HTTP请求中进行token验证，如果请求中没有token或者token内容不正确，则认为CSRF攻击而拒绝该请求。
* **验证码**。通常情况下，验证码能够很好的遏制CSRF攻击，(验明正身) 但是很多情况下，出于用户体验考虑，验证码只能作为一种辅助手段，而不是最主要的解决方案。
* **来源地址 referer识别**。在HTTP Header中有一个字段Referer，它记录了HTTP请求的**来源地址(开盒)** 。如果Referer是其他网站，就有可能是CSRF攻击，则拒绝该请求。但是，服务器并非都能取到Referer。很多用户出于隐私保护的考虑，限制了Referer的发送。在某些情况下，浏览器也不会发送Referer，例如HTTPS跳转到HTTP。

‍

展开的

1. **基于Token的防护**

    * **表单Token**：在页面生成时为每个用户生成一个唯一随机的CSRF token，并在所有需要写操作的表单中以隐藏字段形式提交。在服务器端验证这个Token是否与用户会话中的Token匹配，从而防止伪造请求。
    * **双提交Cookie**：将Token同时存放在Cookie和请求参数中，服务器对比两者是否一致，避免跨站请求伪造。
2. **HTTP Header验证**

    * **Referer或Origin验证**：检查请求头中的Referer或Origin是否来自可信域。虽然这种方法依赖于浏览器行为，但在一定程度上可以阻止来自第三方站点的伪造请求。
    * **自定义Header**：对于AJAX请求，可以要求客户端在请求中添加自定义Header（如X-Requested-With），并在服务器端验证该Header，从而区分同域合法请求与跨域伪造请求。
3. **利用安全框架内置功能**

    * **Spring Security**：Spring Security内置了CSRF防护机制，可以自动生成并验证CSRF token，同时提供配置选项对CSRF攻击进行拦截。使用框架可以减少手工实现的出错概率和维护成本。
4. **SameSite Cookie属性**

    * 配置Cookie的SameSite属性（例如SameSite\=Strict或Lax），可以限制浏览器在跨站请求时发送Cookie，这样即使用户点击恶意链接，Cookie也不会随请求一起发送，从而降低CSRF风险。
5. **额外的验证措施**

    * **验证码（CAPTCHA）** ：在关键操作或敏感操作前加入验证码，可以防止自动化攻击。
    * **限流与日志监控**：对同一用户或IP的频繁请求进行限流，同时记录异常请求行为，及时发现和拦截恶意请求。
    * 对表单提交执行CSRF安全验证, 看具体的业务, 看看能不能短路判断这样, 过期时间啥的

‍

综合来看，多层防护（Token、HTTP头、SameSite、框架内置机制以及限流监控）能够大幅降低CSRF攻击的风险，即使单一机制存在绕过的可能，多重验证能形成有效的防护屏障。

‍

‍

### SQL注入攻击

‍

攻击者在HTTP请求中注入恶意的SQL代码，服务器使用参数构建数据库SQL命令时，恶意SQL被一起构造，并在数据库中执行。  
用户登录，输入用户名 lianggzone，密码 ‘ or ‘1’=’1 ，如果此时使用参数构造的方式，就会出现  
select * from user where name = ‘lianggzone’ and password = ‘’ or ‘1’=‘1’  
不管用户名和密码是什么内容，使查询出来的用户列表不为空。如何防范SQL注入攻击使用预编译的PrepareStatement是必须的，但是一般我们会从两个方面同时入手。

‍

Web端(前端)

1）有效性检验。

2）限制字符串输入的长度。

‍

‍

服务端

1）不用拼接SQL字符串。

2）**使用预编译的PrepareStatement**

3）有效性检验。(为什么服务端还要做有效性检验？**第一准则，外部都是不可信的，防止攻击者绕过Web端请求直接掏后台**)

4）过滤SQL需要的参数中的特殊字符。比如单引号、双引号。

‍

‍

### 网络连接中出现了很多 time-wait, 为什么

‍

在网络连接中出现很多 `TIME-WAIT`​ 状态的原因主要是由于 TCP 协议的设计。`TIME-WAIT`​ 状态是 TCP 连接关闭过程中的一个阶段，确保所有数据包都被正确接收并防止旧的重复数据包干扰新的连接。

‍

原因：

1. **确保数据包的可靠传输**：`TIME-WAIT`​ 状态允许 TCP 协议确保所有数据包都被正确接收。如果有任何丢失的数据包，发送方可以在此期间重传。
2. **防止旧数据包干扰新连接**：`TIME-WAIT`​ 状态的持续时间通常是 2 倍的最大报文段寿命（MSL），以确保旧的重复数据包在新连接建立之前被丢弃。
3. **连接资源的释放**：在 `TIME-WAIT`​ 状态结束后，系统会释放与该连接相关的所有资源。

‍

> 考虑高并发短连接的业务场景，在高并发短连接的 TCP 服务器上，当服务器处理完请求后主动请求关闭连接，这样服务器上会有大量的连接处于 TIME_WAIT 状态，服务器维护每一个连接需要一个 socket，也就是每个连接会占用一个文件描述符，而文件描述符的使用是有上限的，如果持续高并发，会导致一些正常的 连接失败。

‍

‍

#### 解决方法

如果在服务器上看到大量的 `TIME-WAIT`​ 状态，可以考虑以下几种方法来优化：

1. **调整操作系统参数**：可以通过调整操作系统的 **TCP 参数**来减少 `TIME-WAIT`​ 状态的数量。例如，在 Linux 系统中，可以调整 `tcp_tw_reuse`​ 和 `tcp_tw_recycle`​ 参数。
2. **使用长连接**：对于频繁的短连接请求，可以考虑使用长连接（Keep-Alive）来减少连接的建立和关闭次数。
3. **负载均衡**：通过负载均衡将流量分散到多个服务器上，减少单个服务器的连接压力
4. 服务器可以设置 SO\_REUSEADDR 套接字选项来通知内核，如果端口被占用，但 TCP 连接位于 TIME\_WAIT 状态时可以重用端口

‍

‍

### 服务端建立tcp连接的上限是多少, 从计算机原理的角度来分析问题

‍

#### 1. TCP 连接的理论模型

* **四元组唯一性**  
  TCP 连接由四个部分唯一确定：

  * 本地 IP
  * 本地端口
  * 远程 IP
  * 远程端口

    对于服务端来说，通常只需要绑定一个固定端口（例如 80、443 或其它自定义端口），而连接的区分主要依靠客户端的 IP 和客户端的随机端口。所以理论上，服务端能够区分的连接数量取决于“客户端 IP 数量 × 客户端端口数”。在 IPv4 下，客户端 IP 理论上有 2³² 个，而端口号范围是 1～65535（实际可用约 64511 个），理论上能够达到 2³²×2¹⁶（或更高，若服务端同时监听多个端口）的级别，数值远远超出实际需求。
* **客户端与服务端的区别**

  * **客户端**：每个 TCP 连接需要占用一个本地端口（如果不调用 bind，系统会自动选择），所以在单 IP 下，客户端的最大连接数理论上确实受限于 65535 个（实际上可用端口数可能更少，因为有保留端口）。
  * **服务端**：服务端只需要监听一个固定端口，其接收的连接由客户端的不同 IP/端口组合来区分，因此并不受监听端口数的直接限制。

‍

‍

#### 2. 实际制约因素

尽管理论上连接数可以非常巨大，但实际部署中主要受以下几个因素限制

‍

(1) 内存消耗

* **每个连接的内存开销**  
  内核会为每个 TCP 连接分配一定的内存（例如 TCP 控制块、缓冲区、协议栈数据结构等），一般情况下大约在 3～20KB 左右（不同内核版本和配置有所差异）。例如，腾讯云和 CSDN 的相关文章中提到，在默认配置下，每个 socket 的内存消耗可能在 15～20KB 左右
* **内存总量决定并发上限**  
  假设一台服务器有 4GB 内存，如果全部用于保持空闲（无数据传输）的连接，每个连接占用约 3～4KB内存，那么理论上可以支持数十万甚至上百万的连接。不过实际部署中还要预留给操作系统和其他应用程序的内存，所以实际可用连接数会低一些。

‍

(2) 文件描述符限制

* **每个连接占用一个文件描述符**  
  在 Linux 中，socket 也是一种文件，所以每个 TCP 连接都会消耗一个文件描述符。系统对单个进程（或全局）的文件描述符数量都有默认限制（通常是 1024 或 65535），这也是常见的“too many open files”错误的原因
* **调优方向**  
  通过调整 ulimit、修改 `/etc/security/limits.conf`​、以及系统级参数（如 `/proc/sys/fs/file-max`​ 和 `/proc/sys/fs/nr_open`​）可以提高可打开的文件数，从而支持更多的连接。

‍

(3) 内核及网络栈参数

* **TCP 参数调优**  
  内核中还有一系列与 TCP 连接相关的参数，如 `net.core.somaxconn`​（监听队列长度）、`net.ipv4.tcp_max_syn_backlog`​、`net.ipv4.ip_local_port_range`​ 等，这些参数的配置直接影响连接建立、半连接队列和 TIME\_WAIT 状态下资源的回收
* **TIME_WAIT 状态**  
  连接关闭后会进入 TIME\_WAIT 状态，这部分连接也会占用文件描述符和内存。通过参数（例如 `tcp_tw_reuse`​、`tcp_tw_recycle`​，注意新版内核中部分参数已经弃用）可以对其进行一定的优化，但也需要权衡网络环境和 NAT 等情况。

‍

‍

#### 3. 综合结论

‍

* **理论上**：服务端能够建立的 TCP 连接数远远超过 65535，因为服务端不受监听端口的限制，连接的唯一性由客户端的 IP 和端口决定。
* **实际中**：主要限制因素在于服务器的内存、单进程或系统级的文件描述符上限以及 TCP 内核参数调优。经过适当调优后，一台内存较充足（例如 4GB 甚至更高）的 Linux 服务器是可以支持 10 万甚至上百万个空闲 TCP 连接的
* **调优建议**：

  1. 增大文件描述符的上限（ulimit、limits.conf、fs.file-max）。
  2. 调整内核参数，优化 TIME\_WAIT 资源回收和监听队列长度。
  3. 根据实际内存评估每个连接的开销，保证整体资源充足。

‍

因此，从计算机原理的角度看，服务端 TCP 连接的上限并不是固定在 65535 个，而是主要受限于服务器硬件（特别是内存）和操作系统资源配置。只要资源足够，并经过合理调优，单机支持上百万并发 TCP 连接在技术上是可行的。

‍

## 操作系统

‍

### Copy-on-write

写时复制（Copy-on-write，COW），有时也称为隐式共享（implicit sharing）。COW **将复制操作推迟到第一次写入时**进行：在创建一个新副本时，不会立即复制资源，而是共享原始副本的资源；当修改时再执行复制操作。通过这种方式共享资源，可以显著减少创建副本时的开销，以及节省资源；同时，资源修改操作会增加少量开销。

‍

‍

### 用户态内核态

‍

#### 为何要区分用户态和内核态

最简单的运行程序的方式是“直接执行”，即直接在 CPU 上执行任意程序。直接执行的问题是：

1. 如何限制代码行为？比如禁止：设置特殊寄存器的值、访问存储器的任意位置、I/O 请求、申请更多系统资源等
2. 在运行这个程序的时候，如何切换到另一个程序？进程调度应该是 OS 才有的权限

因此引入用户态和内核态和两种模式。用户态无法执行受限操作，如 I/O 请求，执行这些操作会引发异常。核心态只能由操作系统运行，可以执行特权操作。用户程序通过**系统调用** system call 执行这些特权操作。OS 执行前会判断进程是否有**权限**执行相应的指令。

区分用户态和核心态的执行机制称为“受限直接执行”（Limited Direct Execution）。

‍

‍

#### 需要进行用户态 内核态的切换场景

1. **系统调用 trap**
2. **中断** **interrupt**
3. **异常** **exception**
4. **上下文切换 context switch**

‍

系统调用是用户进程主动发起的操作。发起系统调用，陷入内核，由操作系统执行系统调用，然后再返回到进程。

中断和异常是被动的，无法预测发生时机。中断包括 I/O 中断、外部信号中断、各种定时器引起的时钟中断等。异常包括程序运算引起的各种错误如除 0、缓冲区溢出、缺页等。

上下文切换是当操作系统进行进程调度时，需要保存当前进程的状态并加载下一个进程的状态。这一过程也涉及用户态和内核态的切换

在系统的处理上，中断和异常类似，都是通过中断向量表来找到相应的处理程序进行处理。区别在于，中断来自处理器外部，不是由任何一条专门的指令造成，而异常是执行当前指令的结果。

这些切换场景确保了用户程序和操作系统内核之间的隔离，防止用户程序直接访问和修改系统资源，从而提高系统的安全性和稳定性。

‍

‍

‍

‍

### 编码问题综合

‍

#### 常用字符编码所占字节数？

​`utf8`​ (UTF, 全部可用, 但是中文要处理) :英文占 1 字节，中文占 3 字节，`unicode`​ (统一码, 统一规格) ：任何字符都占2 个字节，`gbk`​ (中文专用码) ：英文占 1 字节，中文占 2 字节

‍

#### **ASCII 码**

计算机内部，所有信息最终都是一个二进制值。每一个二进制位（bit）有`0`​和`1`​两种状态，因此八个二进制位就可以组合出256种状态，这被称为一个字节（byte）。也就是说，一个字节一共可以用来表示256种不同的状态，每一个状态对应一个符号，就是256个符号，从`00000000`​到`11111111`​。

上个世纪60年代，美国制定了一套字符编码，对英语字符与二进制位之间的关系，做了统一规定。这被称为 ASCII 码，一直沿用至今。

ASCII 码一共规定了128个字符的编码，比如空格`SPACE`​是32（二进制`00100000`​），大写的字母`A`​是65（二进制`01000001`​）。这128个符号（包括32个不能打印出来的控制符号），只占用了一个字节的后面7位，最前面的一位统一规定为`0`​。

‍

#### **非 ASCII 编码**

英语用128个符号编码就够了，但是用来表示其他语言，128个符号是不够的。

亚洲国家的文字，使用的符号就更多了，汉字就多达10万左右。一个字节只能表示256种符号，肯定是不够的，就必须使用多个字节表达一个符号。比如，简体中文常见的编码方式是 GB2312，使用两个字节表示一个汉字，所以理论上最多可以表示 256 x 256 = 65536 个符号。

GB类的汉字编码与后文的 Unicode 和 UTF-8 是毫无关系的。

‍

#### **Unicode**

如果有一种编码，将世界上所有的符号都纳入其中。每一个符号都给予一个独一无二的编码，那么乱码问题就会消失。这就是 Unicode，就像它的名字都表示的，这是一种所有符号的编码

Unicode 只是一个符号集，它只规定了符号的二进制代码，却没有规定这个二进制代码应该如何存储。

‍

‍

#### **UTF-8**

互联网的普及，强烈要求出现一种统一的编码方式。UTF-8 就是在互联网上使用最广的一种 Unicode 的实现方式。其他实现方式还包括 UTF-16（字符用两个字节或四个字节表示）和 UTF-32（字符用四个字节表示），不过在互联网上基本不用。**重复一遍，这里的关系是，UTF-8 是 Unicode 的实现方式之一。**

UTF-8 最大的一个特点，就是它是一种变长的编码方式。它可以使用1~4个字节表示一个符号，根据不同的符号而变化字节长度。

UTF-8 的编码规则很简单，只有二条：

1）对于单字节的符号，字节的第一位设为`0`​，后面7位为这个符号的 Unicode 码。因此对于英语字母，UTF-8 编码和 ASCII 码是相同的。

2）对于`n`​字节的符号（`n > 1`​），第一个字节的前`n`​位都设为`1`​，第`n + 1`​位设为`0`​，后面字节的前两位一律设为`10`​。剩下的没有提及的二进制位，全部为这个符号的 Unicode 码。

‍

### 单核 CPU 上运行死循环程序咋样

在单核 CPU 上运行死循环程序时，操作系统调度器通过以下步骤来管理 CPU 资源：

‍

1. **时间片轮转**：操作系统使用**时间片轮转调度**算法，将 CPU 时间分成若干个**时间片**（通常是几毫秒）。每个进程在其时间片内运行，时间片结束后，调度器会将 CPU 切换到下一个进程。
2. **中断**：当一个进程的时间片用完时，硬件定时器会触发一个**时钟中断**。中断处理程序会暂停当前进程的执行，并将控制权交给操作系统内核。
3. **上下文切换**：操作系统内核保存当前进程的状态（如寄存器、程序计数器等），然后加载下一个进程的状态。这一过程称为上下文切换。
4. **调度**：调度器根据调度算法（如优先级调度、时间片轮转等）选择下一个要运行的进程，并将其分配给 CPU。

‍

即使一个进程进入死循环，操作系统调度器仍然能够通过**时间片轮转**和**中断机制**确保其他进程有机会获得 CPU 资源

‍

```plaintext
进程 A (死循环)  -->  时间片结束  -->  时钟中断  -->  上下文切换  -->  进程 B 运行  -->  时间片结束  -->  时钟中断  -->  上下文切换  -->  进程 A 继续运行
```

这个过程确保了即使在单核 CPU 上，操作系统也能公平地分配 CPU 资源给所有进程

‍

‍

‍

‍

### 虚拟内存管理综合问题 + 局部性原理

‍

虚拟内存管理是操作系统用来管理计算机内存的一种机制。它通过将物理内存抽象为虚拟地址空间，使得每个程序都认为自己在使用连续的内存空间，而实际上这些地址可能被映射到不同的物理内存位置

‍

1. **程序打印出来的指针属于什么**：

    * 程序打印出来的指针是虚拟地址。它是程序在其虚拟地址空间中使用的地址。
2. **属于虚拟内存吗**：

    * 是的，程序打印出来的指针属于虚拟内存。操作系统将这些虚拟地址映射到物理内存地址。
3. **两个程序能否打印相同的地址吗**：

    * 是的，两个程序可以打印相同的虚拟地址。每个程序都有自己的独立虚拟地址空间，因此相同的虚拟地址在不同的程序中可以指向不同的物理内存位置。
4. **不同的虚拟地址能否映射到同一块物理空间呢**：

    * 是的，不同的虚拟地址可以映射到同一块物理内存。这种情况在共享内存或内存映射文件等机制中很常见。操作系统通过页表来管理这些映射关系。

‍

‍

#### 局部性原理在计算机领域的应用

‍

1. **时间局部性**：时间局部性指的是如果某个数据被访问过，那么在不久的将来它很可能会再次被访问。虚拟内存管理通过将最近访问的数据保存在高速缓存中来利用时间局部性，从而减少访问内存的时间。
2. **空间局部性**：空间局部性指的是如果某个数据被访问过，那么它附近的数据很可能也会被访问。虚拟内存管理通过将相邻的数据块一起加载到内存中来利用空间局部性，从而提高内存访问效率。

‍

在虚拟内存管理中，局部性原理的应用主要体现在以下几个方面：

* **页面置换算法**：操作系统使用页面置换算法（如LRU算法）来决定哪些页面应该被保留在内存中，哪些页面应该被换出。LRU算法利用时间局部性，优先保留最近使用的页面。
* **预取机制**：操作系统可以通过预取机制利用空间局部性，将相邻的页面提前加载到内存中，从而减少缺页中断的次数。
* **缓存管理**：操作系统和硬件通过多级缓存（如L1、L2、L3缓存）来利用时间局部性和空间局部性，提高内存访问速度。

‍

‍

‍

‍

### cpu时间片每个多大? 它cpu的时间片是怎么淘汰的，不同线程获取cpu时间片的竞争过程展开讲讲

‍

CPU时间片的大小与调度竞争机制

* **时间片大小**

  * 时间片的长度由操作系统调度器确定，不同操作系统或不同配置下可能不同；在 Linux 下常见值大约在10\~100毫秒之间。
  * 现代调度器（如 Linux 的 CFS）并非严格采用固定时间片，而是根据各线程的虚拟运行时间动态调整。
* **时间片淘汰与抢占**

  * 当一个线程使用完当前分配的时间片后，操作系统会暂停它，将其状态置为就绪状态，并从就绪队列中选择下一个线程运行。
  * 线程竞争时，调度器依据线程优先级、历史运行时间、等待时间等因素进行调度（如抢占式调度），实现资源公平分配和高响应性。
* **调度竞争过程**

  * 每个线程被分配一个时间片，在运行过程中若遇到阻塞（I/O、等待锁等），则主动释放 CPU。
  * 调度器会在中断、系统调用或时间片耗尽时触发上下文切换，从而让其他就绪线程获得 CPU 时间片。

‍

‍

‍

‍
