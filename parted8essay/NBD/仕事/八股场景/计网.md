‍

‍

# 整体

‍

‍

‍

‍

‍

‍

‍

‍

‍

‍

‍

‍

‍

‍

## 物理层

> 物理层考虑的是怎样才能在连接各种计算机的传输介质上传输数据比特流。现有的计算机网络中的硬件设备和传输媒体(介质)的种类非常多，而通信手段也有许多不同方式。物理层的作用正是要尽可能地屏蔽掉这些传输媒体和硬件设备的差异，使物理层上面的数据链路层感觉不到这些差异，这样就可使数据链路层只考虑如何完成本层的协议和服务，而不必考虑网络具体的传输媒体和通信手段是什么。

‍

给我看仔细了混蛋

​![v2-eff003f3637b95a98c62e56d62b86160_1440w](assets/net-img-v2-eff003f3637b95a98c62e56d62b86160_1440w-20241109225217-3yj8f3k.jpg)​

‍

‍

### OSI七层模型

* 网络七层模型是一个标准，而非实现。
* 网络四层模型是一个实现的应用模型。
* 网络四层模型由七层模型简化合并而来。

‍

* 物理层：底层数据传输，如网线；网卡标准。    数据被称为**比特流**（Bits）
* 数据链路层：定义数据的基本格式，如何传输，如何标识；如网卡MAC地址。    数据被称为**帧**（Frames）
* 网络层：定义IP编址，定义路由功能；如不同设备的数据转发。    数据被称做**包**（Packages）
* 传输层：端到端传输数据的基本功能；如 TCP、UDP。    数据被称作**段**（Segments）
* 会话层：控制应用程序之间会话能力；如不同软件数据分发给不同软件。
* 表示层：数据格式标识，基本压缩加密功能。
* 应用层：各种应用软件，包括 Web 应用。

‍

很好的图例

‍

‍

#### PDU :比特、帧、数据包、数据段、报文

‍

PDU：Protocol data unit，协议数据单元，指对等层协议之间交换的信息单元

PDU 再往上就是数据（data）

‍

在 OSI 模型里，PDU 和底下四层相关

* 物理层———**比特（Bit）**
* 数据链路层———**帧（Frame）**
* 网络层———**分组、数据包（Packet）**
* 传输层———**数据段（Segment）**

第五层或以上为**数据（data）**

也有一种说法是，应用层的信息称为**消息、报文（message）** ，表示完整的信息。

‍

#### 硬件: 集线器、网桥、交换机、路由器

* 网线是物理层的硬件
* 集线器（Hub）是**物理层**的硬件，连接所有的线路，广播所有信息
* 网桥（Bridge）是**数据链路层**的硬件。网桥隔离两个端口，不同的端口形成单独的冲突域，减少网内冲突。网桥在不同或相同类型的 LAN 之间存储并转发数据帧，根据 MAC 头部来决定转发端口，显然是数据链路层的设备
* 交换机（Switch）是**数据链路层**的硬件，相当于多端口的网桥。交换机内部存储 MAC 表，只会将数据帧发送到指定的目的地址
* 路由器（Router）是**网络层**的硬件，根据 IP 地址进行寻址，不同子网间的数据传输隔离

‍

‍

### **TCP/IP四层模型**

实际应用中广泛使用的网络模型。TCP/IP 模型将 OSI 七层模型简化为四层，每层的功能如下：

‍

* **网络接口层**：物理网络上的数据传输（帧）。
* **网络层**：IP 地址和路由选择（包）。
* **传输层**：端到端的数据传输（段）。
* **应用层**：应用程序之间的通信（数据）。

‍

1. **网络接口层（Network Interface Layer）** ：

    * 负责在物理网络上发送和接收数据。
    * 包括 OSI 模型的物理层和数据链路层。
    * 数据被称为 **帧**（Frames）。
2. **网络层（Internet Layer）** ：

    * 负责逻辑地址（IP 地址）的分配和路由选择。
    * 包括 OSI 模型的网络层。
    * 数据被称为 **包**（Packets）。
3. **传输层（Transport Layer）** ：

    * 负责端到端的数据传输和流量控制。
    * 包括 OSI 模型的传输层。
    * 数据被称为 **段**（Segments）。
    * 主要协议有 TCP 和 UDP。
4. **应用层（Application Layer）** ：

    * 负责应用程序之间的通信。
    * 包括 OSI 模型的会话层、表示层和应用层。
    * 数据被称为 **数据**（Data）。
    * 主要协议有 HTTP、FTP、SMTP 等。

‍

‍

‍

## 数链层

> 数据链路层研究的是分组怎样从一台主机传送到另一台主机，但并不经过路由器转发。从整个互联网来看，局域网仍属于数据链路层的范围。数据传送单位是帧。

> 1、在两个相邻节点之间传送数据时，数据链路层将网络层交下来的**IP 数据报**添加首部和尾部组装成帧 **，在两个相邻节点间的链路上传送帧。** 每一帧包括数据和必要的控制信息【如同步信息，地址信息，差错控制等】。
>
> 2、数据链路层在收到一个帧后，通过控制信息检测收到的帧中是否有差错，如果没有就可从中提出**IP数据报**部分，上交给网络层 **。** 如果发现差错，数据链路层就简单地丢弃这个出了差错的帧，以避免继续在网络中传送下去白白浪费网络资源 【在接收数据时，控制信息使接收端能够知道一个帧从哪个比特开始和到哪个比特结束。控制信息还使接收端能够检测到所收到的帧中有误差错。如果发现差错，数据链路层就简单地丢弃这个出了差错的帧，以避免继续在网络中传送下去白白浪费网络资源。如果需要改正数据在链路层传输时出现差错（这就是说，数据链路层不仅要检错，而且还要纠错），那么就要采用可靠性传输协议来纠正出现的差错。这种方法会使链路层的协议复杂些】

‍

---

> ARP 协议：ARP 协议应该属于哪一层？一种说法是属于网络层，因为 IP 协议使用 ARP 协议；另一种说法是属于数据链路层，因为 MAC 地址是数据链路层的内容。在 OSI 模型中，ARP 协议属于链路层；而在 TCP/IP 模型中，ARP 协议属于网络层

我同意后者说法, 是数据链路层的东西, 而不是IP那个. 

‍

‍

‍

‍

## 网络层

> 选择合适的网间路由和交换结点，确保数据及时传送。网络层向上只提供简单灵活的、无连接的、尽最大努力交付的IP数据报服务. 在发送分组时不需要先建立连接，没有给分组进行上编号，所传送的分组可能出错、丢失、重复和失序。如果主机（即端系统）中的进程之间的通信需要是可靠的，那么就由网络的主机中的运输层负责（包括差错处理、流量控制等）

‍

‍

### 常见协议

‍

IP（Internet Protocol，网际协议）：TCP/IP 协议中最重要的协议之一，主要作用是定义数据包的格式、对数据包进行路由和寻址，以便它们可以跨网络传播并到达正确的目的地。目前 IP 协议主要分为两种，一种是过去的 IPv4，另一种是较新的 IPv6，目前这两种协议都在使用，但后者已经被提议来取代前者。

ARP（Address Resolution Protocol，地址解析协议）：ARP 协议解决的是网络层地址和链路层地址之间的转换问题。因为一个 IP 数据报在物理上传输的过程中，总是需要知道下一跳（物理上的下一个目的地）该去往何处，但 IP 地址属于逻辑地址，而 MAC 地址才是物理地址，ARP 协议解决了 IP 地址转 MAC 地址的一些问题。

ICMP（Internet Control Message Protocol，互联网控制报文协议）：一种用于传输网络状态和错误消息的协议，常用于网络诊断和故障排除。例如，Ping 工具就使用了 ICMP 协议来测试网络连通性。

NAT（Network Address Translation，网络地址转换协议）：NAT 协议的应用场景如同它的名称——网络地址转换，应用于内部网到外部网的地址转换过程中。具体地说，在一个小的子网（局域网，LAN）内，各主机使用的是同一个 LAN 下的 IP 地址，但在该 LAN 以外，在广域网（WAN）中，需要一个统一的 IP 地址来标识该 LAN 在整个 Internet 上的位置。

OSPF（Open Shortest Path First，开放式最短路径优先） ）：一种内部网关协议（Interior Gateway Protocol，IGP），也是广泛使用的一种动态路由协议，基于链路状态算法，考虑了链路的带宽、延迟等因素来选择最佳路径。

RIP(Routing Information Protocol，路由信息协议）：一种内部网关协议（Interior Gateway Protocol，IGP），也是一种动态路由协议，基于距离向量算法，使用固定的跳数作为度量标准，选择跳数最少的路径作为最佳路径。

BGP（Border Gateway Protocol，边界网关协议）：一种用来在路由选择域之间交换网络层可达性信息（Network Layer Reachability Information，NLRI）的路由选择协议，具有高度的灵活性和可扩展性。

‍

‍

‍

## 运输层

> 负责向两台主机的进程之间提供通用的数据传输服务。
>
> 复用和分用。这里的“复用"是指在发送方不同的应用进程都可以使用同一个运输层协议传送数据(当然需要加上适当的首部)，而“分用”是指接收方的运输层在剥去报文的首部后能够把这些数据正确交付目的应用进程
>
> 向高层用户屏蔽了下面网络核心的细节【如网络拓扑、所采用的路由选择协议等】，**它使应用进程看见的就是好像在两个运输层实体之间有一条端到端的逻辑通信信道**。但这条逻辑通信信道对上层的表现却因运输层使用的不同协议而有很大的差别。当运输层釆用面向连接的TCP协议时，尽管下面的网络层是不可靠的（只提供尽最大努力服务），但这种逻辑通信信道就相当于一条全双工的可靠信道。但当运输层采用无连接的udp协议时，这种逻辑通信信道仍然是一条不可靠信道。

‍

‍

### TCP

> TCP（Transmission Control Protocol，传输控制协议）是一个面向连接的、可靠的、基于字节流的传输层协议。从它的概念中我们可以看出 TCP 的三个特点：
>
> * **面向连接**
> * **可靠性**
> * **面向字节流**。

‍

‍

#### TCP对应的应用层协议

HTTP, 超文本传输协议

FTP, 文件传输协议

SMTP, 简单邮件传输协议，用来发送电子邮件

SSH 安全外壳协议,用于加密安全登陆

‍

‍

#### TCP头部信息

* 序号（32bit）：传输方向上字节流的字节编号。初始时序号会被设置一个随机的初始值（ISN），之后每次发送数据时，序号值 = ISN + 数据在整个字节流中的偏移。假设A -> B且ISN = 1024，第一段数据512字节已经到B，则第二段数据发送时序号为1024 + 512。用于解决网络包乱序问题。
* 确认号（32bit）：接收方对发送方TCP报文段的响应，其值是收到的序号值 + 1 (期望收到这个)
* 首部长（4bit）：标识首部有多少个4字节 * 首部长，最大为15，即60字节。
* 标志位（6bit）：

  * URG：标志紧急指针是否有效。
  * ACK：标志确认号是否有效（确认报文段）。用于解决丢包问题。
  * PSH：提示接收端立即从缓冲读走数据。
  * RST：表示要求对方重新建立连接（复位报文段）。
  * SYN：表示请求建立一个连接（连接报文段）。
  * FIN：表示关闭连接（断开报文段）。
* 窗口（16bit）：接收窗口。用于告知对方（发送方）本方的缓冲还能接收多少字节数据。用于解决流控。
* 校验和（16bit）：接收端用CRC检验整个报文段有无损坏。

‍

Sequence Number

> 称为「序列号」。用于 TCP 通信过程中某一传输方向上字节流的每个字节的编号，为了确保数据通信的有序性，避免网络中乱序的问题。接收端根据这个编号进行确认，保证分割的数据段在原始数据包的位置。初始序列号由自己定，而后绪的序列号由对端的 ACK 决定：SN_x = ACK_y (x 的序列号 = y 发给 x 的 ACK)。

Acknowledge Number

> 称为「确认序列号」。确认序列号是接收确认端所期望收到的下一序列号。确认序号应当是上次已成功收到数据字节序号加1，只有当标志位中的 ACK 标志为 1 时该确认序列号的字段才有效。主要用来解决不丢包的问题。

‍

‍

#### MSL、TTL、RTT

‍

MSL（Maximum segment lifetime）：报文最大生存时间。

任何 TCP 报文在网络上存在的最长时间，超过这个时间报文将被丢弃。实际应用中常用的设置是 30 秒，1 分钟和 2 分钟。

* 应用场景：TCP 四次挥手时，需要在 TIME-WAIT 状态等待 2MSL 的时间，可以保证本次连接产生的所有报文段都从网络中消失。

‍

TTL（Time to live）：IP 数据报在网络中可以存活的总跳数，称为“生存时间”，但并不是一个真正的时间。该域由源主机设置初始值，每经过一个路由器，跳数减 1，如果减至 0，则丢弃该数据包，同时发送 ICMP 报文通知源主机。取值范围 1-255，如果设置的 TTL 值小于传输过程中需要经过的路由器数量，则该数据包在传输中就会被丢弃。

‍

RTT（Round trip time）：客户端到服务端往返所花时间。RTT 受网络传输拥塞的变化而变化，由 TCP **动态地估算**。

‍

‍

#### 序列号和确认号变化

万能公式

发送的 TCP 报文：

* 公式一：序列号 = **上一次发送的序列号 + len（数据长度）** 。特殊情况，如果上一次发送的报文是 SYN 报文或者 FIN 报文，则改为 上一次发送的序列号 + 1。
* 公式二：确认号 = **上一次收到的报文中的序列号 + len（数据长度）** 。特殊情况，如果收到的是 SYN 报文或者 FIN 报文，则改为上一次收到的报文中的序列号 + 1。

‍

‍

#### 三次握手

‍

TCP 三次握手的执行流程

* SYN（Synchronize Sequence Numbers），同步序列编号；
* ACK（Acknowledge Character），确认字符；
* SEQ（Sequence Number），序列号。

‍

‍

##### 为什么三次

‍

* 第一次握手：客户端发送网络包，服务端收到了。 这样服务端就能得出结论：客户端的发送能力、服务端的接收能力是正常的。
* 第二次握手：服务端发包，客户端收到了。 这样客户端就能得出结论：服务端的接收、发送能力，客户端的接收、发送能力是正常的。不过此时服务器并不能确认客户端的接收能力是否正常。
* 第三次握手：客户端发包，服务端收到了。 这样服务端就能得出结论：客户端的接收、发送能力正常，服务器自己的发送、接收能力也正常。

因此，需要三次握手才能确认双方的接收与发送能力是否正常。

‍

> 其实就是非常简单的 "你能听到我讲话吗?" "我能听到, 你能听到我讲话吗?" "我能听到你讲话, 我们开始吧"

* 两次握手**只能保证单向连接是畅通的**。只有经过第三次握手，才能确保双向都可以接收到对方的发送的数据。两次握手接收方这里不能确定自己的的发送是正常的，发送方的接收是正常的。
* 防止已经失效的连接请求报文突然又传送到了服务器，从而产生错误。

‍

##### SIN/FIN不包含数据却要消耗序列号

凡是需要对端确认的，一定消耗TCP报文的序列号。SYN和FIN需要对端的确认，因此需要消耗一个序列号。

SYN作为三次握手的确认。FIN作为四次挥手的确认。如果没有序列号，会导致SYN请求多次重发，服务端多次处理，造成资源浪费

‍

‍

#### 四次挥手

‍

‍

##### 为什么四次

因为只有在客户端和服务端都**没有数据要发送**的时候才能断开TCP。

而客户端发出FIN报文时只能保证客户端没有数据发了，服务端还有没有数据发客户端是不知道的。而服务端收到客户端的FIN报文后只能先回复客户端一个确认报文来告诉客户端我服务端已经收到你的FIN报文了，但我服务端还有一些数据没发完，等这些数据发完了服务端才能给客户端发FIN报文(所以**不能一次性将确认报文和FIN报文发给客户端**，就是这里多出来了一次)。

‍

> 任何一方都可以在数据传送结束后发出连接释放的通知，待对方确认后进入半关闭状态。当另一方也没有数据再发送的时候，则发出连接释放通知，对方确认后就完全关闭了TCP连接。
>
> A 和 B 打电话，通话即将结束后，A 说“我没啥要说的了”，B回答“我知道了”，但是 B 可能还会有要说的话，最后 B 说“我说完了”，A 回答“知道了”，这样通话才算结束。

‍

‍

##### 四次挥手过程

建立一个连接需要三次握手，而终止一个连接要经过四次挥手（也有将四次挥手叫做四次握手的）。这由TCP的**半关闭**（half-close）造成的。所谓的半关闭，其实就是TCP提供了 连接的一端在结束它的发送后 还能接收来自另一端数据 的能力。

TCP 的连接的拆除需要发送四个包，因此称为四次挥手(Four-way handshake)，客户端或服务器均可主动发起挥手动作。

‍

* **初始化状态**：客户端和服务端都在连接状态，接下来开始进行四次分手断开连接操作。
* **第一次分手**：第一次分手无论是客户端还是服务端都可以发起，因为 TCP 是全双工的。

> 假如客户端发送的数据已经发送完毕，发送 FIN = 1 **告诉服务端，客户端所有数据已经全发完了**，**服务端你可以关闭接收了**，但是如果你们服务端有数据要发给客户端，客户端照样可以接收的。此时客户端处于FIN = 1等待服务端确认释放连接状态。

* **第二次分手**：服务端接收到客户端的释放请求连接之后，**知道客户端没有数据要发给自己了**，**然后服务端发送ACK =**  **1告诉客户端收到你发给我的信息**，此时服务端处于 CLOSE_WAIT 等待关闭状态。（服务端先回应给客户端一声，我知道了，但服务端的发送数据能力即将等待关闭，于是接下来第三次就来了。）
* **第三次分手**：此时服务端向客户端把所有的数据发送完了，然后发送一个FIN = 1，**用于告诉客户端，服务端的所有数据发送完毕**，**客户端你也可以关闭接收数据连接了**。此时服务端状态处于LAST_ACK状态，来等待确认客户端是否收到了自己的请求。（服务端等客户端回复是否收到呢，不收到的话，服务端不知道客户端是不是挂掉了还是咋回事呢，所以服务端不敢关闭自己的接收能力，于是第四次就来了。）
* **第四次分手**：此时如果客户端收到了服务端发送完的信息之后，就发送ACK = 1，告诉服务端，客户端已经收到了你的信息。**有一个 2 MSL 的延迟等待**。

‍

‍

##### 释放连接时等待2MSL的意义?

> **MSL**是Maximum Segment Lifetime的英文缩写，可译为“最长报文段寿命”，它是任何报文在网络上存在的最长时间，超过这个时间报文将被丢弃。

为了保证客户端发送的**最后一个ACK报文段能够到达服务器**。因为这个ACK有可能丢失，从而导致处在LAST-ACK状态的服务器收不到对FIN-ACK的确认报文。服务器会超时重传这个FIN-ACK，接着客户端再重传一次确认，重新启动时间等待计时器。最后客户端和服务器都能正常的关闭。假设客户端不等待2MSL，而是在发送完ACK之后直接释放关闭，一但这个ACK丢失的话，服务器就无法正常的进入关闭连接状态。

‍

‍

1. 保证客户端发送的最后一个ACK报文段能够到达服务端。 这个ACK报文段有可能丢失，使得处于LAST-ACK状态的B收不到对已发送的FIN+ACK报文段的确认，服务端超时重传FIN+ACK报文段，而客户端能在2MSL时间内收到这个重传的FIN+ACK报文段，接着客户端重传一次确认，重新启动2MSL计时器，最后客户端和服务端都进入到CLOSED状态，若客户端在TIME-WAIT状态不等待一段时间，而是发送完ACK报文段后立即释放连接，则无法收到服务端重传的FIN+ACK报文段，所以不会再发送一次确认报文段，则服务端无法正常进入到CLOSED状态。
2. 防止“**已失效的连接请求报文段**”出现在本连接中。 客户端在发送完最后一个ACK报文段后，再经过2MSL，就可以使本连接持续的时间内所产生的所有报文段都从网络中消失，使下一个新的连接中不会出现这种旧的连接请求报文段。

‍

‍

##### CLOSE-WAIT和TIME-WAIT存在的意义

Close-wait存在的意义:

 就是服务端还有数据要发送，这个时间内就是服务端发送完最后的数据

‍

Time-wait存在的意义

* 第一，这里同样是要考虑丢包的问题，如果第四次挥手的报文丢失，服务端没收到确认ack报文就会重发第三次挥手的报文，这样报文一去一回最长时间就是2MSL，所以需要等这么长时间来确认服务端确实已经收到了
* 第二，防止类似与“三次握手”中提到了的“已经失效的连接请求报文段”出现在本连接中。客户端发送完最后一个确认报文后，在这个2MSL时间中，就可以使本连接持续的时间内所产生的所有报文段都从网络中消失。这样新的连接中不会出现旧连接的请求报文。

‍

‍

‍

‍

#### TCP和UDP区别 

TCP 和 UDP 的区别主要体现在以下 7 个方面：

* **可靠性**，TCP 有“状态性”和“可控制性”可以保证消息不重复、按顺序、不丢失的发送和接收，而 UDP 则不能保证消息的可靠性；
* **连接**，TCP 是面向连接的传输层协议，传输数据前先要建立连接，而 UDP 发送数据之前无需建立连接；
* **服务对象**，TCP 服务的对象为一对一的双端应用，而 UDP 可以应用于一对一、一对多和多对多的通信场景；
* **效率**，TCP 的传输效率较低，而 UDP 的传输效率较 高；
* **流量控制**，TCP 有滑动窗口可以用来控制流量，而 UDP 则不具备流量控制的能力；
* **报文**，TCP 是面向字节流的传输层协议，而 UDP 是面向报文的传输层协议；
* **应用场景**，TCP 的应用场景是对消息准确性和顺序要求较高的场景，而 UDP 则是应用于对通信效率较高、准确性要求相对较低的场景。

‍

‍

#### 流量控制原理

* 目的是接收方通过**TCP头窗口字段**告知发送方本方可接收的最大数据量，用以解决发送速率过快导致接收方不能接收的问题。所以流量控制是点对点控制。
* TCP是双工协议，双方可以同时通信，所以发送方接收方各自维护一个发送窗和接收窗。

  * 发送窗：用来限制发送方可以发送的数据大小，其中发送窗口的大小由接收端返回的TCP报文段中窗口字段来控制，接收方通过此字段告知发送方自己的缓冲（受系统、硬件等限制）大小。
  * 接收窗：用来标记可以接收的数据大小。
* TCP是流数据，发送出去的数据流可以被分为以下四部分：已发送且被确认部分 | 已发送未被确认部分 | 未发送但可发送部分 | 不可发送部分，其中发送窗 = 已发送未确认部分 + 未发但可发送部分。接收到的数据流可分为：已接收 | 未接收但准备接收 | 未接收不准备接收。接收窗 = 未接收但准备接收部分。
* 发送窗内数据只有当接收到接收端某段发送数据的ACK响应时才移动发送窗，左边缘紧贴刚被确认的数据。接收窗也只有接收到数据且最左侧连续时才移动接收窗口。

‍

‍

#### 拥塞控制&流量控制4个家伙的区别

‍

> 为了进行拥塞控制，TCP 发送方要维持一个 拥塞窗口(cwnd) 的状态变量。拥塞控制窗口的大小取决于网络的拥塞程度，并且动态变化。发送方让自己的发送窗口取为拥塞窗口和接收方的接受窗口中较小的一个。**慢开始和拥塞避免都是基于窗口的拥塞控制**

‍

==区别 2 -&gt; 4==

拥塞控制

* **慢开始：**  慢开始算法的思路是当主机开始发送数据时，如果立即把大量数据字节注入到网络，那么可能会引起网络阻塞，因为现在还不知道网络的符合情况。经验表明，较好的方法是先探测一下，即由小到大逐渐增大发送窗口，也就是由小到大逐渐增大拥塞窗口数值。cwnd初始值为1，每经过一个传播轮次，cwnd加倍
* **拥塞避免：**  拥塞避免算法的思路是让拥塞窗口cwnd缓慢增大，即每经过一个往返时间RTT就把发送放的cwnd加 1

  拥塞避免算法：让拥塞窗口cwnd缓慢地增大，即每经过一个往返时间RTT就把发送方的拥塞窗口cwnd加1，而不是加倍。这样拥塞窗口cwnd按线性规律缓慢增长，比慢开始算法的拥塞窗口增长速率缓慢得多。

‍

流量控制

* **快重传**

  要求接收方每收到一个失序的报文段后就立即发出重复确认（为的是使发送方及早知道有报文段没有到达对方）而不要等到自己发送数据时才进行捎带确认
* **快恢复**

  发送方知道现在只是丢失了个别的报文段。于是不启动慢开始，而是执行**快恢复算法**。这时，发送方调整门限值，同时设置拥塞窗口，并开始执行拥塞避免算法

‍

‍

#### 可靠传输? 

‍

##### 第一种回答

* **确认和重传**：接收方收到报文就会确认，发送方发送一段时间后没有收到确认就会重传。
* **数据校验**：TCP报文头有校验和，用于校验报文是否损坏。
* **数据合理分片和排序**：tcp会按最大传输单元(MTU)合理分片，接收方会缓存未按序到达的数据，重新排序后交给应用层。而UDP：IP数据报大于1500字节，大于MTU。这个时候发送方的IP层就需要分片，把数据报分成若干片，是的每一片都小于MTU。而接收方IP层则需要进行数据报的重组。由于UDP的特性，某一片数据丢失时，接收方便无法重组数据报，导致丢弃整个UDP数据报。
* **流量控制**：当接收方来不及处理发送方的数据，能通过滑动窗口，提示发送方降低发送的速率，防止包丢失。
* **拥塞控制**：当网络拥塞时，通过拥塞窗口，减少数据的发送，防止包丢失。

‍

##### 第二种回答

* 建立连接（标志位）：通信前确认通信实体存在。
* 序号机制（序号、确认号）：确保了数据是按序、完整到达。
* 数据校验（校验和）：CRC校验全部数据。
* 超时重传（定时器）：保证因链路故障未能到达数据能够被多次重发。
* 窗口机制（窗口）：提供流量控制，避免过量发送。
* 拥塞控制：同上。

‍

##### 第三种回答

‍

**首部校验**  
这个校验机制能够确保数据传输不会出错吗？ 答案是不能。

‍

**原因**

TCP协议中规定，TCP的首部字段中有一个字段是校验和，发送方将伪首部、TCP首部、TCP数据使用累加和校验的方式计算出一个数字，然后存放在首部的校验和字段里，接收者收到TCP包后重复这个过程，然后将计算出的校验和和接收到的首部中的校验和比较，如果不一致则说明数据在传输过程中出错。

这就是TCP的数据校验机制。 但是这个机制能够保证检查出一切错误吗？**显然不能**。

因为这种校验方式是累加和，也就是将一系列的数字（TCP协议规定的是数据中的每16个比特位数据作为一个数字）求和后取末位。 但是小学生都知道A+B=B+A，假如在传输的过程中有前后两个16比特位的数据前后颠倒了（至于为什么这么巧合？我不知道，也许路由器有bug？也许是宇宙中的高能粒子击中了电缆？反正这个事情的概率不为零，就有可能会发生），那么校验和的计算结果和颠倒之前是一样的，那么接收端肯定无法检查出这是错误的数据。

‍

**解决方案**

传输之前先使用**MD5加密数据**获得摘要，跟数据一起发送到服务端，服务端接收之后对数据也进行MD5加密，如果加密结果和摘要一致，则认为没有问题

‍

‍

‍

‍

### UDP

‍

#### UDP对应的应用层协议

DHCP协议：动态主机配置协议，动态配置IP地址

NTP协议：网络时间协议，用于网络时间同步

RIP（路由选择协议）

DNS

‍

‍

#### TCP和UDP的区别

1、TCP面向连接（如打电话要先拨号建立连接）;UDP是无连接的，即发送数据之前不需要建立连接

2、TCP提供可靠的服务。也就是说，通过TCP连接传送的数据，无差错，不丢失，不重复，且按序到达;UDP尽最大努力交付，即不保证可靠交付

3、TCP面向字节流，实际上是TCP把数据看成一连串无结构的字节流;UDP是面向报文的

UDP没有拥塞控制，因此网络出现拥塞不会使源主机的发送速率降低（对实时应用很有用，如IP电话，实时视频会议等）

4、每一条TCP连接只能是点到点的; UDP支持一对一，一对多，多对一和多对多的交互通信

5、TCP首部开销20字节; UDP的首部开销小，只有8个字节

6、TCP的逻辑通信信道是全双工的可靠信道，UDP则是不可靠信道

7、UDP是面向报文的，发送方的UDP对应用层交下来的报文，不合并，不拆分，只是在其上面**加上首部**后就交给了下面的网络层，论应用层交给UDP多长的报文，它统统发送，一次发送一个。而对接收方，接到后直接去除首部，交给上面的应用层就完成任务了。因此，它需要应用层控制报文的大小

TCP是面向字节流的，它把上面应用层交下来的数据看成无结构的字节流会发送，可以想象成流水形式的，发送方TCP会将数据放入“蓄水池”（缓存区），等到可以发送的时候就发送，不能发送就等着TCP会根据当前网络的拥塞状态来确定每个报文段的大小。

‍

‍

‍

‍

### 粘包现象

**TCP粘包**是指发送方发送的若干包数据到接收方接收时粘成一包，从接收缓冲区看，后一包数据的头紧接着前一包数据的尾。

不是所有的粘包现象都需要处理，若传输的数据为**不带结构的连续流数据**（如文件传输）- 流式传输，则不必把粘连的包分开（简称分包）。但在实际工程应用中，传输的数据一般为带结构的数据，这时就需要做分包处理。分包一般难度较大,所以尽量避免粘包

‍

‍

#### 原因

一个完整的业务可能会被TCP拆分成多个包进行发送，也有可能把多个小的包封装成一个大的数据包发送，这个就是TCP的拆包和粘包问题。

‍

1.UDP协议的保护消息边界使得每一个消息都是独立的

2.而tcp是基于流的传输，流传输却把数据当作一串数据流，他不认为数据是一个一个的消息

3.发送端需要等缓冲区满才发送出去，造成粘包

4.接收方不及时接收缓冲区的包，造成多个包粘包

‍

具体点：**分发送方和接收方来看**

（1）发送方引起的粘包是由TCP协议本身造成的，TCP为提高传输效率，发送方往往要收集到足够多的数据后才发送一包数据。若连续几次发送的数据都很少，通常TCP会根据优化算法把这些数据合成一包后一次发送出去，这样接收方就收到了粘包数据。

（2）接收方引起的粘包是由于接收方用户进程不及时接收数据，从而导致粘包现象。这是因为接收方先把收到的数据放在系统接收缓冲区，用户进程从该缓冲区取数据，若下一包数据到达时前一包数据尚未被用户进程取走，则下一包数据放到系统接收缓冲区时就接到前一包数据之后，而用户进程根据预先设定的缓冲区大小从系统接收缓冲区取数据，这样就一次取到了多包数据。

‍

1、应用程序写入数据的字节大小大于套接字发送缓冲区的大小.

2、进行MSS大小的TCP分段。( MSS=TCP报文段长度-TCP首部长度)

3、以太网的payload大于MTU进行IP分片。（ MTU指：一种通信协议的某一层上面所能通过的最大数据包大小。）

‍

* 由TCP**连接复用**造成的粘包问题。
* 因为TCP默认会使用**Nagle算法**，此算法会导致粘包问题。

  * 只有上一个分组得到确认，才会发送下一个分组；
  * 收集多个小分组，在一个确认到来时一起发送。
* **数据包过大**造成的粘包问题。
* 流量控制，**拥塞控制**也可能导致粘包。
* **接收方不及时接收缓冲区的包，造成多个包接收**

---

‍

‍

#### 解决方法

（1）对于发送方引起的粘包现象，用户可通过编程设置来避免，TCP提供了强制数据立即传送的操作指令push，TCP程序收到该操作指令后，就**立即将本段数据发送出去**，而不必等待发送缓冲区满；

（2）对于接收方引起的粘包，则可通过优化程序设计、精简接收进程工作量、提高接收进程优先级等措施，使其及时接收数据，从而尽量避免出现粘包现象；

（3）由接收方控制，将一包数据按结构字段，人为控制分多次接收，然后合并，通过这种手段来避免粘包。

‍

1. **Nagle算法**问题导致的，需要结合应用场景适当关闭该算法
2. 尾部标记序列。通过特殊标识符表示数据包的边界，例如\n\r，\t，或者一些隐藏字符。
3. 头部标记分步接收。在TCP报文的头部加上表示数据长度。
4. 应用层发送数据时**定长**发送。

‍

‍

‍

## 应用层

> 不同的网络应用的应用进程之间，还需要有不同的通信规则. 每个应用层协议都是为了解决某一类应用问题， (而问题的解决又必须通过位于不同主机中的多个应用进程之间的通信和协同工作来完成) 。应用进程之间的这种通信必须遵循严格的规则。应用层的具体内容就是精确定义这些通信规则。  
> 运输层是两台主机间进程的交互。应用层是为了更加细化不同网络应用的交互规则。

‍

‍

### 常用协议

应用层常见协议：

HTTP（Hypertext Transfer Protocol，超文本传输协议）：基于 TCP 协议，是一种用于传输超文本和多媒体内容的协议，主要是为 Web 浏览器与 Web 服务器之间的通信而设计的。当我们使用浏览器浏览网页的时候，我们网页就是通过 HTTP 请求进行加载的。

FTP（File Transfer Protocol，文件传输协议） : 基于 TCP 协议，是一种用于在计算机之间传输文件的协议，可以屏蔽操作系统和文件存储方式。

> 不安全的协议，因为它在传输过程中不会对数据进行加密。建议在传输敏感数据时使用更安全的协议，如 SFTP

‍

Telnet（远程登陆协议）：基于 TCP 协议，用于通过一个终端登陆到其他服务器。Telnet 协议的最大缺点之一是所有数据（包括用户名和密码）均以明文形式发送，这有潜在的安全风险。这就是为什么如今很少使用 Telnet，而是使用一种称为 SSH 的非常安全的网络传输协议的主要原因。

SSH（Secure Shell Protocol，安全的网络传输协议）：基于 TCP 协议，通过加密和认证机制实现安全的访问和文件传输等业务

RTP（Real-time Transport Protocol，实时传输协议）：通常基于 UDP 协议，但也支持 TCP 协议。它提供了端到端的实时传输数据的功能，但不包含资源预留存、不保证实时传输质量，这些功能由 WebRTC 实现。

DNS（Domain Name System，域名管理系统）: 基于 UDP 协议，用于解决域名和 IP 地址的映射问题。

‍

‍

### DNS

DNS（Domain Name System，域名系统），因特网上作为**域名和IP地址相互映射**的一个**分布式数据库**，能够使用户更方便的访问互联网，而不用去记住能够被机器直接读取的IP数串。

通过主机名，最终得到该主机名对应的IP地址的过程叫做**域名解析**（或主机名解析）

将主机域名转换为ip地址，使用UDP传输(快啊)

‍

**过程**

> 1） 当用户输入域名时，**浏览器**先检查自己的缓存中是否 这个域名映射的ip地址，有解析结束。  
> 2） 若没命中，则检查**操作系统缓存**（如Windows的hosts）中有没有解析过的结果，有解析结束。  
> 3） 若无命中，则请求**本地域名服务器解析**（LDNS）。 (作代理进行后续遍历)  
> 4） 若LDNS没有命中就直接跳到**根域名服务器**请求解析。根域名服务器返回给LDNS一个 主域名服务器地址。  
> 5） 此时LDNS再发送请求给上一步返回的gTLD（通用顶级域）， 接受请求的gTLD查找并返回这个域名对应的Name Server的地址  
> 6） Name Server根据映射关系表找到目标ip，返回给LDNS  
> 7） LDNS缓存这个域名和对应的ip， 把解析的结果返回给用户，用户根据TTL值缓存到本地系统缓存中，域名解析过程至此结束

‍

**总结**

浏览器缓存，系统缓存，路由器缓存，IPS服务器缓存，根域名服务器缓存，顶级域名服务器缓存，主域名服务器缓存。

主机向本地域名服务器的查询一般都是采用递归查询。

本地域名服务器向根域名服务器的查询的迭代查询。

‍

‍

‍

#### 负载均衡策略

‍

当一个网站有足够多的用户的时候，假如每次请求的资源都位于同一台机器上面，那么这台机器随时可能会蹦掉。

处理办法就是用DNS负载均衡技术

原理是在**DNS服务器中为同一个主机名配置多个IP地址,在应答DNS查询时,DNS服务器对每个查询将以DNS文件中主机记录的IP地址按顺序返回不同的解析结果,将客户端的访问引导到不同的机器上去,使得不同的客户端访问不同的服务器**,从而达到负载均衡的目的

‍

‍

### 服务器缓存

* 缓解服务器压力；
* 降低客户端获取资源的延迟：缓存通常位于内存中，读取缓存的速度更快。并且缓存服务器在地理位置上也有可能比源服务器来得近，例如浏览器缓存。

‍

‍

**实现方法**

* 让代理服务器进行缓存
* 让客户端浏览器进行缓存

‍

‍

### URI 和 URL 的区别?

* URI(Uniform Resource Identifier) 是统一资源标志符，可以唯一标识一个资源。
* URL(Uniform Resource Locator) 是统一资源定位符，可以提供该资源的路径。它是一种具体的 URI，即 URL 可以用来标识一个资源，而且还指明了如何 locate 这个资源。

URI 的作用像身份证号一样，URL 的作用更像家庭住址一样。URL 是一种具体的 URI，它不仅唯一标识资源，而且还提供了定位该资源的信息。

‍

URL=协议+服务器地址+URI

应用协议://服务器地址信息/请求资源的抽象路径

‍

# 实操

‍

## JavaWeb日常

‍

### GET 和 POST 的区别，你知道哪些？

1. 语义上get是获取数据，post是修改数据
2. get把请求的数据放在url上， 以?分割URL和传输数据，参数之间以&相连，所以get不太安全。而post把数据放在**HTTP的包体**内（requrest body）
3. get提交的数据最大是2k（限制实际上取决于浏览器）， post理论上没有限制。
4. GET产生一个TCP数据包，浏览器会把http header和data一并发送出去，服务器响应200(返回数据); POST产生两个TCP数据包，浏览器先发送header，服务器响应100 continue，浏览器再发送data，服务器响应200 ok(返回数据)。
5. GET请求会被**浏览器主动缓存**，而POST不会，除非手动设置。
6. **本质区别：GET是幂等的，而POST不是幂等的**

    > 这里的幂等性：幂等性是指一次和多次请求某一个资源应该具有同样的副作用。简单来说意味着对同一URL的多个请求应该返回同样的结果。
    >

‍

正因为它们有这样的区别，所以不应该且**不能用get请求做数据的增删改这些有副作用的操作**。因为get请求是幂等的，**在网络不好的隧道中会**​**==尝试重试==**。如果用get请求增数据，会有**重复操作**的风险，而这种重复操作可能会导致副作用（浏览器和操作系统并不知道你会用get请求去做增操作）

‍

---

GET 和 POST 是 HTTP 协议中两种常用的请求方法，它们在不同的场景和目的下有不同的特点和用法。一般来说，可以从以下几个方面来区分二者（重点搞清两者在语义上的区别即可）：

* 语义（主要区别）：GET 通常用于获取或查询资源，而 POST 通常用于创建或修改资源。
* 幂等：GET 请求是幂等的，即多次重复执行不会改变资源的状态，而 POST 请求是不幂等的，即每次执行可能会产生不同的结果或影响资源的状态。
* 格式：GET 请求的参数通常放在 URL 中，形成查询字符串（querystring），而 POST 请求的参数通常放在请求体（body）中，可以有多种编码格式，如 application/x-www-form-urlencoded、multipart/form-data、application/json 等。GET 请求的 URL 长度受到浏览器和服务器的限制，而 POST 请求的 body 大小则没有明确的限制。不过，实际上 GET 请求也可以用 body 传输数据，只是并不推荐这样做，因为这样可能会导致一些兼容性或者语义上的问题。
* 缓存：由于 GET 请求是幂等的，它可以被浏览器或其他中间节点（如代理、网关）缓存起来，以提高性能和效率。而 POST 请求则不适合被缓存，因为它可能有副作用，每次执行可能需要实时的响应。
* 安全性：GET 请求和 POST 请求如果使用 HTTP 协议的话，那都不安全，因为 HTTP 协议本身是**明文传输**的，必须使用 HTTPS 协议来加密传输数据。另外，GET 请求相比 POST 请求更容易泄露敏感数据，因为 GET 请求的参数通常放在 URL 中

‍

‍

‍

### Cookie - Session

Cookie

HTTP 协议是**无状态**的，主要是为了让 HTTP 协议尽可能简单，使得它能够处理大量事务，HTTP/1.1 引入 Cookie 来保存状态信息。

Cookie 是**服务器发送到用户浏览器并保存在本地的一小块数据**，它会在浏览器之后向同一服务器再次发起请求时被携带上，用于告知服务端两个请求是否来自同一浏览器。由于之后每次请求都会需要携带 Cookie 数据，因此会带来额外的性能开销（尤其是在移动环境下）。

‍

新的浏览器 API 已经允许开发者直接将数据存储到本地，如使用 Web storage API（本地存储和会话存储）或 IndexedDB

‍

---

Session知识大总结

利用 Session 存储在服务器端，存储在**服务器端的信息更加安全**。

‍

我项目这样做的: 

==使用 Session 维护用户登录状态的过程==

1. 用户进行登录时，用户提交包含用户名和密码的表单，放入 HTTP 请求报文中；
2. 服务器验证该用户名和密码，如果正确则把用户信息存储到 Redis 中，它在 Redis 中的 Key 称为 Session ID；
3. 服务器返回的响应报文的 Set-Cookie **首部字段包含了这个 Session ID**，客户端收到响应报文之后将该 **Cookie 值存入浏览器中**；
4. 客户端之后对同一个服务器进行请求时会**包含该 Cookie 值**，服务器收到之后提取出 Session ID，从 Redis 中取出用户信息，继续之前的业务操作。

‍

抽象地概括一下：一个 cookie 可以认为是一个「变量」，形如 name=value，存储在浏览器；一个 session 可以理解为一种数据结构，多数情况是「映射」（键值对），存储在服务器上。

‍

‍

#### 如何去选择（适用场景）

* Cookie 只能存储 ASCII 码字符串，而 Session 则可以存储任何类型的数据，因此在考虑**数据复杂性**时首选 Session；
* Cookie 存储在浏览器中，容易被恶意查看。如果非要将一些隐私数据存在 Cookie 中，可以将 Cookie 值进行加密，然后在服务器进行解密；
* 对于大型网站，如果用户所有的信息都存储在 Session 中，那么**开销是非常大的**，因此不建议将所有的用户信息都存储到 Session 中

‍

### 简易区别

‍

Cookie：

* 存在浏览器里，可以设置过期时间
* 每次访问服务器时，浏览器会自动在 header 中携带 cookie
* 如果浏览器禁用了 cookie，可以使用 **URL 重写机制**，将信息保存在 URL 里

‍

Session:

* 存在服务端，由服务器维护，一段时间后 session 就失效了
* **本质上，session 还是通过 cookie 实现的**。浏览器的 cookie 中只保存一个 `sessionId`​，所有其他信息均保存在服务端，由 `sessionId`​ 标识
* Session 失效，其实是服务器设置了失效时间。如果用户长时间不和服务器交互（比如 30 分钟），那么 session 就会被销毁；交互的话，就会刷新 session

‍

### 端口有效范围是多少到多少？

0-1023为知名端口号，比如其中HTTP是80，FTP是20（数据端口）、21（控制端口）

UDP和TCP报头使用两个字节存放端口号，所以端口号的有效范围是从0到65535。动态端口的范围是从1024到65535

2^16-1

‍

‍

## 网络行为

‍

### 301、302、307 重定向的原理

返回的 Header 中有一个 `Location`​ 字段指向目标 URL，浏览器会重定向到这个 URL

‍

### 重定向和转发区别

1、重定向是两次请求，转发是一次请求。因此转发的速度要快于重定向  
**重定向过程**:第一次，客户端request一个网址,服务器响应，并response回来，告诉浏览器，你应该去别一个网址

2、重定向之后地址栏上的地址会发生变化，变化成第二次请求的地址，转发之后地址栏上的地址不会变化，还是第一次请求的地址

3、**转发是服务器行为**，**重定向是客户端行为**

4、重定向时的网址可以是任何网址，转发的网址必须是本站点的网址

‍

‍

### 两台主机通信过程

‍

‍

#### 不同局域网的两台主机

以主机 ping 一个域名为例，过程如下：

‍

1. [主机] [应用层] 通过 DNS 协议获取域名的 IP 地址；
2. [主机] [网络层] 构造 IP 数据包，源 IP 为本机 IP，目的 IP 为域名 IP；
3. [主机] [网络层] 根据路由表，选择下一跳的 IP 地址，即当前局域网的网关；
4. [主机] [链路层] 根据 ARP 表，查找网关 IP 的 MAC 地址；在 IP 数据包外面包一层 MAC 地址；
5. ‍
6. [局域网] 根据 MAC 地址，上一步的报文最终会发送到当前局域网的网关；
7. [网关] [网络层] 网关查看数据包的目的 IP 地址，重复上述 2～3 步，继续发给下一跳；
8. ‍
9. [互联网] 中间经过若干个下一跳主机，最终数据包发送到域名所在的网络中心的网关；
10. ‍
11. [网关] [网络层] 网络中心的网关查看数据包的目的 IP 地址，根据路由表发现目的 IP 对应的 `Gateway`​ 为 0.0.0.0 ，这表明目的机器和自己位于同一个局域网内；
12. [网关] [链路层] 根据 ARP 表，查找目的 IP 的 MAC 地址，构造链路层报文；
13. [局域网] 根据 MAC 地址，上一步的报文最终会发送到目的主机；
14. [目的主机] [网络层] 目的主机查看数据包中的目的 IP，发现是给自己的，解析其内容，过程结束。

‍

‍

#### 同局域网内的两台主机

两台主机通过网线、网桥或者交换机连接，就构成了一个局域网

网桥或交换机的作用是连接多台主机，隔离不同的端口，每个端口形成单独的冲突域。当主机连接到网桥或交换机端口的时候，这些设备会要求主机上报 MAC 地址，并在设备内保存 MAC 地址与端口的对应关系。

同局域网内的两台主机进行通信时，只需要根据 ARP 协议获取目的主机的 MAC 地址，构造链路层报文。报文会经过网桥或交换机，后两者根据目的 MAC 地址，在 MAC 地址表里查询目的端口，然后将报文从目的端口转发给对应的主机。

‍

**注意**：

(1) 交换机是链路层的设备，主要根据 MAC 地址进行转发、隔离冲突域；不具有路由功能，不记录路由表。这类设备也称为**二层交换机**。如果只使用二层交换机、不使用路由器来构建局域网，需要为交换机和每台主机分配同属于一个子网的静态 IP。

(2) 路由器工作在 OSI 的第三层网络层，记录路由表，并以此控制数据传输过程。

(3) 有些交换机也具有路由功能，记录了路由表，能够简化路由过程、实现高速转发。这类设备也称为**三层交换机**。

‍

‍

‍

### URL到页面过程

​![15826961451953](assets/net-img-15826961451953-20241022133709-95kxvv4.jpg)​

‍

1. 用户在某个标签页输入 URL 并回车后，浏览器主进程会新开一个网络线程，发起 HTTP 请求
2. 浏览器会进行 DNS 查询，将域名解析为 IP 地址
3. 浏览器获得 IP 地址(以及默认的端口号)后，首先尝试http然后调用Socket建立TCP连接
4. 经过三次握手成功建立连接后，开始传送数据，如果正是http协议的话，就返回就完事了
5. 如果不是http协议，服务器会返回一个5开头的的重定向消息，告诉我们用的是https，那就是说IP没变，但是端口号从80变成443了，好了，再四次挥手关闭TCP，完事. 除了上述的端口号从80变成443之外，还会采用SSL的加密技术来保证传输数据的安全性，保证数据传输过程中不被修改或者替换之类的: 这次依然是三次握手，沟通好双方使用的认证算法，加密和检验算法，在此过程中也会检验对方的CA安全证书
6. 浏览器向服务器发起 HTTP 请求
7. 服务器处理请求，返回 HTTP 响应, css, html文件回来
8. 浏览器的渲染进程解析数据并绘制页面
9. 如果遇到 JS/CSS/图片 等静态资源的引用链接，重复上述过程，向服务器请求这些资源

‍

* 根据域名，进行DNS域名解析
* 拿到解析的IP地址，建立TCP连接
* 向IP地址，发送HTTP请求
* 服务器处理请求
* 返回响应结果
* 关闭TCP连接
* 浏览器解析HTML
* 浏览器布局渲染
* 补充资源(如果需要)

‍

‍

#### **用到的协议**

* TCP:与服务器建立TCP连接
* IP: 建立TCP协议时，需要发送数据，发送数据在网络层使用IP协议
* OPSF: IP数据包在路由器之间，路由选择使用OPSF协议
* ARP: 路由器在与服务器通信时，需要将ip地址转换为MAC地址，需要使用ARP协议
* HTTP: 在TCP建立完成后，使用HTTP协议访问网页

‍

​​

#### 该DNS解析流程

（1）浏览器先检查**自身缓存**中有没有被解析过的这个域名对应的ip地址，(如果有，解析结束。同时域名被缓存的时间也可通过TTL属性来设置。)

（2）在主机查询**操作系统**DNS缓存，也就是hosts文件里配置的。

（3）如果浏览器和系统缓存都没有，系统的 gethostname 函数就会找**本地 DNS 服务器**发送请求。而网络服务一般都会先经过路由器以及网络服务商（电信），所以会先查询**路由器缓存**，然后再查询 **ISP** 的 DNS 缓存。

（4）如果至此还没有命中域名，才会真正的请求本地域名服务器（LDNS）来解析这个域名

（5）如果LDNS仍然没有命中，本地的DNS服务器向根域名服务器发送查询请求，根域名服务器返回该域名的顶级级域名服务器。依次类推：根域名服务器-顶级域名服务器-主域名服务器

* **本地DNS服务器** 继续向域服务器发出请求，在域服务器收到请求之后，也不会直接返回域名和IP地址的对应关系，而是告诉本地DNS服务器，你的域名的解析服务器的地址。
* 最后，本地DNS服务器向**域名的解析服务器**发出请求，这时就能收到一个域名和IP地址对应关系，本地DNS服务器不仅要把IP地址返回给用户电脑，还要把这个对应关系保存在缓存中，以备下次别的用户查询时，可以直接返回结果，加快网络访问。

‍

‍

‍

‍

### 负载均衡怎么做

互联网分布式使用中的

‍

‍

#### 发送端

DNS缓存

‍

‍

#### 接收端

Nginx, Tomcat

‍

‍

‍

## HTTP

‍

### 完整的HTTP请求过程内容

‍

* 建立起客户机和服务器连接。
* 建立连接后，客户机发送一个请求给服务器。
* 服务器收到请求给予响应信息。
* 客户端浏览器将返回的内容解析并呈现，断开连接

‍

‍

### HTTP常见响应状态码

‍

需要记住的（下面的应该足够了）

100, 101

200, 201, 202, 204

301, 302 , 304, 305

400, 401, 403, 404, 408

500, 502 ,504

‍

‍

### 禁用缓存？如何确认缓存？

HTTP/1.1 通过 Cache-Control 首部字段来控制缓存。

‍

**禁止进行缓存**

no-store 指令规定不能对请求或响应的任何一部分进行缓存。

```html
Cache-Control: no-store
```

强制确认缓存

no-cache 指令规定缓存服务器需要先向源服务器验证缓存资源的有效性，只有当缓存资源有效时才能使用该缓存对客户端的请求进行响应。

```html
Cache-Control: no-cache
```

‍

‍

### HTTP Header 中常见的字段有哪些

|请求头字段名|说明|示例|
| ---------------------| ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------| ----------------------------------------------------------------------------------|
|Accept|能够接受的回应内容类型（Content-Types）。|Accept: text/plain|
|Accept-Charset|能够接受的字符集|Accept-Charset: utf-8|
|Accept-Datetime|能够接受的按照时间来表示的版本|Accept-Datetime: Thu, 31 May 2007 20:35:00 GMT|
|Accept-Encoding|能够接受的编码方式列表。参考 HTTP 压缩。|Accept-Encoding: gzip, deflate|
|Accept-Language|能够接受的回应内容的自然语言列表。|Accept-Language: en-US|
|**Authorization**|用于超文本传输协议的认证的认证信息|Authorization: Basic QWxhZGRpbjpvcGVuIHNlc2FtZQ==|
|Cache-Control|用来指定在这次的请求/响应链中的所有缓存机制 都必须 遵守的指令|Cache-Control: no-cache|
|Connection|该浏览器想要优先使用的连接类型|Connection: keep-alive Connection: Upgrade|
|Content-Length|以 八位字节数组 （8 位的字节）表示的请求体的长度|Content-Length: 348|
|Content-MD5|请求体的内容的二进制 MD5 散列值，以 Base64 编码的结果|Content-MD5: Q2hlY2sgSW50ZWdyaXR5IQ==|
|Content-Type|请求体的 多媒体类型 （用于 POST 和 PUT 请求中）|Content-Type: application/x-www-form-urlencoded|
|**Cookie**|之前由服务器通过 Set- Cookie （下文详述）发送的一个 超文本传输协议 Cookie|Cookie: $Version=1; Skin=new;|
|Date|发送该消息的日期和时间(按照 RFC 7231 中定义的"超文本传输协议日期"格式来发送)|Date: Tue, 15 Nov 1994 08:12:31 GMT|
|Expect|表明客户端要求服务器做出特定的行为|Expect: 100-continue|
|From|发起此请求的用户的邮件地址|From: [user@example.com](mailto:user@example.com)|
|Host|服务器的域名(用于虚拟主机 )，以及服务器所监听的传输控制协议端口号。如果所请求的端口是对应的服务的标准端口，则端口号可被省略。|Host: [en.wikipedia.org:80open in new window](http://en.wikipedia.org/)|
|If-Match|仅当客户端提供的实体与服务器上对应的实体相匹配时，才进行对应的操作。主要作用时，用作像 PUT 这样的方法中，仅当从用户上次更新某个资源以来，该资源未被修改的情况下，才更新该资源。|If-Match: “737060cd8c284d8af7ad3082f209582d”|
|If-Modified-Since|允许在对应的内容未被修改的情况下返回 304 未修改（ 304 Not Modified ）|If-Modified-Since: Sat, 29 Oct 1994 19:43:31 GMT|
|If-None-Match|允许在对应的内容未被修改的情况下返回 304 未修改（ 304 Not Modified ）|If-None-Match: “737060cd8c284d8af7ad3082f209582d”|
|If-Range|如果该实体未被修改过，则向我发送我所缺少的那一个或多个部分；否则，发送整个新的实体|If-Range: “737060cd8c284d8af7ad3082f209582d”|
|If-Unmodified-Since|仅当该实体自某个特定时间已来未被修改的情况下，才发送回应。|If-Unmodified-Since: Sat, 29 Oct 1994 19:43:31 GMT|
|Max-Forwards|限制该消息可被代理及网关转发的次数。|Max-Forwards: 10|
|Origin|发起一个针对 跨来源资源共享 的请求。|Origin: [http://www.example-social-network.comopen in new window](http://www.example-social-network.com/)|
|Pragma|与具体的实现相关，这些字段可能在请求/回应链中的任何时候产生多种效果。|Pragma: no-cache|
|Proxy-Authorization|用来向代理进行认证的认证信息。|Proxy-Authorization: Basic QWxhZGRpbjpvcGVuIHNlc2FtZQ==|
|Range|仅请求某个实体的一部分。字节偏移以 0 开始。参见字节服务。|Range: bytes=500-999|
|Referer|表示浏览器所访问的前一个页面，正是那个页面上的某个链接将浏览器带到了当前所请求的这个页面。|Referer: [http://en.wikipedia.org/wiki/Main_Pageopen in new window](https://en.wikipedia.org/wiki/Main_Page)|
|TE|浏览器预期接受的传输编码方式：可使用回应协议头 Transfer-Encoding 字段中的值；|TE: trailers, deflate|
|Upgrade|要求服务器升级到另一个协议。|Upgrade: HTTP/2.0, SHTTP/1.3, IRC/6.9, RTA/x11|
|**User-Agent**|浏览器的浏览器身份标识字符串|User-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:12.0) Gecko/20100101 Firefox/21.0|
|Via|向服务器告知，这个请求是由哪些代理发出的。|Via: 1.0 fred, 1.1 [example.comopen in new window](http://example.com/) (Apache/1.1)|
|Warning|一个一般性的警告，告知，在实体内容体中可能存在错误。|Warning: 199 Miscellaneous warning|

‍

‍

#### 发送post请求时，可以发送二进制数据吗

请求头里面Content-Type 设置为 application/octet-stream 就可以

‍

‍

### 一个TCP连接可以对应几个HTTP请求？

如果维持连接，一个 TCP 连接是可以发送多个 HTTP 请求的。

（比如一起发三个请求，再三个响应一起接收）

‍

HTTP/1.1 存在一个问题，单个 TCP 连接在同一时刻只能处理一个请求，意思是说：两个请求的生命周期不能重叠，任意两个 HTTP 请求从开始到结束的时间在同一个 TCP 连接里不能重叠。

在 HTTP/1.1 存在 Pipelining 技术可以完成这个多个请求同时发送，但是由于浏览器默认关闭，所以可以认为这是不可行的。在 HTTP2 中由于 Multiplexing 特点的存在，多个 HTTP 请求可以在同一个 TCP 连接中并行进行。

那么在 HTTP/1.1 时代，浏览器是如何提高页面加载效率的呢？主要有下面两点：

* 维持和服务器已经建立的 TCP 连接，在同一连接上顺序处理多个请求。
* 和服务器建立多个 TCP 连接。

‍

### 浏览器对同一 Host 建立 TCP 连接到的数量有没有限制？

‍

HTTP/1.1 时代，那个时候没有多路传输，**有。**

**Chrome 最多允许对同一个 Host 建立六个 TCP 连接。不同的浏览器有一些区别**

‍

如果图片都是 HTTPS 连接并且在同一个域名下，那么浏览器在 SSL 握手之后会和服务器商量==能不能用 HTTP2==，如果能的话就使用 **Multiplexing** 功能在这个连接上进行**多路传输**。不过也未必会所有挂在这个域名的资源都会使用一个 TCP 连接去获取，但是可以确定的是 Multiplexing 很可能会被用到。

‍

如果发现用不了 HTTP2 呢？或者用不了 HTTPS（现实中的 HTTP2 都是在 HTTPS 上实现的，所以也就是只能使用 HTTP/1.1）。那浏览器就会在一个 HOST 上建立多个 TCP 连接，连接数量的最大限制取决于**浏览器设置**，这些连接会在空闲的时候被浏览器用来发送新的请求，如果所有的连接都正在发送请求呢？那其他的请求就只能等等了。

‍

‍

### HTTP 2.0

HTTP 2.0 的三大特性是**头部压缩、服务端推送、多路复用**。

‍

HTTP 1.1 允许通过同一个连接发起多个请求。但是由于 HTTP 1.X 使用**文本格式**传输数据，服务端必须按照客户端请求到来的顺序，**串行返回**数据。此外，HTTP 1.1 中浏览器会限制同时发起的最大连接数，超过该数量的连接会被**阻塞**。

HTTP 2 引入了**多路复用 MP**，允许通过同一个连接发起多个请求，并且可以**并行传输**数据。HTTP 2 使用**二进制数据帧**作为传输的最小单位，每个`帧`​标识了自己属于哪个`流`​，每个`流`​对应一个请求。服务端可以并行地传输数据，而接收端可以根据帧中的顺序标识，自行合并数据。

‍

在 HTTP 1.1 中，由于浏览器限制每个域名下最多同时有 6 个请求，其余请求会被阻塞，因此我们通常使用**多个域名**（比如 CDN）来提高浏览器的下载速度。HTTP 2 就不再需要这样优化了。

同理，在 HTTP 1.1 中，我们会将多个 JS 文件、CSS 文件等打包成一个文件，将多个小图片合并为雪碧图，目的是减少 HTTP 请求数。HTTP 2 也不需要这样优化了。

‍

‍

### HTTP 的长连接与短连接

**HTTP/1.0 默认使用的是短连接**。也就是说，浏览器每请求一个静态资源，就建立一次连接，任务结束就中断连接。

**HTTP/1.1 默认使用的是长连接**。长连接是指在一个网页打开期间，所有网络请求都使用同一条已经建立的连接。当没有数据发送时，双方需要发检测包以维持此连接。长连接不会永久保持连接，而是有一个保持时间。实现长连接要客户端和服务端都支持长连接。

长连接的优点：TCP 三次握手时会有 1.5 RTT 的延迟，以及建立连接后慢启动（slow-start）特性，当请求频繁时，建立和关闭 TCP 连接会浪费时间和带宽，而重用一条已有的连接性能更好。

长连接的缺点：长连接会占用服务器的资源。

‍

长连接适用于操作频繁、点对点通讯，如数据库的连接

短连接常用于一点对多点通讯，如web网站的http服务. 长连接对于服务端来说会耗费一定的资源，而像 WEB 网站这么频繁的成千上万甚至上亿客户端的连接用短连接会更省一些资源

‍

### HTTP缓存是怎么实现去获取资源？以及他有一个字段是缓存资源的过期时间的？

HTTP缓存都是从第二次请求开始的

Expires：指定资源的过期时间

‍

第一次请求资源时，服务器返回资源，并在response header中回传资源的缓存策略；`Cache-Control`​  
第二次请求时，浏览器判断这些请求参数，击中强缓存就直接200，否则就把请求参数加到request header头中传给服务器，看是否击中协商缓存，击中则返回304，否则服务器会返回新的资源

‍

‍

### HTTP的缺点有哪些？

* 使用明文进行通信，内容可能会被窃听；
* 不验证通信方的身份，通信方的身份有可能遭遇伪装；
* 无法证明报文的完整性，报文有可能遭篡改。

‍

‍

## HTTPS

HTTPS（HyperText Transfer Protocol Secure）协议是基于HTTP协议和SSL/TLS协议实现的。以下是它的主要组成部分：

1. **HTTP**：==超文本传输协议==，用于在客户端和服务器之间传输超文本数据（如HTML、CSS、JavaScript等）。
2. **SSL/TLS**：==安全套接字层==（SSL）和==传输层安全==（TLS）协议，用于在传输层提供加密和数据完整性保护。

‍

HTTPS 并不是新协议，而是让 **HTTP 先和 SSL（Secure Sockets Layer）通信，再由 SSL 和 TCP 通信，也就是说 HTTPS 使用了隧道进行通信**。通过使用 SSL，HTTPS 具有了加密（防窃听）、认证（防伪装）和完整性保护（防篡改）

‍

### HTTPS精简版

HTTPS 并不是新协议，而是让 HTTP 先和 SSL（Secure Sockets Layer）通信，再由 SSL 和 TCP 通信，也就是说 HTTPS 使用了隧道进行通信。

通过使用 SSL，HTTPS 具有了加密（防窃听）、认证（防伪装）和完整性保护（防篡改）。

‍

#### 1.加密(对称加密和非对称加密)

加密分为对称密匙加密和非对称密匙加密2种。

‍

对称密钥加密是指加密和解密使用同一个密钥的方式，这种方式存在的最大问题就是密钥发送问题，即如何安全地将密钥发给对方。

‍

优点: 对称加密算法的优点是算法公开、计算量小、加密速度快、加密效率高。

缺点: 没有非对称加密安全.

‍

用途： 一般用于保存用户手机号、身份证等敏感但能解密的信息。

常见的对称加密算法有: AES、HS256

---

非对称加密是指使用一对非对称密钥，即公钥和私钥，公钥可以随意发布，但私钥只有自己知道。发送密文的一方使用对方的公钥进行加密处理，对方接收到加密信息后，使用自己的私钥进行解密。

‍

优点: 非对称加密与对称加密相比，其安全性更好；

缺点: 非对称加密的缺点是加密和解密花费时间长、速度慢，只适合对少量数据进行加密。 用途： 一般用于签名和认证。私钥服务器保存, 用来加密, 公钥客户拿着用于对于令牌或者签名的解密或者校验使用.

‍

‍

常见的非对称加密算法有： RSA、DSA（数字签名用）、ECC（移动设备用）、RS256 (采用SHA-256 的 RSA 签名)

HTTPS 采用混合的加密机制：

* 使用非对称密钥加密方式，传输对称密钥加密方式所需要的 Secret Key，从而保证安全性;
* 获取到 Secret Key 后，再使用对称密钥加密方式进行通信，从而保证效率。

‍

‍

#### 2.认证

通过使用 证书 来对通信方进行认证。

数字证书认证机构（CA，Certificate Authority）是客户端与服务器双方都可信赖的第三方机构。

服务器的运营人员向 CA 提出公开密钥的申请，CA 在判明提出申请者的身份之后，会对已申请的公开密钥做数字签名，然后分配这个已签名的公开密钥，并将该公开密钥放入公开密钥证书后绑定在一起。

进行 HTTPS 通信时，服务器会把证书发送给客户端。客户端取得其中的公开密钥之后，先使用数字签名进行验证，如果验证通过，就可以开始通信了。

‍

‍

#### 3.完整性保护SSL

SSL 提供报文摘要功能来进行完整性保护。

HTTP 也提供了 MD5 报文摘要功能，但不是安全的。例如报文内容被篡改之后，同时重新计算 MD5 的值，通信接收方是无法意识到发生了篡改。

HTTPS 的报文摘要功能之所以安全，是因为它结合了加密和认证这两个操作。试想一下，加密之后的报文，遭到篡改之后，也很难重新计算报文摘要，因为无法轻易获取明文。

‍

‍

#### 4.简单说下 HTTPS 和 HTTP 的区别

Http协议运行在TCP之上，明文传输(不安全)，客户端与服务器端都无法验证对方的身份；Https是身披SSL(Secure Socket Layer)外壳的Http，运行于SSL上，SSL运行于TCP之上，是添加了加密和认证机制的HTTP。二者之间存在如下不同：

1、端口不同：Http与Https使用不同的连接方式，用的端口也不一样，前者是80，后者是443；

2、资源消耗：和HTTP通信相比，Https通信会由于加减密处理消耗更多的CPU和内存资源；

3、开销：Https通信需要证书，而证书一般需要向认证机构购买；　 4、安全性：HTTP 的连接很简单，是无状态的；HTTPS 协议是由 TLS+HTTP 协议构建的可进行加密传输、身份认证的网络协议，比 HTTP 协议安全

‍

‍

#### 5.HTTPS是如何保证数据安全的呢？

HTTPS是基于HTTP的上层添加了一个叫做TLS的安全层，对数据的加密等操作都是在这个安全层中进行处理的，其底层还是应用的HTTP。用对称加密来传送消息，但对称加密所使用的密钥是通过非对称加密的方式发送出去。

‍

‍

‍

‍

### HTTPS链接建立的过程

‍

HTTPS在传输的过程中会涉及到三个密钥

* 服务器端的公钥和私钥，用来进行**非对称加密**
* 客户端生成的随机密钥，用来进行**对称加密**

‍

一个HTTPS请求实际上包含了**两次HTTP传输**，可以细分为8步

1. 客户端向服务器发起HTTPS请求，连接到服务器的443端口。
2. 服务器端有一个密钥对，即公钥和私钥，是用来进行非对称加密使用的，服务器端保存着私钥，不能将其泄露，公钥可以发送给任何人。
3. 服务器将自己的公钥发送给客户端。
4. 客户端收到服务器端的公钥之后，会对公钥进行检查，验证其合法性，如果发现发现公钥有问题，那么HTTPS传输就无法继续。

    > 【严格的说，这里应该是验证服务器发送的**数字证书的合法性**，关于客户端如何验证数字证书的合法性，下文会进行说明】如果公钥合格，那么客户端会生成一个随机值，这个随机值就是用于进行对称加密的密钥，我们将该密钥称之为client key，即客户端密钥，这样在概念上和服务器端的密钥容易进行区分。然后用服务器的公钥对客户端密钥进行非对称加密，这样客户端密钥就变成密文了，至此，HTTPS中的第一次HTTP请求结束。
    >
5. 客户端会发起HTTPS中的第二个HTTP请求，将加密之后的客户端密钥发送给服务器。
6. 服务器接收到客户端发来的密文之后，会用自己的私钥对其进行非对称解密，解密之后的明文就是客户端密钥，然后用客户端密钥对数据进行对称加密，这样数据就变成了密文。
7. 然后服务器将加密后的密文发送给客户端。
8. 客户端收到服务器发送来的密文，用客户端密钥对其进行对称解密，得到服务器发送的数据。这样HTTPS中的第二个HTTP请求结束，整个HTTPS传输完成。

‍

简单版:

‍

1. **客户端发起请求**：客户端向服务器发起HTTPS请求。
2. **服务器响应并发送证书**：服务器响应请求并发送SSL/TLS证书给客户端。
3. **客户端验证证书**：客户端验证服务器的证书是否合法。
4. **协商加密算法**：客户端和服务器协商使用的加密算法和会话密钥。
5. **建立加密通道**：使用协商好的会话密钥建立加密通道。
6. **传输数据**：在加密通道中传输HTTP数据。

‍

HTTPS 请求示意图

```plaintext
客户端  -->  服务器
  |            |
  |  发起请求   |
  |----------->|
  |            |
  |  发送证书   |
  |<-----------|
  |            |
  |  验证证书   |
  |            |
  |  协商加密   |
  |<---------->|
  |            |
  |  建立加密   |
  |            |
  |  传输数据   |
  |<---------->|
```

‍

‍

‍

### HTTPS是如何保证数据传输的安全，整体的流程是什么？（SSL是怎么工作保证安全的）

‍

（1）客户端向服务器端发起SSL连接请求；  
（2）服务器把公钥发送给客户端，并且服务器端保存着唯一的私钥  
（3）客户端用公钥对双方通信的对称秘钥进行加密，并发送给服务器端  
（4）服务器利用自己唯一的私钥对客户端发来的对称秘钥进行解密，  
（5）进行数据传输，服务器和客户端双方用公有的相同的对称秘钥对数据进行加密解密，可以保证在数据收发过程中的安全，即是第三方获得数据包，也无法对其进行加密，解密和篡改。

‍

因为数字签名、摘要是证书防伪非常关键的武器。 “摘要”就是对传输的内容，通过hash算法计算出一段固定长度的串。然后，通过发送方的私钥对这段摘要进行加密，加密后得到的结果就是“数字签名”

SSL/TLS协议的基本思路是采用公钥加密法，也就是说，客户端先向服务器端索要公钥，然后用公钥加密信息，服务器收到密文后，用自己的私钥解密。

‍

‍

#### 什么是SSL/TLS ？

SSL代表安全套接字层。它是一种用于加密和验证应用程序（如浏览器）和Web服务器之间发送的数据的协议。 身份验证 ， 加密Https的加密机制是一种共享密钥加密和公开密钥加密并用的混合加密机制。

SSL/TLS协议作用：认证用户和服务，加密数据，维护数据的完整性的应用层协议加密和解密需要两个不同的密钥，故被称为非对称加密；加密和解密都使用同一个密钥的

对称加密：优点在于加密、解密效率通常比较高 ，HTTPS 是基于非对称加密的， 公钥是公开的，

‍

‍

#### HTTPS采用的加密方式有哪些？是对称还是非对称？

HTTPS 采用混合的加密机制，使用**非对称密钥加密用于传输对称密钥来保证传输过程的安全性**，之后使用**对称密钥加密进行通信来保证通信过程的效率**。

‍

确保传输安全过程（其实就是rsa原理）：

1. Client给出协议版本号、一个客户端生成的随机数（Client random），以及客户端支持的加密方法。
2. Server确认双方使用的加密方法，并给出数字证书、以及一个服务器生成的随机数（Server random）。
3. Client确认数字证书有效，然后生成一个新的随机数（Premaster secret），并使用数字证书中的公钥，加密这个随机数，发给Server。
4. Server使用自己的私钥，获取Client发来的随机数（Premaster secret）。
5. Client和Server根据约定的加密方法，使用前面的三个随机数，生成”对话密钥”（session key），用来加密接下来的整个对话过程。

‍

‍

### 区别to HTTP

‍

#### **端口：**

HTTP的URL由“http://”起始且默认使用端口80，而HTTPS的URL由“https://”起始且默认使用端口443。

‍

#### **安全性：**

HTTP协议运行在TCP之上，所有传输的内容都是明文，客户端和服务器端都无法验证对方的身份。

HTTPS是运行在SSL/TLS之上的HTTP协议，SSL/TLS 运行在TCP之上。所有传输的内容都经过加密，**加密采用对称加密，但对称加密的密钥**用服务器方的证书进行了非对称加密。

‍

#### 性能

HTTPS 比HTTP耗费更多服务器资源

‍

‍

‍

‍

## 网安

‍

### 对称加密算法和非对称加密算法

**对称加密**：密钥只有一个，加密解密为同一个密码，且加解密速度快，典型的对称加密算法有DES、AES等； 速度快

> 这种方式存在的最大问题就是密钥发送问题，即如何安全地将密钥发给对方,对称加密所使用的密钥我们可以通过非对称加密的方式发送出去。

‍

**非对称加密**：密钥成对出现（且根据公钥无法推知私钥，根据私钥也无法推知公钥），加密解密使用不同密钥（公钥加密需要私钥解密，私钥加密需要公钥解密），相对对称加密速度较慢，典型的非对称加密算法有 ==RSA、DSA==等。

> 非对称加密算法实现机密信息交换的基本过程是:
>
> 甲方生成一对密钥并将其中的一把作为公用密钥向其它方公开;得到该公用密钥的乙方使用该密钥对机密信息进行加密后再发送给甲方;甲方再用自己保存的另一把专用密钥对加密后的信息进行解密

‍

> 例如与熊对话这种思路, 双方都是讲英文, 但是套在秘密私聊软件里面进行公开加密, 我们自己对自己的对话进行英译中

公开密钥所有人都可以获得，**通信发送方获得接收方的公开密钥之后，就可以使用公开密钥进行加密**，**接收方收到通信内容后使用私有密钥解密**。

非对称密钥除了用来加密，还可以用来进行签名。因为私有密钥无法被其他人获取，因此通信发送方使用其私有密钥进行签名，通信接收方使用发送方的公开密钥对签名进行解密，就能判断这个签名是否正确。

* 优点：可以更安全地将公开密钥传输给通信发送方；
* 缺点：运算速度慢。

‍

‍

#### SSL中的认证中的证书是什么？了解过吗？

通过使用 **证书** 来对通信方进行认证。

数字证书认证机构（CA，Certificate Authority）是客户端与服务器双方都可信赖的第三方机构。

服务器的运营人员向 CA 提出公开密钥的申请，CA 在判明提出申请者的身份之后，会对已申请的公开密钥做数字签名，然后分配这个已签名的公开密钥，并将该公开密钥放入公开密钥证书后绑定在一起。

进行 HTTPS 通信时，服务器会把证书发送给客户端。客户端取得其中的公开密钥之后，先使用数字签名进行验证，如果验证通过，就可以开始通信了。

‍

‍

#### 如何保证公钥不被篡改？

将公钥放在数字证书中。只要证书是可信的，公钥就是可信的

‍

‍

#### 公钥加密计算量太大，如何减少耗用的时间？

每一次对话（session），客户端和服务器端都生成一个"**对话密钥**"（session key），用它来加密信息。由于"对话密钥"是对称加密，所以运算速度非常快，而服务器公钥只用于加密"对话密钥"本身，这样就减少了加密运算的消耗时间。

‍

（1） 客户端向服务器端索要并验证公钥。  
（2） 双方协商生成"对话密钥"。  
（3） 双方采用"对话密钥"进行加密通信。上面过程的前两步，又称为"握手阶段"（handshake）。

‍

‍

‍

### DDos分布式拒绝服务攻击

TCP的内容

客户端向服务端发送请求链接数据包，服务端向客户端发送确认数据包，客户端不向服务端发送确认数据包，服务器一直等待来自客户端的确认  
没有彻底根治的办法，除非不使用TCP

‍

治理

* 限制同时打开SYN半链接的数目
* 缩短SYN半链接的 Time out 时间
* 关闭不必要的服务

‍

#### DDOS - SYN攻击

TCP内容

‍

**服务器端的资源分配是在二次握手时分配的，而客户端的资源是在完成三次握手时分配的**，所以服务器容易受到SYN洪泛攻击。

SYN攻击就是Client在短时间内伪造大量不存在的IP地址，并向Server不断地发送SYN包，**Server则回复确认包**，并等待Client确认，由于源地址不存在，因此Server需要不断重发直至超时，这些伪造的SYN包将长时间占用未连接队列，导致正常的SYN请求因为队列满而被丢弃，从而引起网络**拥塞**甚至系统瘫痪。SYN 攻击是一种典型的 DoS/DDoS 攻击。

检测 SYN 攻击非常的方便，当你在服务器上看到大量的半连接状态时，特别是源IP地址是随机的，基本上可以断定这是一次SYN攻击

‍

常见的防御 SYN 攻击的方法同上.

* 缩短超时（SYN Timeout）时间
* 增加最大半连接数
* 过滤网关防护
* SYN cookies技术

‍

‍

### CSRF跨站点请求伪造攻击

跨站点请求伪造，指攻击者通过跨站请求，以**合法的用户**的身份进行非法操作。

可以这么理解CSRF攻击：攻击者**盗用你的身份**，以你的名义向第三方网站发送恶意请求。

‍

治理

* **安全框架**，例如Spring Security
* **token机制**。在HTTP请求中进行token验证，如果请求中没有token或者token内容不正确，则认为CSRF攻击而拒绝该请求。
* **验证码**。通常情况下，验证码能够很好的遏制CSRF攻击，(验明正身) 但是很多情况下，出于用户体验考虑，验证码只能作为一种辅助手段，而不是最主要的解决方案。
* **来源地址 referer识别**。在HTTP Header中有一个字段Referer，它记录了HTTP请求的**来源地址(开盒)** 。如果Referer是其他网站，就有可能是CSRF攻击，则拒绝该请求。但是，服务器并非都能取到Referer。很多用户出于隐私保护的考虑，限制了Referer的发送。在某些情况下，浏览器也不会发送Referer，例如HTTPS跳转到HTTP。

‍

‍

### SQL注入攻击

‍

攻击者在HTTP请求中注入恶意的SQL代码，服务器使用参数构建数据库SQL命令时，恶意SQL被一起构造，并在数据库中执行。  
用户登录，输入用户名 lianggzone，密码 ‘ or ‘1’=’1 ，如果此时使用参数构造的方式，就会出现  
select * from user where name = ‘lianggzone’ and password = ‘’ or ‘1’=‘1’  
不管用户名和密码是什么内容，使查询出来的用户列表不为空。如何防范SQL注入攻击使用预编译的PrepareStatement是必须的，但是一般我们会从两个方面同时入手。

‍

Web端(前端)

1）有效性检验。  
2）限制字符串输入的长度。

‍

服务端

1）不用拼接SQL字符串。  
2）**使用预编译的PrepareStatement**  
3）有效性检验。(为什么服务端还要做有效性检验？**第一准则，外部都是不可信的，防止攻击者绕过Web端请求直接掏后台**)  
4）过滤SQL需要的参数中的特殊字符。比如单引号、双引号。

‍

‍

### 网络连接中出现了很多 time-wait, 为什么

在网络连接中出现很多 `TIME-WAIT`​ 状态的原因主要是由于 TCP 协议的设计。`TIME-WAIT`​ 状态是 TCP 连接关闭过程中的一个阶段，确保所有数据包都被正确接收并防止旧的重复数据包干扰新的连接。以下是详细原因：

1. **确保数据包的可靠传输**：`TIME-WAIT`​ 状态允许 TCP 协议确保所有数据包都被正确接收。如果有任何丢失的数据包，发送方可以在此期间重传。
2. **防止旧数据包干扰新连接**：`TIME-WAIT`​ 状态的持续时间通常是 2 倍的最大报文段寿命（MSL），以确保旧的重复数据包在新连接建立之前被丢弃。
3. **连接资源的释放**：在 `TIME-WAIT`​ 状态结束后，系统会释放与该连接相关的所有资源。

‍

> 考虑高并发短连接的业务场景，在高并发短连接的 TCP 服务器上，当服务器处理完请求后主动请求关闭连接，这样服务器上会有大量的连接处于 TIME_WAIT 状态，服务器维护每一个连接需要一个 socket，也就是每个连接会占用一个文件描述符，而文件描述符的使用是有上限的，如果持续高并发，会导致一些正常的 连接失败。

‍

#### 解决方法

如果在服务器上看到大量的 `TIME-WAIT`​ 状态，可以考虑以下几种方法来优化：

1. **调整操作系统参数**：可以通过调整操作系统的 **TCP 参数**来减少 `TIME-WAIT`​ 状态的数量。例如，在 Linux 系统中，可以调整 `tcp_tw_reuse`​ 和 `tcp_tw_recycle`​ 参数。
2. **使用长连接**：对于频繁的短连接请求，可以考虑使用长连接（Keep-Alive）来减少连接的建立和关闭次数。
3. **负载均衡**：通过负载均衡将流量分散到多个服务器上，减少单个服务器的连接压力
4. 服务器可以设置 SO\_REUSEADDR 套接字选项来通知内核，如果端口被占用，但 TCP 连接位于 TIME\_WAIT 状态时可以重用端口

‍

## 其他

‍

### 客户端和服务端距离很远，建立连接后，客户端长时间不发消息，连接会怎么样

‍

* 会发送数据包保活，超时会断开连接
* tcp 的keep-alive机制
* 应用层心跳机制
* 中间设备（如防火墙，NAT）的超时机制
* 配置连接超时
* 使用长连接协议

‍

‍

‍

‍

‍

‍
